import streamlit as st
import os
import sys
from datetime import datetime
import pandas as pd
import json
from collections import defaultdict
import hashlib
import pickle
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from utils.data_loader import DataLoader
from utils.cache_manager import CacheManager
from utils.llm_processor import LLMProcessor, get_keywords

# æ·»åŠ ç¬¬äºŒé˜¶æ®µç¼“å­˜ç®¡ç†å™¨
from utils.stage2_cache_manager import Stage2CacheManager

# ========== Sessionåˆå§‹åŒ– ==========
def init_session_state():
    # å‚è€ƒåŸæœ‰åˆå§‹åŒ–ï¼Œç®€åŒ–ç‰ˆ
    if "api_key" not in st.session_state:
        st.session_state.api_key = ""
    if "loaded_data" not in st.session_state:
        st.session_state.loaded_data = None
    if "selected_keywords" not in st.session_state:
        st.session_state.selected_keywords = []
    if "keyword_subsets" not in st.session_state:
        st.session_state.keyword_subsets = {"ML": [], "DL": [], "LLM": []}
    if "results_cache" not in st.session_state:
        st.session_state.results_cache = None
    if "stage2_results_cache" not in st.session_state:
        st.session_state.stage2_results_cache = None
    if "last_session_time" not in st.session_state:
        st.session_state.last_session_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if "show_detail_view" not in st.session_state:
        st.session_state.show_detail_view = False
    if "selected_result" not in st.session_state:
        st.session_state.selected_result = None
    if "filter_no_match" not in st.session_state:
        st.session_state.filter_no_match = False
    if "deduplication" not in st.session_state:
        st.session_state.deduplication = True

# ========== æ•°æ®åŠ è½½é¡µé¢ ==========
def render_data_loading_page():
    st.header("ğŸ“Š æ•°æ®åŠ è½½")
    cache_manager = CacheManager()
    data_loader = DataLoader()
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("é€‰æ‹©æ•°æ®æ¥æºå’Œæ–‡ä»¶")
        available_files = data_loader.get_available_data_files()
        # æ£€æŸ¥ç¼“å­˜ä¸­çš„ä¸Šä¸€æ¬¡åŠ è½½çš„æ•°æ®
        cached_data, metadata = None, None
        if hasattr(cache_manager, 'load_last_data'):
            try:
                cached_data, metadata = cache_manager.load_last_data()
            except Exception:
                cached_data, metadata = None, None
        # æ£€æŸ¥ç¼“å­˜ä¸­çš„ä¸Šä¸€æ¬¡åŠ è½½çš„æ•°æ®
        cached_data, metadata = cache_manager.load_last_data()
        if cached_data is not None and metadata is not None:
            st.info(f"å‘ç°ä¸Šæ¬¡åŠ è½½çš„æ•°æ®: {metadata['rows']}æ¡è®°å½•ï¼Œæ¥æº: {metadata['source']}ï¼ŒåŠ è½½æ—¶é—´: {metadata['timestamp']}")
            if st.button("æ¢å¤ä¸Šæ¬¡åŠ è½½çš„æ•°æ®"):
                st.session_state.loaded_data = cached_data
                st.session_state.last_loaded_source = metadata['source']
                st.session_state.last_loaded_files = metadata['file_paths']
                st.success(f"å·²æ¢å¤ä¸Šæ¬¡åŠ è½½çš„æ•°æ®ï¼Œå…±{len(cached_data)}æ¡è®°å½•")
                st.rerun()
        # ====== æ”¯æŒå¤šæ•°æ®æºåŒæ—¶åŠ è½½ ======
        data_sources = st.multiselect("é€‰æ‹©æ•°æ®æ¥æº:", ["CNKI", "WOS"], default=["CNKI"])
        file_type = st.radio("é€‰æ‹©æ–‡ä»¶ç±»å‹:", ["xls", "csv"])
        selected_file_paths = []
        for data_source in data_sources:
            available_file_types = available_files[data_source]
            if available_file_types[file_type]:
                selected_files = st.multiselect(
                    f"é€‰æ‹©è¦åŠ è½½çš„{data_source}æ–‡ä»¶:",
                    [os.path.basename(f) for f in available_file_types[file_type]],
                    key=f"{data_source}_{file_type}_selection"
                )
                selected_file_paths += [
                    os.path.join(os.path.dirname(f), s)
                    for f in available_file_types[file_type]
                    for s in selected_files
                    if os.path.basename(f) == s
                ]
        if selected_file_paths and st.button("åŠ è½½é€‰ä¸­çš„æ–‡ä»¶"):
            try:
                with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
                    dfs = []
                    for data_source in data_sources:
                        ds_files = [f for f in selected_file_paths if data_source in f]
                        if ds_files:
                            df = data_loader.load_multiple_files(ds_files, data_source)
                            if not df.empty:
                                dfs.append(df)
                    if dfs:
                        df = pd.concat(dfs, ignore_index=True)
                        st.session_state.loaded_data = df
                        st.session_state.last_loaded_source = ','.join(data_sources)
                        st.session_state.last_loaded_files = selected_file_paths
                        # ä¿å­˜åŠ è½½æ•°æ®åˆ°ç¼“å­˜
                        cache_manager.save_loaded_data(df, ','.join(data_sources), selected_file_paths)
                        st.success(f"æˆåŠŸåŠ è½½ {len(df)} æ¡æ•°æ®ï¼")
                        import time; time.sleep(1)
                        st.rerun()
                    else:
                        st.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®ã€‚")
            except Exception as e:
                st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        else:
            st.info(f"è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ•°æ®æºçš„ {file_type} æ–‡ä»¶ã€‚")
    with col2:
        st.subheader("å·²åŠ è½½çš„æ•°æ®é¢„è§ˆ")
        if st.session_state.loaded_data is not None:
            df = st.session_state.loaded_data
            st.write(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            # ====== æ–°å¢ï¼šæ–¹æ³•é—´é‡å¤æ–‡çŒ®ç»Ÿè®¡è¡¨æ ¼åŠç‹¬ç‰¹æ–‡çŒ®æ•° ======
            if all(col in df.columns for col in ['title', 'method']):
                ml_titles = set(df[df['method'] == 'machine learning']['title'])
                dl_titles = set(df[df['method'] == 'deep learning']['title'])
                llm_titles = set(df[df['method'] == 'LLMs']['title'])
                all_titles = set(df['title'])
                # æ–¹æ³•äº¤å‰
                ml_dl = ml_titles & dl_titles
                dl_llm = dl_titles & llm_titles
                ml_llm = ml_titles & llm_titles
                ml_dl_llm = ml_titles & dl_titles & llm_titles
                # ç‹¬ç‰¹æ–‡çŒ®æ•°ï¼ˆå»é™¤æ‰€æœ‰æ–¹æ³•é—´é‡å¤ï¼‰
                unique_titles = ml_titles | dl_titles | llm_titles
                # ç»Ÿè®¡è¡¨æ ¼
                cross_table = pd.DataFrame({
                    '(MLâˆ©DL)': [len(ml_dl)],
                    '(DLâˆ©LLM)': [len(dl_llm)],
                    '(MLâˆ©LLM)': [len(ml_llm)],
                    '(MLâˆ©DLâˆ©LLM)': [len(ml_dl_llm)]
                }, index=['æ–¹æ³•äº¤å‰æ–‡çŒ®æ•°'])
                st.markdown("**ä¸‰æ–¹æ³•äº¤å‰é‡å¤æ–‡çŒ®ç»Ÿè®¡**")
                st.dataframe(cross_table)
                # æ€»æ–‡çŒ®æ•°ã€ç‹¬ç‰¹æ–‡çŒ®æ•°
                st.markdown(f"**æ‰€æœ‰æ–‡çŒ®æ€»æ•°ï¼š{len(df)}**")
                st.markdown(f"**å»é™¤æ–¹æ³•é—´é‡å¤åçš„ç‹¬ç‰¹æ–‡çŒ®æ•°ï¼š{len(unique_titles)}**")
            # ====== åŸæœ‰æ•°æ®åˆ†å¸ƒä¸é¢„è§ˆ ======
            stats_tab1, stats_tab2 = st.tabs(["æ•°æ®åˆ†å¸ƒ", "æ•°æ®é¢„è§ˆ"])
            with stats_tab1:
                if 'area' in df.columns and 'method' in df.columns:
                    area_counts = df['area'].value_counts().reset_index()
                    area_counts.columns = ['area', 'count']
                    method_counts = df['method'].value_counts().reset_index()
                    method_counts.columns = ['method', 'count']
                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader("æŒ‰é¢†åŸŸåˆ†å¸ƒ")
                        import plotly.express as px
                        fig = px.pie(area_counts, values='count', names='area', title='æŒ‰é¢†åŸŸåˆ†å¸ƒ')
                        st.plotly_chart(fig, use_container_width=True)
                    with col2:
                        st.subheader("æŒ‰æ–¹æ³•åˆ†å¸ƒ")
                        fig = px.pie(method_counts, values='count', names='method', title='æŒ‰æ–¹æ³•åˆ†å¸ƒ')
                        st.plotly_chart(fig, use_container_width=True)
                if 'year' in df.columns:
                    year_counts = df['year'].value_counts().reset_index()
                    year_counts.columns = ['year', 'count']
                    year_counts = year_counts.sort_values('year')
                    st.subheader("æŒ‰å¹´ä»½åˆ†å¸ƒ")
                    fig = px.bar(year_counts, x='year', y='count', title='æŒ‰å¹´ä»½åˆ†å¸ƒ')
                    st.plotly_chart(fig, use_container_width=True)
            with stats_tab2:
                st.dataframe(df.head(10))
                if st.button("éšæœºå±•ç¤ºä¸€æ¡æ•°æ®"):
                    if len(df) > 0:
                        import random
                        random_idx = random.randint(0, len(df) - 1)
                        random_row = df.iloc[random_idx]
                        st.subheader("éšæœºæ•°æ®æ ·ä¾‹")
                        st.markdown(f"**æ ‡é¢˜**: {random_row['title']}")
                        st.markdown(f"**æ‘˜è¦**: {random_row['abstract']}")
                        st.markdown(f"**å¹´ä»½**: {random_row['year']}")
                        st.markdown(f"**é¢†åŸŸ**: {random_row['area']}")
                        st.markdown(f"**æ–¹æ³•**: {random_row['method']}")
                        st.markdown(f"**å¤„ç†é˜¶æ®µ**: {'ç¬¬äºŒé˜¶æ®µ' if random_row.get('stage') == 2 else 'ç¬¬ä¸€é˜¶æ®µ'}")
                    else:
                        st.error("æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•å±•ç¤ºéšæœºæ•°æ®ã€‚")
        else:
            st.info("è¯·å…ˆåŠ è½½æ•°æ®ã€‚")

# ========== å…³é”®è¯ç®¡ç†é¡µé¢ ==========
def render_keywords_management_page():
    st.header("ğŸ”‘ å…³é”®è¯ç®¡ç†")
    cache_manager = CacheManager()
    # å¦‚æœè¿˜æ²¡æœ‰é€‰æ‹©å…³é”®è¯ï¼Œå°è¯•ä»ç¼“å­˜æ¢å¤ä¸Šæ¬¡é€‰æ‹©çš„å…³é”®è¯
    if not st.session_state.selected_keywords:
        if hasattr(cache_manager, 'load_last_keywords'):
            last_keywords = cache_manager.load_last_keywords()
            if last_keywords:
                st.info(f"å‘ç°ä¸Šæ¬¡é€‰æ‹©çš„{len(last_keywords)}ä¸ªå…³é”®è¯")
                if st.button("æ¢å¤ä¸Šæ¬¡é€‰æ‹©çš„å…³é”®è¯"):
                    st.session_state.selected_keywords = last_keywords
                    st.success(f"å·²æ¢å¤ä¸Šæ¬¡é€‰æ‹©çš„{len(last_keywords)}ä¸ªå…³é”®è¯")
                    st.rerun()
    keywords_dict = get_keywords()
    if not keywords_dict:
        st.error("æ— æ³•åŠ è½½å…³é”®è¯ï¼Œè¯·æ£€æŸ¥keywords.pyæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚")
        return
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("å…³é”®è¯é€‰æ‹©")
        categories = list(keywords_dict.keys())
        category = st.selectbox("é€‰æ‹©å…³é”®è¯ç±»åˆ«:", categories)
        if category in keywords_dict:
            keywords = keywords_dict[category]
            st.write(f"å…± {len(keywords)} ä¸ªå…³é”®è¯")
            st.subheader("æ‰¹é‡é€‰æ‹©")
            batch_col1, batch_col2 = st.columns(2)
            with batch_col1:
                select_all = st.checkbox("å…¨é€‰", key=f"select_all_{category}")
            with batch_col2:
                if select_all:
                    st.session_state.to_select_keywords = keywords.copy()
                if st.button("æ‰¹é‡æ·»åŠ ", key=f"batch_add_{category}"):
                    added_count = 0
                    for kw in st.session_state.to_select_keywords:
                        if kw not in st.session_state.selected_keywords:
                            st.session_state.selected_keywords.append(kw)
                            added_count += 1
                    st.session_state.to_select_keywords = []
                    if hasattr(cache_manager, 'save_current_keywords'):
                        cache_manager.save_current_keywords(st.session_state.selected_keywords)
                    if added_count > 0:
                        st.success(f"å·²æ·»åŠ {added_count}ä¸ªå…³é”®è¯")
                        st.rerun()
            st.subheader("å…³é”®è¯åˆ—è¡¨")
            keyword_container = st.container()
            keywords_count = len(keywords)
            num_columns = 4
            rows_per_column = (keywords_count + num_columns - 1) // num_columns
            keyword_controls = []
            for keyword_idx, keyword in enumerate(keywords):
                col_idx = keyword_idx // rows_per_column
                row_idx = keyword_idx % rows_per_column
                is_selected = keyword in st.session_state.selected_keywords
                is_in_batch = keyword in st.session_state.to_select_keywords
                keyword_controls.append((row_idx, col_idx, keyword, is_selected, is_in_batch))
            keyword_controls.sort()
            cols = st.columns(4)
            for row_idx in range(rows_per_column):
                for col_idx in range(num_columns):
                    current_controls = [
                        control for control in keyword_controls
                        if control[0] == row_idx and control[1] == col_idx
                    ]
                    if current_controls:
                        _, _, keyword, is_selected, is_in_batch = current_controls[0]
                        with cols[col_idx]:
                            batch_select = st.checkbox(
                                keyword,
                                value=select_all or is_in_batch,
                                key=f"kw_{category}_{keyword}"
                            )
                            add_disabled = is_selected
                            if st.button(
                                "â•",
                                key=f"add_{category}_{keyword}",
                                disabled=add_disabled
                            ):
                                st.session_state.selected_keywords.append(keyword)
                                if hasattr(cache_manager, 'save_current_keywords'):
                                    cache_manager.save_current_keywords(st.session_state.selected_keywords)
                                st.rerun()
                            if batch_select and keyword not in st.session_state.to_select_keywords:
                                st.session_state.to_select_keywords.append(keyword)
                            elif not batch_select and keyword in st.session_state.to_select_keywords:
                                st.session_state.to_select_keywords.remove(keyword)
        else:
            st.error(f"æ‰¾ä¸åˆ°ç±»åˆ«'{category}'çš„å…³é”®è¯ã€‚")
        # ====== å…³é”®è¯å­é›†ä¸€é”®ä¿å­˜/åŠ è½½ ======
        st.markdown("---")
        st.subheader("å…³é”®è¯å­é›†ä¸€é”®ä¿å­˜/åŠ è½½")
        ml_keywords = keywords_dict.get("machine learning", [])
        dl_keywords = keywords_dict.get("deep learning", [])
        llm_keywords = keywords_dict.get("LLMs", [])
        col_ml, col_dl, col_llm = st.columns(3)
        with col_ml:
            if st.button("ä¿å­˜MLå­é›†"):
                st.session_state.keyword_lists["MLå­é›†"] = ml_keywords
                cache_manager.save_keyword_list("MLå­é›†", ml_keywords)
                st.success("å·²ä¿å­˜MLå­é›†")
            if st.button("åŠ è½½MLå­é›†"):
                st.session_state.selected_keywords = ml_keywords.copy()
                cache_manager.save_current_keywords(ml_keywords)
                st.success("å·²åŠ è½½MLå­é›†")
                st.rerun()
        with col_dl:
            if st.button("ä¿å­˜DLå­é›†"):
                st.session_state.keyword_lists["DLå­é›†"] = dl_keywords
                cache_manager.save_keyword_list("DLå­é›†", dl_keywords)
                st.success("å·²ä¿å­˜DLå­é›†")
            if st.button("åŠ è½½DLå­é›†"):
                st.session_state.selected_keywords = dl_keywords.copy()
                cache_manager.save_current_keywords(dl_keywords)
                st.success("å·²åŠ è½½DLå­é›†")
                st.rerun()
        with col_llm:
            if st.button("ä¿å­˜LLMå­é›†"):
                st.session_state.keyword_lists["LLMå­é›†"] = llm_keywords
                cache_manager.save_keyword_list("LLMå­é›†", llm_keywords)
                st.success("å·²ä¿å­˜LLMå­é›†")
            if st.button("åŠ è½½LLMå­é›†"):
                st.session_state.selected_keywords = llm_keywords.copy()
                cache_manager.save_current_keywords(llm_keywords)
                st.success("å·²åŠ è½½LLMå­é›†")
                st.rerun()
    with col2:
        st.subheader("å·²é€‰å…³é”®è¯")
        num_selected = len(st.session_state.selected_keywords)
        st.write(f"å·²é€‰æ‹© {num_selected} ä¸ªå…³é”®è¯")
        st.subheader("ä¿å­˜å…³é”®è¯åˆ—è¡¨")
        save_container = st.container()
        list_name = save_container.text_input("å…³é”®è¯åˆ—è¡¨åç§°:", placeholder="è¾“å…¥åç§°...")
        save_disabled = not list_name or num_selected == 0
        if save_container.button("ä¿å­˜å…³é”®è¯åˆ—è¡¨", disabled=save_disabled):
            try:
                st.session_state.keyword_lists[list_name] = st.session_state.selected_keywords.copy()
                if hasattr(cache_manager, 'save_keyword_list'):
                    cache_manager.save_keyword_list(list_name, st.session_state.selected_keywords)
                st.success(f"å·²ä¿å­˜å…³é”®è¯åˆ—è¡¨ï¼š{list_name}")
                if hasattr(cache_manager, 'save_current_keywords'):
                    cache_manager.save_current_keywords(st.session_state.selected_keywords)
            except Exception as e:
                st.error(f"ä¿å­˜å…³é”®è¯åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        saved_lists_from_cache = cache_manager.get_all_keyword_lists()
        for name, keywords in saved_lists_from_cache.items():
            if name not in st.session_state.keyword_lists:
                st.session_state.keyword_lists[name] = keywords
        if st.session_state.keyword_lists:
            saved_lists = list(st.session_state.keyword_lists.keys())
            selected_list = st.selectbox("åŠ è½½å·²ä¿å­˜çš„åˆ—è¡¨:", [""] + saved_lists)
            if selected_list and st.button("åŠ è½½åˆ—è¡¨"):
                try:
                    st.session_state.selected_keywords = st.session_state.keyword_lists[selected_list].copy()
                    if hasattr(cache_manager, 'save_current_keywords'):
                        cache_manager.save_current_keywords(st.session_state.selected_keywords)
                    st.success(f"å·²åŠ è½½å…³é”®è¯åˆ—è¡¨ï¼š{selected_list}")
                    st.rerun()
                except Exception as e:
                    st.error(f"åŠ è½½å…³é”®è¯åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}")
        st.subheader("å…³é”®è¯ç®¡ç†")
        delete_container = st.container()
        delete_all = delete_container.checkbox("å…¨é€‰åˆ é™¤")
        if delete_all:
            st.session_state.to_delete_keywords = st.session_state.selected_keywords.copy()
        delete_disabled = len(st.session_state.to_delete_keywords) == 0
        if delete_container.button("æ‰¹é‡åˆ é™¤", disabled=delete_disabled):
            try:
                removed_count = 0
                for kw in st.session_state.to_delete_keywords:
                    if kw in st.session_state.selected_keywords:
                        st.session_state.selected_keywords.remove(kw)
                        removed_count += 1
                st.session_state.to_delete_keywords = []
                if hasattr(cache_manager, 'save_current_keywords'):
                    cache_manager.save_current_keywords(st.session_state.selected_keywords)
                if removed_count > 0:
                    st.success(f"å·²åˆ é™¤{removed_count}ä¸ªå…³é”®è¯")
                    st.rerun()
            except Exception as e:
                st.error(f"åˆ é™¤å…³é”®è¯æ—¶å‡ºé”™: {str(e)}")
        st.subheader("å·²é€‰å…³é”®è¯åˆ—è¡¨")
        if st.session_state.selected_keywords:
            selected_keywords_container = st.container()
            num_selected = len(st.session_state.selected_keywords)
            num_columns = 4
            rows_per_column = (num_selected + num_columns - 1) // num_columns
            keyword_controls = []
            for keyword_idx, keyword in enumerate(st.session_state.selected_keywords):
                col_idx = keyword_idx // rows_per_column
                row_idx = keyword_idx % rows_per_column
                keyword_controls.append((row_idx, col_idx, keyword))
            keyword_controls.sort()
            selected_cols = st.columns(4)
            for row_idx, col_idx, keyword in keyword_controls:
                with selected_cols[col_idx]:
                    is_in_delete_batch = keyword in st.session_state.to_delete_keywords
                    delete_select = st.checkbox(
                        keyword,
                        value=delete_all or is_in_delete_batch,
                        key=f"del_{keyword}"
                    )
                    if st.button("åˆ é™¤", key=f"remove_{keyword}"):
                        st.session_state.selected_keywords.remove(keyword)
                        if hasattr(cache_manager, 'save_current_keywords'):
                            cache_manager.save_current_keywords(st.session_state.selected_keywords)
                        st.rerun()
                    if delete_select and keyword not in st.session_state.to_delete_keywords:
                        st.session_state.to_delete_keywords.append(keyword)
                    elif not delete_select and keyword in st.session_state.to_delete_keywords:
                        st.session_state.to_delete_keywords.remove(keyword)
        else:
            st.info("è¯·å…ˆä»å·¦ä¾§é€‰æ‹©å…³é”®è¯ã€‚")
        st.subheader("æ·»åŠ è‡ªå®šä¹‰å…³é”®è¯")
        custom_container = st.container()
        new_keyword = custom_container.text_input("è¾“å…¥å…³é”®è¯:")
        add_disabled = not new_keyword or new_keyword in st.session_state.selected_keywords
        if custom_container.button("æ·»åŠ è‡ªå®šä¹‰å…³é”®è¯", disabled=add_disabled, key="add_custom"):
            if new_keyword and new_keyword not in st.session_state.selected_keywords:
                st.session_state.selected_keywords.append(new_keyword)
                if hasattr(cache_manager, 'save_current_keywords'):
                    cache_manager.save_current_keywords(st.session_state.selected_keywords)
                st.success(f"å·²æ·»åŠ å…³é”®è¯ï¼š{new_keyword}")
                st.rerun()

# ========== ç»“æœæŸ¥çœ‹é¡µé¢ ==========
def render_results_view_page():
    st.header("ğŸ“‹ ç»“æœæŸ¥çœ‹")
    from utils.cache_manager import CacheManager
    import pandas as pd
    import random
    import time
    
    # åˆå§‹åŒ–ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µç¼“å­˜ç®¡ç†å™¨
    cache_manager = CacheManager()
    stage2_cache_manager = Stage2CacheManager()
    
    # åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ
    if "results_cache" not in st.session_state or st.session_state.results_cache is None:
        st.session_state.results_cache = cache_manager.get_results_by_filter({})
    
    # åŠ è½½ç¬¬äºŒé˜¶æ®µç»“æœ
    if "stage2_results_cache" not in st.session_state or st.session_state.stage2_results_cache is None:
        try:
            # è·å–ç¬¬äºŒé˜¶æ®µç´¢å¼•æ•°æ®
            stage2_index = stage2_cache_manager.get_index()
            
            # å°†ç´¢å¼•æ•°æ®è½¬æ¢ä¸ºä¸ç¬¬ä¸€é˜¶æ®µç¼“å­˜æ ¼å¼å…¼å®¹çš„æ ¼å¼
            stage2_results = []
            
            for item in stage2_index:
                try:
                    cache_key = item.get('cache_key', '')
                    if not cache_key:
                        continue
                        
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    detail = stage2_cache_manager.get_detail(cache_key)
                    if detail:
                        # è½¬æ¢ä¸ºç¬¬ä¸€é˜¶æ®µå…¼å®¹æ ¼å¼
                        metadata = {
                            'title': item.get('title', ''),
                            'abstract': detail.get('paper', {}).get('abstract', ''),
                            'year': item.get('year', ''),
                            'area': item.get('area', ''),
                            'method': item.get('method', ''),
                            'source': item.get('source', ''),
                            'cache_key': cache_key,
                            'timestamp': item.get('timestamp', ''),
                            'stage': 2,  # æ ‡è®°ä¸ºç¬¬äºŒé˜¶æ®µæ•°æ®
                            'stage1_cache_key': item.get('stage1_cache_key', '') or detail.get('paper', {}).get('cache_key', '')  # æ·»åŠ å…³è”åˆ°ç¬¬ä¸€é˜¶æ®µçš„ç¼“å­˜é”®
                        }
                        
                        # è·å–ç¬¬ä¸€é˜¶æ®µçš„å¤„ç†ç»“æœï¼Œä»¥ä¾¿è·å–å…³é”®è¯è§£é‡Š
                        stage1_explanations = {}
                        if 'paper' in detail and 'cache_key' in detail['paper']:
                            stage1_cache_key = detail['paper'].get('cache_key', '')
                            if stage1_cache_key:
                                try:
                                    # ä»ç¬¬ä¸€é˜¶æ®µç¼“å­˜ä¸­è·å–è§£é‡Šå†…å®¹
                                    stage1_result = cache_manager.get_cached_result(stage1_cache_key)
                                    if stage1_result and 'explanations' in stage1_result:
                                        stage1_explanations = stage1_result.get('explanations', {})
                                except Exception:
                                    pass  # å¿½ç•¥è·å–ç¬¬ä¸€é˜¶æ®µè§£é‡Šçš„é”™è¯¯
                        
                        result = {
                            'relevant_keywords': item.get('stage1_keywords', []),
                            'application_domains': item.get('application_domains', []),
                            'justification': detail.get('domain_result', {}).get('justification', ''),
                            'explanations': stage1_explanations,  # æ·»åŠ ä»ç¬¬ä¸€é˜¶æ®µè·å–çš„å…³é”®è¯è§£é‡Š
                            'success': True
                        }
                        
                        stage2_results.append({'metadata': metadata, 'result': result})
                except Exception:
                    continue  # å¿½ç•¥å•ä¸ªé¡¹ç›®çš„å¤„ç†é”™è¯¯
            
            st.session_state.stage2_results_cache = stage2_results
            
        except Exception:
            st.session_state.stage2_results_cache = []
    
    # åˆå¹¶ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µç»“æœ
    all_results = []
    
    # æ·»åŠ ç¬¬ä¸€é˜¶æ®µç»“æœ
    if st.session_state.results_cache:
        # æ ‡è®°ç¬¬ä¸€é˜¶æ®µæ•°æ®
        for item in st.session_state.results_cache:
            if 'metadata' in item:
                item['metadata']['stage'] = 1
        all_results.extend(st.session_state.results_cache)
    
    # æ·»åŠ ç¬¬äºŒé˜¶æ®µç»“æœ
    if st.session_state.stage2_results_cache:
        all_results.extend(st.session_state.stage2_results_cache)
    
    # æå–å…ƒæ•°æ®åˆ—è¡¨ç”¨äºç­›é€‰
    all_items = [r['metadata'] for r in all_results]
    
    if not all_items:
        st.info("æš‚æ— å¤„ç†ç»“æœã€‚è¯·å…ˆåœ¨LLMå¤„ç†é¡µé¢å¤„ç†æ•°æ®ã€‚")
        return

    # ====== æ–°å¢ï¼šæœç´¢æ¡† ======
    search_title = st.text_input("æŒ‰æ ‡é¢˜å…³é”®å­—æœç´¢ï¼š", "")
    col1, col2 = st.columns([1, 3])
    with col1:
        st.subheader("ç­›é€‰æ¡ä»¶")
        areas = sorted(set(item.get('area') for item in all_items if item.get('area')))
        methods = sorted(set(item.get('method') for item in all_items if item.get('method')))
        sources = sorted(set(item.get('source') for item in all_items if item.get('source')))
        selected_area = st.selectbox("é¢†åŸŸ:", ["å…¨éƒ¨"] + areas)
        selected_method = st.selectbox("æ–¹æ³•:", ["å…¨éƒ¨"] + methods)
        selected_source = st.selectbox("æ•°æ®æº:", ["å…¨éƒ¨"] + sources)
        
        # æ·»åŠ å¤„ç†é˜¶æ®µç­›é€‰
        stages = ["å…¨éƒ¨", "ç¬¬ä¸€é˜¶æ®µ", "ç¬¬äºŒé˜¶æ®µ"]
        selected_stage = st.selectbox("å¤„ç†é˜¶æ®µ:", stages)
        
        # æ·»åŠ ç¬¬äºŒé˜¶æ®µé¢†åŸŸåˆ†ç±»ç»“æœç­›é€‰
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„åº”ç”¨é¢†åŸŸå€¼
        all_domains = []
        for result_item in all_results:
            if 'result' in result_item and 'application_domains' in result_item['result']:
                domains = result_item['result'].get('application_domains', [])
                if domains and domains != ["None"]:
                    all_domains.extend(domains)
        # å»é‡å¹¶æ’åº
        unique_domains = sorted(set(all_domains))
        # æ·»åŠ "æ— åº”ç”¨é¢†åŸŸ"é€‰é¡¹
        domain_options = ["å…¨éƒ¨", "æ— åº”ç”¨é¢†åŸŸ"] + unique_domains
        if unique_domains:
            selected_domain = st.selectbox("åº”ç”¨é¢†åŸŸ:", domain_options)
        
        # ====== å…³é”®è¯åŒ¹é…ç»“æœç­›é€‰ ======
        match_filter = st.radio("å…³é”®è¯åŒ¹é…ç»“æœ:", ["å…¨éƒ¨", "æœ‰åŒ¹é…", "æ— åŒ¹é…"])
        annotations = cache_manager.get_all_annotations()
        has_annotations = bool(annotations)
        if has_annotations:
            annotation_filter = st.radio("æ ‡æ³¨çŠ¶æ€:", ["å…¨éƒ¨", "å·²æ ‡æ³¨", "æœªæ ‡æ³¨"])
            
        # æ„å»ºç­›é€‰æ¡ä»¶
        filter_criteria = {}
        if selected_area != "å…¨éƒ¨":
            filter_criteria['area'] = selected_area
        if selected_method != "å…¨éƒ¨":
            filter_criteria['method'] = selected_method
        if selected_source != "å…¨éƒ¨":
            filter_criteria['source'] = selected_source
            
        # å¤„ç†é˜¶æ®µç­›é€‰
        if selected_stage == "ç¬¬ä¸€é˜¶æ®µ":
            filtered_results = [r for r in all_results if r['metadata'].get('stage') == 1]
        elif selected_stage == "ç¬¬äºŒé˜¶æ®µ":
            filtered_results = [r for r in all_results if r['metadata'].get('stage') == 2]
        else:
            filtered_results = all_results.copy()
            
        # æŒ‰å±æ€§ç­›é€‰
        filtered_results = [r for r in filtered_results if all(
            (k not in r['metadata'] or r['metadata'][k] == v) for k, v in filter_criteria.items()
        )]
        
        # æ·»åŠ é¢†åŸŸç­›é€‰
        if 'selected_domain' in locals() and selected_domain != "å…¨éƒ¨":
            if selected_domain == "æ— åº”ç”¨é¢†åŸŸ":
                # ç­›é€‰æ²¡æœ‰åº”ç”¨é¢†åŸŸçš„æ–‡çŒ®
                filtered_results = [r for r in filtered_results if 
                                   'result' not in r or 
                                   'application_domains' not in r['result'] or 
                                   not r['result']['application_domains'] or 
                                   r['result']['application_domains'] == ["None"]]
            else:
                # ç­›é€‰åŒ…å«ç‰¹å®šåº”ç”¨é¢†åŸŸçš„æ–‡çŒ®
                filtered_results = [r for r in filtered_results if 
                                   'result' in r and 
                                   'application_domains' in r['result'] and 
                                   selected_domain in r['result']['application_domains']]
        
        # ====== æ–°å¢ï¼šæŒ‰titleå…³é”®å­—ç­›é€‰ ======
        if search_title.strip():
            filtered_results = [r for r in filtered_results if search_title.strip().lower() in r['metadata'].get('title', '').lower()]
        # å…³é”®è¯åŒ¹é…ç»“æœç­›é€‰
        if match_filter == "æœ‰åŒ¹é…":
            filtered_results = [r for r in filtered_results if r['result'].get('relevant_keywords', [])]
        elif match_filter == "æ— åŒ¹é…":
            filtered_results = [r for r in filtered_results if not r['result'].get('relevant_keywords', [])]
        # æ ‡æ³¨ç­›é€‰
        if has_annotations and annotation_filter != "å…¨éƒ¨":
            if annotation_filter == "å·²æ ‡æ³¨":
                filtered_results = [r for r in filtered_results if r['metadata']['cache_key'] in annotations]
            else:
                filtered_results = [r for r in filtered_results if r['metadata']['cache_key'] not in annotations]
        st.write(f"å…±æ‰¾åˆ° {len(filtered_results)} æ¡ç»“æœ")
        view_col1, view_col2 = st.columns(2)
        with view_col1:
            if st.button("éšæœºæŸ¥çœ‹ä¸€æ¡"):
                if filtered_results:
                    random_idx = random.randint(0, len(filtered_results) - 1)
                    st.session_state.selected_result = filtered_results[random_idx]
                    st.session_state.show_detail_view = True
                    st.rerun()
        with view_col2:
            if st.button("æŸ¥çœ‹åˆ—è¡¨"):
                st.session_state.show_detail_view = False
                st.session_state.selected_result = None
                st.rerun()
        if "to_delete_results" not in st.session_state:
            st.session_state.to_delete_results = []
        st.subheader("æ‰¹é‡æ“ä½œ")
        if len(st.session_state.to_delete_results) > 0:
            st.write(f"å·²é€‰æ‹© {len(st.session_state.to_delete_results)} æ¡ç»“æœå¾…åˆ é™¤")
        if st.button("åˆ é™¤é€‰ä¸­é¡¹", disabled=len(st.session_state.to_delete_results) == 0):
            try:
                delete_count = 0
                stage1_deleted = False
                stage2_deleted = False
                
                # æŸ¥æ‰¾æ¯ä¸ªè¦åˆ é™¤çš„ç¼“å­˜é”®å±äºå“ªä¸ªé˜¶æ®µ
                for cache_key in st.session_state.to_delete_results:
                    # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®é¡¹
                    data_item = next((item for item in all_results if item['metadata'].get('cache_key') == cache_key), None)
                    
                    if data_item:
                        # æ ¹æ®é˜¶æ®µé€‰æ‹©å¯¹åº”çš„ç¼“å­˜ç®¡ç†å™¨
                        if data_item['metadata'].get('stage') == 2:
                            if stage2_cache_manager.delete_result(cache_key):
                                delete_count += 1
                                stage2_deleted = True
                        else:
                            if cache_manager.delete_result(cache_key):
                                delete_count += 1
                                stage1_deleted = True
                
                if delete_count > 0:
                    st.success(f"æˆåŠŸåˆ é™¤{delete_count}æ¡ç»“æœ")
                    st.session_state.to_delete_results = []
                    # æ ¹æ®åˆ é™¤çš„æ•°æ®é˜¶æ®µåˆ·æ–°å¯¹åº”çš„ç¼“å­˜
                    if stage1_deleted:
                        st.session_state.results_cache = None
                    if stage2_deleted:
                        st.session_state.stage2_results_cache = None
                    
                    time.sleep(1)
                    if st.session_state.show_detail_view and st.session_state.selected_result:
                        deleted_key = st.session_state.selected_result['metadata'].get('cache_key', '')
                        if deleted_key in st.session_state.to_delete_results:
                            st.session_state.show_detail_view = False
                            st.session_state.selected_result = None
                    st.rerun()
                else:
                    st.error("åˆ é™¤å¤±è´¥")
            except Exception as e:
                st.error(f"åˆ é™¤ç»“æœæ—¶å‡ºé”™: {str(e)}")
    with col2:
        view_mode = st.radio("æŸ¥çœ‹æ¨¡å¼:", ["è¯¦æƒ…è§†å›¾", "åˆ—è¡¨è§†å›¾"], horizontal=True,
                            index=0 if st.session_state.show_detail_view else 1)
        if st.session_state.show_detail_view != (view_mode == "è¯¦æƒ…è§†å›¾"):
            st.session_state.show_detail_view = (view_mode == "è¯¦æƒ…è§†å›¾")
            if view_mode == "åˆ—è¡¨è§†å›¾":
                st.session_state.selected_result = None
            st.rerun()
        if st.session_state.show_detail_view and st.session_state.selected_result:
            st.subheader("ç»“æœè¯¦æƒ…")
            if st.button("è¿”å›åˆ—è¡¨"):
                st.session_state.show_detail_view = False
                st.session_state.selected_result = None
                st.rerun()
            else:
                selected = st.session_state.selected_result
                metadata = selected['metadata']
                result = selected['result']

                # åŸºæœ¬ä¿¡æ¯å±•ç¤º
                st.markdown("### è®ºæ–‡åŸºæœ¬ä¿¡æ¯")
                st.markdown(f"**æ ‡é¢˜**: {metadata.get('title', '')}")
                with st.expander("æ˜¾ç¤ºå®Œæ•´æ‘˜è¦"):
                    st.markdown(f"{metadata.get('abstract', '')}")
                
                # ä¸¤åˆ—æ˜¾ç¤ºå¹´ä»½ã€é¢†åŸŸã€æ–¹æ³•ç­‰ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown(f"**å¹´ä»½**: {metadata.get('year', '')}")
                    st.markdown(f"**è¯»å–æ—¶é¢†åŸŸ**: {metadata.get('area', '')}")
                    st.markdown(f"**è¯»å–æ—¶æ–¹æ³•**: {metadata.get('method', '')}")
                    st.markdown(f"**æ•°æ®æ¥æº**: {metadata.get('source', '')}")
                    st.markdown(f"**å¤„ç†é˜¶æ®µ**: {'ç¬¬äºŒé˜¶æ®µ' if metadata.get('stage') == 2 else 'ç¬¬ä¸€é˜¶æ®µ'}")
                
                with col2:
                    # æ ¹æ®å…³é”®è¯åŒ¹é…ç¡®å®šé¢†åŸŸå’Œæ–¹æ³•
                    relevant_keywords = result.get('relevant_keywords', [])
                    if relevant_keywords:
                        # å¦‚æœæœ‰å¯¼å…¥keywords.pyï¼Œåˆ™ä½¿ç”¨å®ƒæ¥åˆ†ç±»å…³é”®è¯
                        from utils.llm_processor import get_keywords
                        keywords_dict = get_keywords()
                        
                        # é»˜è®¤ç±»åˆ«
                        match_method = "æœªåŒ¹é…"
                        
                        # æ£€æŸ¥å…³é”®è¯å±äºå“ªä¸ªæ–¹æ³•ç±»åˆ«
                        ml_keywords = set(keywords_dict.get("machine learning", []))
                        dl_keywords = set(keywords_dict.get("deep learning", []))
                        llm_keywords = set(keywords_dict.get("LLMs", []))
                        
                        # ä¸å…³é”®è¯åˆ—è¡¨æ±‚äº¤é›†ï¼Œç¡®å®šæ–¹æ³•åˆ†ç±»
                        relevant_kw_set = set(relevant_keywords)
                        ml_match = relevant_kw_set.intersection(ml_keywords)
                        dl_match = relevant_kw_set.intersection(dl_keywords)
                        llm_match = relevant_kw_set.intersection(llm_keywords)
                        
                        if llm_match:
                            match_method = "LLMs"
                        elif dl_match:
                            match_method = "deep learning"
                        elif ml_match:
                            match_method = "machine learning"
                        
                        st.markdown(f"**åŒ¹é…åæ–¹æ³•**: {match_method}")
                    else:
                        st.markdown("**åŒ¹é…åæ–¹æ³•**: æ— åŒ¹é…å…³é”®è¯")
                    
                    # æ˜¾ç¤ºåŒ¹é…åçš„åº”ç”¨é¢†åŸŸ
                    application_domains = result.get('application_domains', [])
                    if application_domains and application_domains != ["None"]:
                        st.markdown(f"**åŒ¹é…åé¢†åŸŸ**: {', '.join(application_domains)}")
                    else:
                        st.markdown("**åŒ¹é…åé¢†åŸŸ**: æœªåº”ç”¨äºç‰¹å®šé‡‘èé¢†åŸŸ")
                    
                    # æ—¶é—´ä¿¡æ¯
                    st.markdown(f"**å¤„ç†æ—¶é—´**: {metadata.get('timestamp', '').split('T')[0] if 'timestamp' in metadata else 'æœªçŸ¥'}")
                
                # åŒ¹é…çš„å…³é”®è¯å’Œè§£é‡Š
                st.markdown("### å…³é”®è¯åŒ¹é…ç»“æœ")
                relevant_keywords = result.get('relevant_keywords', [])
                
                # ç¡®ä¿relevant_keywordsæ˜¯åˆ—è¡¨ï¼Œå¤„ç†å¯èƒ½çš„JSONå­—ç¬¦ä¸²æƒ…å†µ
                if isinstance(relevant_keywords, str):
                    try:
                        import json
                        relevant_keywords = json.loads(relevant_keywords)
                    except:
                        relevant_keywords = []
                
                # ç¡®ä¿explanationsæ­£ç¡®è·å–ï¼Œå¤„ç†å¯èƒ½çš„JSONå­—ç¬¦ä¸²æƒ…å†µ
                explanations = result.get('explanations', {})
                # å¦‚æœexplanationsæ˜¯JSONå­—ç¬¦ä¸²ï¼Œåˆ™è§£æå®ƒ
                if isinstance(explanations, str):
                    try:
                        import json
                        explanations = json.loads(explanations)
                    except:
                        explanations = {}
                
                # å¤„ç†ç¬¬äºŒé˜¶æ®µæ•°æ®çš„ç‰¹æ®Šæƒ…å†µ
                if metadata.get('stage') == 2 and (not explanations or len(explanations) == 0):
                    # å°è¯•ä»ç¬¬ä¸€é˜¶æ®µç¼“å­˜è·å–åŸå§‹çš„explanations
                    stage1_cache_key = metadata.get('stage1_cache_key', '')
                    if stage1_cache_key:
                        stage1_result = cache_manager.get_cached_result(stage1_cache_key)
                        if stage1_result:
                            if isinstance(stage1_result, dict) and 'explanations' in stage1_result:
                                explanations = stage1_result.get('explanations', {})
                            elif isinstance(stage1_result, dict) and 'result' in stage1_result and 'explanations' in stage1_result['result']:
                                explanations = stage1_result['result'].get('explanations', {})
                
                if not relevant_keywords:
                    reason = explanations.get('reason', "æœªæä¾›åŸå› ")
                    st.warning(f"**æ— åŒ¹é…å…³é”®è¯**: {reason}")
                else:
                    # åˆ›å»ºè¡¨æ ¼ä»¥å±•ç¤ºå…³é”®è¯å’Œè§£é‡Š
                    keyword_data = []
                    for keyword in relevant_keywords:
                        keyword_type = "æœªçŸ¥"
                        if keyword in ml_keywords:
                            keyword_type = "æœºå™¨å­¦ä¹ "
                        elif keyword in dl_keywords:
                            keyword_type = "æ·±åº¦å­¦ä¹ "
                        elif keyword in llm_keywords:
                            keyword_type = "å¤§è¯­è¨€æ¨¡å‹"
                        
                        explanation = explanations.get(keyword, "")
                        keyword_data.append({
                            "å…³é”®è¯": keyword,
                            "ç±»å‹": keyword_type,
                            "è§£é‡Š": explanation
                        })
                    
                    # æ˜¾ç¤ºå…³é”®è¯è¡¨æ ¼
                    if keyword_data:
                        st.dataframe(pd.DataFrame(keyword_data), use_container_width=True)
                    
                    # åˆ†ç±»æ˜¾ç¤ºå…³é”®è¯
                    matches_by_type = {
                        "æœºå™¨å­¦ä¹ ": [k for k in relevant_keywords if k in ml_keywords],
                        "æ·±åº¦å­¦ä¹ ": [k for k in relevant_keywords if k in dl_keywords],
                        "å¤§è¯­è¨€æ¨¡å‹": [k for k in relevant_keywords if k in llm_keywords],
                        "å…¶ä»–": [k for k in relevant_keywords if k not in ml_keywords and k not in dl_keywords and k not in llm_keywords]
                    }
                    
                    # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
                    for category, keywords in matches_by_type.items():
                        if keywords:
                            with st.expander(f"{category} ({len(keywords)}ä¸ªå…³é”®è¯)"):
                                for kw in keywords:
                                    exp = explanations.get(kw, "æ— è§£é‡Š")
                                    st.markdown(f"**{kw}**: {exp}")
                
                # æ˜¾ç¤ºè®ºæ–‡é¢†åŸŸåˆ†ç±»ä¿¡æ¯
                if "application_domains" in result:
                    st.markdown("### è®ºæ–‡é¢†åŸŸåˆ†ç±»")
                    domains = result.get("application_domains", [])
                    
                    # ç¡®ä¿domainsæ˜¯åˆ—è¡¨ï¼Œå¤„ç†å¯èƒ½çš„JSONå­—ç¬¦ä¸²æƒ…å†µ
                    if isinstance(domains, str):
                        try:
                            import json
                            domains = json.loads(domains)
                        except:
                            domains = []
                    
                    justification = result.get("justification", "æ— è§£é‡Š")
                    
                    if domains and domains != ["None"]:
                        st.success(f"**åº”ç”¨é¢†åŸŸ**: {', '.join(domains)}")
                    else:
                        st.warning("è®ºæ–‡æœªåº”ç”¨äºç‰¹å®šé‡‘èé¢†åŸŸ")
                    
                    # åˆ›å»ºé¢†åŸŸåˆ†ç±»ç»“æœè¡¨æ ¼
                    domain_data = [{
                        "åº”ç”¨é¢†åŸŸ": ', '.join(domains) if domains and domains != ["None"] else "æœªåº”ç”¨äºç‰¹å®šé‡‘èé¢†åŸŸ",
                        "åˆ¤æ–­ç†ç”±": justification
                    }]
                    
                    # æ˜¾ç¤ºé¢†åŸŸåˆ†ç±»è¡¨æ ¼
                    st.dataframe(pd.DataFrame(domain_data), use_container_width=True)
                    
                    # è¯¦ç»†æ˜¾ç¤ºåˆ¤æ–­ç†ç”±
                    with st.expander("æŸ¥çœ‹è¯¦ç»†åˆ¤æ–­ç†ç”±"):
                        st.markdown(f"{justification}")
                
                # äººå·¥æ ‡æ³¨éƒ¨åˆ†
                st.markdown("### äººå·¥æ ‡æ³¨")
                cache_key = metadata.get('cache_key')
                annotation = cache_manager.get_annotation(cache_key) if cache_key else None
                is_correct = st.radio(
                    "LLMåˆ¤æ–­æ˜¯å¦æ­£ç¡®:",
                    ["æ­£ç¡®", "éƒ¨åˆ†æ­£ç¡®", "ä¸æ­£ç¡®"],
                    index=0 if not annotation else (0 if annotation.get('is_correct') == "æ­£ç¡®" else 1 if annotation.get('is_correct') == "éƒ¨åˆ†æ­£ç¡®" else 2)
                )
                feedback = st.text_area(
                    "æ ‡æ³¨åé¦ˆ:",
                    value="" if not annotation else annotation.get('feedback', ""),
                    height=100
                )
                if st.button("ä¿å­˜æ ‡æ³¨"):
                    try:
                        annotation_data = {
                            "is_correct": is_correct,
                            "feedback": feedback
                        }
                        if cache_manager.save_annotation(cache_key, annotation_data):
                            st.success("æ ‡æ³¨å·²ä¿å­˜ã€‚")
                            if 'annotation_results' not in st.session_state:
                                st.session_state.annotation_results = {}
                            st.session_state.annotation_results[cache_key] = annotation_data
                        else:
                            st.error("ä¿å­˜æ ‡æ³¨æ—¶å‡ºé”™ã€‚")
                    except Exception as e:
                        st.error(f"ä¿å­˜æ ‡æ³¨æ—¶å‡ºé”™: {str(e)}")
        else:
            st.subheader("ç»“æœåˆ—è¡¨")
            if filtered_results:
                results_df = pd.DataFrame([r['metadata'] for r in filtered_results])
                def paginate_dataframe(df, page_key, page_size_key=None):
                    if df.empty:
                        return df, 0, 0, 0, 0
                    page = st.session_state.display_page.get(page_key, 0)
                    page_size = st.session_state.display_page.get("page_size", 10)
                    total_pages = (len(df) + page_size - 1) // page_size
                    page = max(0, min(page, total_pages - 1))
                    start_idx = page * page_size
                    end_idx = min(start_idx + page_size, len(df))
                    return df.iloc[start_idx:end_idx], page, total_pages, start_idx, end_idx
                def render_pagination_controls(page_key, total_pages, current_page):
                    if total_pages <= 1:
                        return
                    col1, col2, col3, col4, col5 = st.columns([1, 1, 3, 1, 1])
                    with col1:
                        if st.button("é¦–é¡µ", key=f"first_{page_key}", disabled=current_page == 0):
                            st.session_state.display_page[page_key] = 0
                            st.rerun()
                    with col2:
                        if st.button("ä¸Šä¸€é¡µ", key=f"prev_{page_key}", disabled=current_page == 0):
                            st.session_state.display_page[page_key] = max(0, current_page - 1)
                            st.rerun()
                    with col3:
                        st.write(f"ç¬¬ {current_page + 1} é¡µï¼Œå…± {total_pages} é¡µ")
                    with col4:
                        if st.button("ä¸‹ä¸€é¡µ", key=f"next_{page_key}", disabled=current_page >= total_pages - 1):
                            st.session_state.display_page[page_key] = min(total_pages - 1, current_page + 1)
                            st.rerun()
                    with col5:
                        if st.button("æœ«é¡µ", key=f"last_{page_key}", disabled=current_page >= total_pages - 1):
                            st.session_state.display_page[page_key] = total_pages - 1
                            st.rerun()
                    jump_col1, jump_col2 = st.columns([3, 1])
                    with jump_col1:
                        jump_page = st.number_input(
                            "è·³è½¬åˆ°é¡µç :", 
                            min_value=1, 
                            max_value=total_pages,
                            value=current_page + 1,
                            key=f"jump_input_{page_key}"
                        )
                    with jump_col2:
                        if st.button("è·³è½¬", key=f"jump_{page_key}"):
                            st.session_state.display_page[page_key] = jump_page - 1
                            st.rerun()
                current_df, current_page, total_pages, start_idx, end_idx = paginate_dataframe(results_df, "results_list")
                current_page_items = [filtered_results[i] for i in range(start_idx, end_idx)]
                st.write(f"ç»“æœ ({len(filtered_results)}æ¡ï¼Œæ˜¾ç¤ºç¬¬ {start_idx+1}-{end_idx} æ¡):")
                render_pagination_controls("results_list", total_pages, current_page)
                select_all = st.checkbox("å…¨é€‰å½“å‰é¡µ", key="select_all_results")
                if select_all:
                    for item in current_page_items:
                        cache_key = item['metadata'].get('cache_key', '')
                        if cache_key and cache_key not in st.session_state.to_delete_results:
                            st.session_state.to_delete_results.append(cache_key)
                for i, item in enumerate(current_page_items):
                    metadata = item['metadata']
                    result = item['result']
                    cache_key = metadata.get('cache_key', '')
                    col1, col2 = st.columns([1, 11])
                    with col1:
                        is_selected = cache_key in st.session_state.to_delete_results
                        if st.checkbox("", value=is_selected or select_all, key=f"select_{cache_key}"):
                            if cache_key not in st.session_state.to_delete_results:
                                st.session_state.to_delete_results.append(cache_key)
                        else:
                            if cache_key in st.session_state.to_delete_results:
                                st.session_state.to_delete_results.remove(cache_key)
                    with col2:
                        with st.expander(f"ç»“æœ {start_idx + i + 1}: {metadata.get('title', '')[:50]}..."):
                            if st.button("æŸ¥çœ‹è¯¦æƒ…", key=f"view_{i}"):
                                st.session_state.selected_result = item
                                st.session_state.show_detail_view = True
                                st.rerun()
                            st.markdown(f"**å¹´ä»½**: {metadata.get('year', '')}")
                            st.markdown(f"**è¯»å–æ—¶é¢†åŸŸ**: {metadata.get('area', '')}")
                            st.markdown(f"**è¯»å–æ—¶æ–¹æ³•**: {metadata.get('method', '')}")
                            
                            # æ·»åŠ åŒ¹é…åçš„æ–¹æ³•å’Œé¢†åŸŸåˆ†ç±»ä¿¡æ¯
                            relevant_keywords = result.get('relevant_keywords', [])
                            
                            # ç¡®å®šåŒ¹é…åçš„æ–¹æ³•åˆ†ç±»
                            if relevant_keywords:
                                # å¯¼å…¥å…³é”®è¯åˆ†ç±»
                                from utils.llm_processor import get_keywords
                                keywords_dict = get_keywords()
                                
                                # è·å–å„æ–¹æ³•çš„å…³é”®è¯é›†åˆ
                                ml_keywords = set(keywords_dict.get("machine learning", []))
                                dl_keywords = set(keywords_dict.get("deep learning", []))
                                llm_keywords = set(keywords_dict.get("LLMs", []))
                                
                                # ä¸å…³é”®è¯åˆ—è¡¨æ±‚äº¤é›†
                                relevant_kw_set = set(relevant_keywords)
                                ml_match = relevant_kw_set.intersection(ml_keywords)
                                dl_match = relevant_kw_set.intersection(dl_keywords)
                                llm_match = relevant_kw_set.intersection(llm_keywords)
                                
                                # ç¡®å®šæ–¹æ³•åˆ†ç±»
                                match_method = "æœªåŒ¹é…"
                                if llm_match:
                                    match_method = "LLMs"
                                elif dl_match:
                                    match_method = "deep learning"
                                elif ml_match:
                                    match_method = "machine learning"
                                
                                st.markdown(f"**åŒ¹é…åæ–¹æ³•**: {match_method}")
                            else:
                                st.markdown("**åŒ¹é…åæ–¹æ³•**: æ— åŒ¹é…å…³é”®è¯")
                            
                            # æ˜¾ç¤ºé¢†åŸŸåˆ†ç±»ä¿¡æ¯
                            if "application_domains" in result:
                                domains = result.get("application_domains", [])
                                if domains and domains != ["None"]:
                                    st.markdown(f"**åº”ç”¨é¢†åŸŸ**: {', '.join(domains)}")
                                else:
                                    st.markdown("**åº”ç”¨é¢†åŸŸ**: æœªåº”ç”¨äºç‰¹å®šé‡‘èé¢†åŸŸ")
                            
                            # æ˜¾ç¤ºå…³é”®è¯åŒ¹é…ä¿¡æ¯
                            if relevant_keywords:
                                st.markdown(f"**ç›¸å…³å…³é”®è¯æ•°é‡**: {len(relevant_keywords)}")
                                max_kw_display = 5  # åœ¨åˆ—è¡¨è§†å›¾ä¸­æœ€å¤šæ˜¾ç¤º5ä¸ªå…³é”®è¯
                                display_kws = relevant_keywords[:max_kw_display]
                                display_text = ', '.join(display_kws)
                                if len(relevant_keywords) > max_kw_display:
                                    display_text += f"... (å…±{len(relevant_keywords)}ä¸ª)"
                                st.markdown(f"**å…³é”®è¯**: {display_text}")
                            else:
                                st.markdown("**æ— åŒ¹é…å…³é”®è¯**")
                            
                            # åˆ é™¤åŠŸèƒ½
                            if st.button("åˆ é™¤", key=f"delete_{i}"):
                                try:
                                    # æ ¹æ®é˜¶æ®µé€‰æ‹©å¯¹åº”çš„ç¼“å­˜ç®¡ç†å™¨
                                    if metadata.get('stage') == 2:
                                        delete_success = stage2_cache_manager.delete_result(cache_key)
                                    else:
                                        delete_success = cache_manager.delete_result(cache_key)
                                        
                                    if delete_success:
                                        st.success("å·²åˆ é™¤")
                                        if cache_key in st.session_state.to_delete_results:
                                            st.session_state.to_delete_results.remove(cache_key)
                                        # åˆ·æ–°ç¼“å­˜
                                        if metadata.get('stage') == 2:
                                            st.session_state.stage2_results_cache = None
                                        else:
                                            st.session_state.results_cache = None
                                        time.sleep(1)
                                        st.rerun()
                                    else:
                                        st.error("åˆ é™¤å¤±è´¥")
                                except Exception as e:
                                    st.error(f"åˆ é™¤ç»“æœæ—¶å‡ºé”™: {str(e)}")
                if not select_all:
                    current_page_keys = [item['metadata'].get('cache_key', '') for item in current_page_items]
                    for cache_key in list(st.session_state.to_delete_results):
                        if cache_key in current_page_keys:
                            checkbox_key = f"select_{cache_key}"
                            if checkbox_key in st.session_state and not st.session_state[checkbox_key]:
                                st.session_state.to_delete_results.remove(cache_key)
                if len(filtered_results) > 10:
                    st.info(f"å½“å‰æ˜¾ç¤ºç¬¬ {start_idx + 1} - {end_idx} æ¡ï¼Œå…± {len(filtered_results)} æ¡ç»“æœ")
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœ")
        # ====== æ–°å¢ï¼šåº•éƒ¨ç»Ÿè®¡è¡¨æ ¼ ======
        if 'loaded_data' in st.session_state and st.session_state.loaded_data is not None:
            df = st.session_state.loaded_data
            ml_titles = set(df[df['method'] == 'machine learning']['title'])
            dl_titles = set(df[df['method'] == 'deep learning']['title'])
            llm_titles = set(df[df['method'] == 'LLMs']['title'])
            all_titles = set(df['title'])
            ml_dl = ml_titles & dl_titles
            dl_llm = dl_titles & llm_titles
            ml_llm = ml_titles & llm_titles
            ml_dl_llm = ml_titles & dl_titles & llm_titles
            unique_titles = ml_titles | dl_titles | llm_titles
            total_loaded = len(df)
            unique_count = len(unique_titles)
            cross_table = pd.DataFrame({
                '(MLâˆ©DL)': [len(ml_dl)],
                '(DLâˆ©LLM)': [len(dl_llm)],
                '(MLâˆ©LLM)': [len(ml_llm)],
                '(MLâˆ©DLâˆ©LLM)': [len(ml_dl_llm)]
            }, index=['æ–¹æ³•äº¤å‰æ–‡çŒ®æ•°'])
            # å¤„ç†å®Œæˆ/æœªå®Œæˆæ•°é‡
            cache_manager = CacheManager()
            processed = cache_manager.get_all_processed_items()
            processed_titles_list = [
                item['metadata'].get('title') if 'metadata' in item else item.get('title')
                for item in processed if ('metadata' in item and item['metadata'].get('title')) or ('title' in item and item.get('title'))
            ]
            processed_titles = set(processed_titles_list)
            processed_count = len(processed_titles & unique_titles)
            unprocessed_count = unique_count - processed_count
            # æ–°å¢ï¼šå¤„ç†ç»“æœç‹¬ç‰¹titleæ•°ã€äº¤é›†titleæ•°ã€é‡å¤titleæ•°
            processed_unique_count = len(processed_titles)
            processed_intersection_count = len(processed_titles & unique_titles)
            processed_duplicate_count = len(processed_titles_list) - processed_unique_count
            stat_table = pd.DataFrame({
                'æ€»åŠ è½½æ•°æ®é‡': [total_loaded],
                'ç‹¬ç‰¹æ–‡çŒ®æ•°': [unique_count],
                'MLâˆ©DL': [len(ml_dl)],
                'DLâˆ©LLM': [len(dl_llm)],
                'MLâˆ©LLM': [len(ml_llm)],
                'MLâˆ©DLâˆ©LLM': [len(ml_dl_llm)],
                'å·²å¤„ç†æ–‡çŒ®æ•°': [processed_count],
                'æœªå¤„ç†æ–‡çŒ®æ•°': [unprocessed_count],
                'å¤„ç†ç»“æœç‹¬ç‰¹titleæ•°': [processed_unique_count],
                'å¤„ç†ç»“æœä¸åŠ è½½æ•°æ®äº¤é›†titleæ•°': [processed_intersection_count],
                'å¤„ç†ç»“æœé‡å¤titleæ•°': [processed_duplicate_count]
            })
            st.markdown("---")
            st.markdown("**æ•°æ®ç»Ÿè®¡æ€»è§ˆ**")
            st.dataframe(stat_table)
            st.info("\n\n**è¯´æ˜ï¼š**\n- ç‹¬ç‰¹æ–‡çŒ®æ•°æ˜¯æŒ‡åŠ è½½æ•°æ®åæ‰€æœ‰MLã€DLã€LLMæ–¹æ³•ä¸‹titleçš„å¹¶é›†ã€‚\n- å¤„ç†ç»“æœæ•°é‡ç»Ÿè®¡çš„æ˜¯æ‰€æœ‰å¤„ç†ç»“æœçš„æ¡æ•°ï¼Œè‹¥åŒä¸€æ–‡çŒ®è¢«ä¸åŒæ–¹æ³•å¤šæ¬¡å¤„ç†æˆ–ç¼“å­˜ä¸­æœ‰å†å²é—ç•™æ•°æ®ï¼Œå¯èƒ½å¯¼è‡´æ•°é‡å¤§äºç‹¬ç‰¹æ–‡çŒ®æ•°ã€‚\n- å¤„ç†ç»“æœç‹¬ç‰¹titleæ•°ç»Ÿè®¡å¤„ç†ç»“æœä¸­titleå»é‡åçš„æ•°é‡ã€‚\n- å¤„ç†ç»“æœä¸åŠ è½½æ•°æ®äº¤é›†titleæ•°ä¸ºä¸¤è€…å…±æœ‰çš„æ–‡çŒ®æ•°ã€‚\n- å¤„ç†ç»“æœé‡å¤titleæ•°ä¸ºå¤„ç†ç»“æœä¸­titleå‡ºç°å¤šæ¬¡çš„æ•°é‡ã€‚\n- è‹¥å‘ç°æ•°é‡ä¸ä¸€è‡´ï¼Œå»ºè®®æ£€æŸ¥å¤„ç†æµç¨‹æ˜¯å¦æœ‰é‡å¤å¤„ç†ã€ç¼“å­˜æœªæ¸…ç†æˆ–æ•°æ®æºä¸ä¸€è‡´ç­‰é—®é¢˜ã€‚\n")
            # ====== æ–°å¢ï¼šå±•ç¤ºé‡å¤titleçš„æˆå¯¹æ•°æ®åŠdeep seekç»“æœ ======
            st.markdown("---")
            st.markdown("**é‡å¤titleå¤„ç†ç»“æœå¯¹æ¯”**")
            # æ„å»ºtitleåˆ°å¤„ç†ç»“æœçš„æ˜ å°„
            title_to_items = defaultdict(list)
            for item in processed:
                title = item['metadata'].get('title') if 'metadata' in item else item.get('title')
                if title:
                    title_to_items[title].append(item)
            # æ‰¾å‡ºé‡å¤title
            repeated_titles = [t for t, items in title_to_items.items() if len(items) > 1]
            if not repeated_titles:
                st.success("æ²¡æœ‰å‘ç°é‡å¤titleçš„å¤„ç†ç»“æœã€‚")
            else:
                # æ·»åŠ å±•å¼€/æ”¶èµ·æ§åˆ¶
                show_all = st.checkbox("å±•å¼€æ˜¾ç¤ºæ‰€æœ‰é‡å¤æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯")
                st.write(f"å‘ç°{len(repeated_titles)}ä¸ªé‡å¤titleçš„æ–‡çŒ®ï¼Œå…±æ¶‰åŠ{sum(len(items) for t, items in title_to_items.items() if len(items) > 1)}æ¡é‡å¤è®°å½•")
                
                for t in repeated_titles:
                    items = title_to_items[t]
                    with st.expander(f"æ ‡é¢˜: {t}", expanded=show_all):
                        # ç›´æ¥å±•ç¤ºæ‰€æœ‰ä¿¡æ¯
                        st.markdown("---")
                        
                        for idx, item in enumerate(items):
                            # ä»å…ƒæ•°æ®ä¸­è·å–cache_key
                            meta = item['metadata'] if 'metadata' in item else item
                            cache_key = meta.get('cache_key', '')
                            
                            # ä½¿ç”¨cache_manager.get_cached_resultæ–¹æ³•è·å–å®Œæ•´ç¼“å­˜ç»“æœ
                            full_result = cache_manager.get_cached_result(cache_key) if cache_key else None
                            
                            # åŸºæœ¬å…ƒæ•°æ®ä¿¡æ¯
                            st.markdown(f"### ç¬¬{idx+1}æ¡æ•°æ®")
                            st.markdown(f"**é¢†åŸŸ**: {meta.get('area','')}, **æ–¹æ³•**: {meta.get('method','')}, **æ•°æ®æº**: {meta.get('source','')}")
                            st.markdown(f"**Cache Key**: `{cache_key}`")
                            
                            # ä»å¤„ç†ç»“æœå’Œç¼“å­˜ä¸­è·å–å…³é”®è¯åŒ¹é…ä¿¡æ¯
                            result = item.get('result', {})
                            relevant_keywords = result.get('relevant_keywords', [])
                            explanations = result.get('explanations', {})
                            
                            # è‹¥æœ‰full_resultï¼Œä¼˜å…ˆä½¿ç”¨å®ƒçš„ä¿¡æ¯
                            if full_result:
                                relevant_keywords = full_result.get('relevant_keywords', relevant_keywords)
                                explanations = full_result.get('explanations', explanations)
                            
                            # æ˜¾ç¤ºåŒ¹é…å…³é”®è¯åŠè§£é‡Š
                            st.markdown("**åŒ¹é…å…³é”®è¯ç»“æœ**:")
                            if relevant_keywords:
                                for kw in relevant_keywords:
                                    explanation = explanations.get(kw, 'æ— è§£é‡Š')
                                    st.markdown(f"- **{kw}**: {explanation}")
                            else:
                                reason = explanations.get('reason', 'æ— åŸå› ')
                                st.markdown(f"- **æ— åŒ¹é…å…³é”®è¯**: {reason}")
                            
                            # æ˜¾ç¤ºå®Œæ•´ç¼“å­˜å†…å®¹(æ”¹ç”¨æŒ‰é’®+ä»£ç å—æ–¹å¼ï¼Œé¿å…åµŒå¥—expander)
                            show_cache = st.checkbox(f"æŸ¥çœ‹å®Œæ•´ç¼“å­˜æ•°æ® #{idx+1}", key=f"show_cache_{t}_{idx}")
                            if show_cache and full_result:
                                st.code(json.dumps(full_result, ensure_ascii=False, indent=2), language="json")
                            
                            if idx < len(items) - 1:  # ä¸æ˜¯æœ€åä¸€é¡¹æ—¶æ·»åŠ åˆ†éš”çº¿
                                st.markdown("---")

# ========== ç»Ÿè®¡åˆ†æé¡µé¢ ==========
def render_statistics_page():
    st.header("ğŸ“ˆ ç»Ÿè®¡åˆ†æ")
    from utils.cache_manager import CacheManager
    import pandas as pd
    import plotly.express as px
    from utils.llm_processor import get_keywords
    import os
    
    # åˆå§‹åŒ–ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µç¼“å­˜ç®¡ç†å™¨
    cache_manager = CacheManager()
    stage2_cache_manager = Stage2CacheManager()
    
    # ç»Ÿè®¡åˆ†ææ•°æ®ç¼“å­˜æ–‡ä»¶è·¯å¾„
    stats_cache_dir = os.path.join("cache", "stats_cache")
    os.makedirs(stats_cache_dir, exist_ok=True)
    stats_cache_file = os.path.join(stats_cache_dir, "stats_dataframe.pkl")
    stats_hash_file = os.path.join(stats_cache_dir, "data_hash.txt")
    
    # è·å–å½“å‰æ•°æ®çš„å“ˆå¸Œå€¼
    def get_current_data_hash():
        # è·å–ç¬¬ä¸€é˜¶æ®µç¼“å­˜æ–‡ä»¶å¤¹çš„ä¿®æ”¹æ—¶é—´
        stage1_mtime = 0
        stage1_cache_dir = os.path.join("cache", "results")
        if os.path.exists(stage1_cache_dir):
            stage1_mtime = os.path.getmtime(stage1_cache_dir)
        
        # è·å–ç¬¬äºŒé˜¶æ®µç¼“å­˜æ–‡ä»¶å¤¹çš„ä¿®æ”¹æ—¶é—´
        stage2_mtime = 0
        stage2_cache_dir = os.path.join("cache", "stage2_results")
        if os.path.exists(stage2_cache_dir):
            stage2_mtime = os.path.getmtime(stage2_cache_dir)
        
        # ç»„åˆæ—¶é—´æˆ³åˆ›å»ºå“ˆå¸Œå€¼
        data_hash = f"{stage1_mtime}_{stage2_mtime}"
        return hashlib.md5(data_hash.encode()).hexdigest()
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    def is_cache_valid():
        if not os.path.exists(stats_cache_file) or not os.path.exists(stats_hash_file):
            return False
        
        # è¯»å–ä¿å­˜çš„å“ˆå¸Œå€¼
        with open(stats_hash_file, 'r') as f:
            saved_hash = f.read().strip()
        
        # æ¯”è¾ƒå½“å‰å“ˆå¸Œå€¼ä¸ä¿å­˜çš„å“ˆå¸Œå€¼
        current_hash = get_current_data_hash()
        return saved_hash == current_hash
    
    # å°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
    @st.cache_data
    def load_cached_data_frame():
        if is_cache_valid():
            try:
                with st.spinner("æ­£åœ¨ä»ç¼“å­˜åŠ è½½æ•°æ®..."):
                    with open(stats_cache_file, 'rb') as f:
                        df = pickle.load(f)
                    st.success("æˆåŠŸä»ç¼“å­˜åŠ è½½æ•°æ®")
                    return df
            except Exception as e:
                st.warning(f"ä»ç¼“å­˜åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
        
        # å¦‚æœç¼“å­˜æ— æ•ˆæˆ–åŠ è½½å¤±è´¥ï¼Œè¿”å›None
        return None
    
    # å°è¯•ä»ç¼“å­˜åŠ è½½æ•°æ®
    df = load_cached_data_frame()
    
    # å¦‚æœç¼“å­˜æ— æ•ˆæˆ–åŠ è½½å¤±è´¥ï¼Œé‡æ–°å¤„ç†æ•°æ®
    if df is None:
        with st.spinner("æ­£åœ¨ä»åŸå§‹æ•°æ®æ„å»ºåˆ†ææ•°æ®..."):
            start_time = time.time()
            
            # æ˜¾ç¤ºæ•°æ®å¤„ç†è¿›åº¦
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # è®¾ç½®è¿›åº¦æ¡æ­¥éª¤
            total_steps = 5
            current_step = 0
            
            def update_progress(step, message):
                nonlocal current_step
                current_step = step
                progress_bar.progress(step / total_steps)
                status_text.text(f"æ­¥éª¤ {step}/{total_steps}: {message}")
            
            # æ­¥éª¤1: åŠ è½½æ•°æ®
            update_progress(1, "æ­£åœ¨åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ...")
            
            # åŠ è½½ç¬¬ä¸€é˜¶æ®µç»“æœ
            if "results_cache" not in st.session_state or st.session_state.results_cache is None:
                st.session_state.results_cache = cache_manager.get_results_by_filter({})
            
            # æ­¥éª¤2: åŠ è½½ç¬¬äºŒé˜¶æ®µç»“æœ
            update_progress(2, "æ­£åœ¨åŠ è½½ç¬¬äºŒé˜¶æ®µç»“æœ...")
            
            # åŠ è½½ç¬¬äºŒé˜¶æ®µç»“æœ
            if "stage2_results_cache" not in st.session_state or st.session_state.stage2_results_cache is None:
                try:
                    # è·å–ç¬¬äºŒé˜¶æ®µç´¢å¼•æ•°æ®
                    stage2_index = stage2_cache_manager.get_index()
                    
                    # å°†ç´¢å¼•æ•°æ®è½¬æ¢ä¸ºä¸ç¬¬ä¸€é˜¶æ®µç¼“å­˜æ ¼å¼å…¼å®¹çš„æ ¼å¼
                    stage2_results = []
                    
                    for item in stage2_index:
                        try:
                            cache_key = item.get('cache_key', '')
                            if not cache_key:
                                continue
                                
                            # è·å–è¯¦ç»†ä¿¡æ¯
                            detail = stage2_cache_manager.get_detail(cache_key)
                            if detail:
                                # è½¬æ¢ä¸ºç¬¬ä¸€é˜¶æ®µå…¼å®¹æ ¼å¼
                                metadata = {
                                    'title': item.get('title', ''),
                                    'abstract': detail.get('paper', {}).get('abstract', ''),
                                    'year': item.get('year', ''),
                                    'area': item.get('area', ''),
                                    'method': item.get('method', ''),
                                    'source': item.get('source', ''),
                                    'cache_key': cache_key,
                                    'timestamp': item.get('timestamp', ''),
                                    'stage': 2,  # æ ‡è®°ä¸ºç¬¬äºŒé˜¶æ®µæ•°æ®
                                    'stage1_cache_key': item.get('stage1_cache_key', '') or detail.get('paper', {}).get('cache_key', '')  # æ·»åŠ å…³è”åˆ°ç¬¬ä¸€é˜¶æ®µçš„ç¼“å­˜é”®
                                }
                                
                                # è·å–ç¬¬ä¸€é˜¶æ®µçš„å¤„ç†ç»“æœï¼Œä»¥ä¾¿è·å–å…³é”®è¯è§£é‡Š
                                stage1_explanations = {}
                                if 'paper' in detail and 'cache_key' in detail['paper']:
                                    stage1_cache_key = detail['paper'].get('cache_key', '')
                                    if stage1_cache_key:
                                        try:
                                            # ä»ç¬¬ä¸€é˜¶æ®µç¼“å­˜ä¸­è·å–è§£é‡Šå†…å®¹
                                            stage1_result = cache_manager.get_cached_result(stage1_cache_key)
                                            if stage1_result and 'explanations' in stage1_result:
                                                stage1_explanations = stage1_result.get('explanations', {})
                                        except Exception:
                                            pass  # å¿½ç•¥è·å–ç¬¬ä¸€é˜¶æ®µè§£é‡Šçš„é”™è¯¯
                                
                                result = {
                                    'relevant_keywords': item.get('stage1_keywords', []),
                                    'application_domains': item.get('application_domains', []),
                                    'justification': detail.get('domain_result', {}).get('justification', ''),
                                    'explanations': stage1_explanations,  # æ·»åŠ ä»ç¬¬ä¸€é˜¶æ®µè·å–çš„å…³é”®è¯è§£é‡Š
                                    'success': True
                                }
                                
                                stage2_results.append({'metadata': metadata, 'result': result})
                        except Exception:
                            continue  # å¿½ç•¥å•ä¸ªé¡¹ç›®çš„å¤„ç†é”™è¯¯
                    
                    st.session_state.stage2_results_cache = stage2_results
                    
                except Exception:
                    st.session_state.stage2_results_cache = []
            
            # æ­¥éª¤3: æ„å»ºDataFrame
            update_progress(3, "æ­£åœ¨æ„å»ºæ•°æ®æ¡†æ¶...")
            
            # åˆå¹¶ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µç»“æœ
            all_results = []
            
            # æ·»åŠ ç¬¬ä¸€é˜¶æ®µç»“æœ
            if st.session_state.results_cache:
                # æ ‡è®°ç¬¬ä¸€é˜¶æ®µæ•°æ®
                for item in st.session_state.results_cache:
                    if 'metadata' in item:
                        item['metadata']['stage'] = 1
                all_results.extend(st.session_state.results_cache)
            
            # æ·»åŠ ç¬¬äºŒé˜¶æ®µç»“æœ
            if st.session_state.stage2_results_cache:
                all_results.extend(st.session_state.stage2_results_cache)
            
            if not all_results:
                st.info("æš‚æ— å¤„ç†ç»“æœã€‚è¯·å…ˆåœ¨LLMå¤„ç†é¡µé¢å¤„ç†æ•°æ®ã€‚")
                progress_bar.empty()
                status_text.empty()
                return
            
            # æ„å»ºDataFrame
            df = pd.DataFrame([{
                **item['metadata'],
                'relevant_keywords': item['result'].get('relevant_keywords', []),
                'application_domains': item['result'].get('application_domains', []),
                'method': item['metadata'].get('method', ''),
                'source': item['metadata'].get('source', ''),
                'area': item['metadata'].get('area', ''),
                'cache_key': item['metadata'].get('cache_key', ''),
                'stage': item['metadata'].get('stage', 1),  # é»˜è®¤ä¸ºç¬¬ä¸€é˜¶æ®µ
            } for item in all_results])
            
            # åªä¿ç•™æœ‰å¹´ä»½çš„æ•°æ®
            df['year'] = pd.to_numeric(df['year'], errors='coerce')
            df = df.dropna(subset=['year'])
            df['year'] = df['year'].astype(int)
            
            # æ­¥éª¤4: è¿›è¡Œå…³é”®è¯åŒ¹é…å’Œåˆ†ç±»
            update_progress(4, "æ­£åœ¨è¿›è¡Œå…³é”®è¯åŒ¹é…å’Œåˆ†ç±»...")
            
            # ====== åŸºäºå…³é”®è¯åŒ¹é…ç»“æœå¯¹æ–‡çŒ®è¿›è¡Œåˆ†ç±» ======
            # è·å–ä¸‰ä¸ªæ–¹æ³•çš„å…³é”®è¯
            keywords_dict = get_keywords()
            ml_keywords = keywords_dict.get("machine learning", [])
            dl_keywords = keywords_dict.get("deep learning", [])
            llm_keywords = keywords_dict.get("LLMs", [])
            
            # ç¡®ä¿relevant_keywordsæ˜¯åˆ—è¡¨
            def ensure_list(value):
                if isinstance(value, str):
                    try:
                        import json
                        return json.loads(value)
                    except:
                        return []
                elif isinstance(value, list):
                    return value
                return []
            
            # é¢„å¤„ç†relevant_keywordsåˆ—ï¼Œç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
            df['relevant_keywords'] = df['relevant_keywords'].apply(ensure_list)
            
            # å°†å…³é”®è¯åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆä»¥åŠ é€ŸåŒ¹é…
            ml_keywords_set = set(ml_keywords)
            dl_keywords_set = set(dl_keywords)
            llm_keywords_set = set(llm_keywords)
            
            # ä¼˜åŒ–å…³é”®è¯åŒ¹é…é€»è¾‘ï¼Œä½¿ç”¨å‘é‡åŒ–æ“ä½œ
            def check_match(keywords_list, target_keywords_set):
                if not keywords_list:
                    return False
                return bool(set(keywords_list).intersection(target_keywords_set))
            
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œè®¡ç®—åŒ¹é…æ ‡è®°
            df['has_match'] = df['relevant_keywords'].apply(lambda x: len(x) > 0)
            df['ml_match'] = df['relevant_keywords'].apply(lambda x: check_match(x, ml_keywords_set) if x else False)
            df['dl_match'] = df['relevant_keywords'].apply(lambda x: check_match(x, dl_keywords_set) if x else False)
            df['llm_match'] = df['relevant_keywords'].apply(lambda x: check_match(x, llm_keywords_set) if x else False)
            
            # ä½¿ç”¨æ¡ä»¶è¡¨è¾¾å¼åˆ›å»ºåŒ¹é…åˆ†ç±»
            df['match_class'] = 'no_match'
            df.loc[df['ml_match'] & ~df['dl_match'] & ~df['llm_match'], 'match_class'] = 'ML'
            df.loc[df['dl_match'] & ~df['llm_match'], 'match_class'] = 'DL'
            df.loc[df['llm_match'], 'match_class'] = 'LLM'
            
            # å¤„ç†application_domainsåˆ—ï¼Œç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
            df['application_domains'] = df['application_domains'].apply(ensure_list)
            
            # æ­¥éª¤5: ç¼“å­˜å¤„ç†ç»“æœ
            update_progress(5, "æ­£åœ¨ç¼“å­˜å¤„ç†ç»“æœ...")
            
            # ç¼“å­˜DataFrameåˆ°ç¡¬ç›˜
            try:
                with open(stats_cache_file, 'wb') as f:
                    pickle.dump(df, f)
                
                # ä¿å­˜å½“å‰æ•°æ®å“ˆå¸Œå€¼
                current_hash = get_current_data_hash()
                with open(stats_hash_file, 'w') as f:
                    f.write(current_hash)
                
                end_time = time.time()
                
                # æ¸…é™¤è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress_bar.empty()
                status_text.empty()
                
                st.success(f"æ•°æ®å¤„ç†å®Œæˆå¹¶å·²ç¼“å­˜åˆ°ç¡¬ç›˜ï¼ˆè€—æ—¶: {end_time - start_time:.2f}ç§’ï¼‰")
            except Exception as e:
                # æ¸…é™¤è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
                progress_bar.empty()
                status_text.empty()
                
                st.warning(f"ç¼“å­˜æ•°æ®åˆ°ç¡¬ç›˜å¤±è´¥: {str(e)}")
    
    # å­˜å‚¨å…¨éƒ¨ç»“æœä»¥ä¾›åç»­ä½¿ç”¨
    all_results = []
    for _, row in df.iterrows():
        metadata = {col: row[col] for col in df.columns if col not in ['relevant_keywords', 'application_domains']}
        result = {
            'relevant_keywords': row['relevant_keywords'],
            'application_domains': row['application_domains'],
        }
        all_results.append({'metadata': metadata, 'result': result})
    
    st.subheader("ç»Ÿè®¡åˆ†æ")
    
    # ====== æ·»åŠ å¤„ç†é˜¶æ®µç»Ÿè®¡ ======
    st.markdown("### å¤„ç†é˜¶æ®µåˆ†å¸ƒ")
    stage_counts = df['stage'].value_counts().reset_index()
    stage_counts.columns = ['é˜¶æ®µ', 'æ•°é‡']
    stage_counts['é˜¶æ®µ'] = stage_counts['é˜¶æ®µ'].map({1: 'ç¬¬ä¸€é˜¶æ®µ', 2: 'ç¬¬äºŒé˜¶æ®µ'})
    st.dataframe(stage_counts)
    
    # ====== æ·»åŠ é¢†åŸŸåˆ†ç±»ç»Ÿè®¡ ======
    st.markdown("### ç¬¬äºŒé˜¶æ®µé¢†åŸŸåˆ†ç±»ç»Ÿè®¡")
    # åªç­›é€‰ç¬¬äºŒé˜¶æ®µçš„æ•°æ®
    stage2_df = df[df['stage'] == 2]
    
    if not stage2_df.empty:
        # å±•å¼€åº”ç”¨é¢†åŸŸåˆ—è¡¨ï¼Œä»¥ä¾¿ç»Ÿè®¡
        domain_counts = {}
        no_domain_count = 0  # ç»Ÿè®¡æ— åº”ç”¨é¢†åŸŸçš„æ•°é‡
        
        for domains in stage2_df['application_domains']:
            # ç¡®ä¿domainsæ˜¯åˆ—è¡¨ï¼Œå¤„ç†å¯èƒ½çš„JSONå­—ç¬¦ä¸²æƒ…å†µ
            if isinstance(domains, str):
                try:
                    import json
                    domains = json.loads(domains)
                except:
                    domains = []
                    
            if isinstance(domains, list):
                if not domains or domains == ["None"]:
                    no_domain_count += 1  # å¢åŠ æ— åº”ç”¨é¢†åŸŸè®¡æ•°
                else:
                    for domain in domains:
                        if domain != "None":
                            domain_counts[domain] = domain_counts.get(domain, 0) + 1
        
        if domain_counts or no_domain_count > 0:
            # æ·»åŠ "æ— åº”ç”¨é¢†åŸŸ"åˆ°ç»Ÿè®¡ä¸­
            domain_data = list(domain_counts.items())
            if no_domain_count > 0:
                domain_data.append(("æ— åº”ç”¨é¢†åŸŸ", no_domain_count))
                
            domain_df = pd.DataFrame({
                'åº”ç”¨é¢†åŸŸ': [item[0] for item in domain_data],
                'æ•°é‡': [item[1] for item in domain_data]
            }).sort_values('æ•°é‡', ascending=False)
            
            # æ˜¾ç¤ºé¢†åŸŸåˆ†ç±»ç»Ÿè®¡
            st.dataframe(domain_df)
            
            # ç»˜åˆ¶é¥¼å›¾
            fig = px.pie(domain_df, values='æ•°é‡', names='åº”ç”¨é¢†åŸŸ', title='ç¬¬äºŒé˜¶æ®µé¢†åŸŸåˆ†ç±»åˆ†å¸ƒ')
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("ç¬¬äºŒé˜¶æ®µæ•°æ®ä¸­æ²¡æœ‰é¢†åŸŸåˆ†ç±»ä¿¡æ¯")
    else:
        st.info("æ²¡æœ‰ç¬¬äºŒé˜¶æ®µçš„å¤„ç†æ•°æ®")
        
    # ====== ä¿®æ”¹å¹´åº¦å…³é”®è¯åŒ¹é…ç»Ÿè®¡ï¼Œä½¿ç”¨åŒ¹é…åçš„æ•°æ® ======
    def yearly_match_stats(df, source):
        # åªä¿ç•™ç¬¬ä¸€é˜¶æ®µçš„æ•°æ® å¹¶ç­›é€‰æº
        df_filtered = df[(df['stage'] == 1) & (df['source'] == source)]
        
        # å»é™¤åŒä¸€è®ºæ–‡å¤šæ¬¡å¤„ç†çš„æƒ…å†µï¼ˆåŸºäºtitleå»é‡ï¼‰
        df_filtered = df_filtered.drop_duplicates(subset=['title'], keep='first')
        
        if df_filtered.empty:
            return pd.DataFrame()
            
        # æŒ‰å¹´ä»½åˆ†ç»„ç»Ÿè®¡
        years = sorted(df_filtered['year'].unique())
        stats = []
        
        for year in years:
            year_df = df_filtered[df_filtered['year'] == year]
            
            # ä½¿ç”¨å‘é‡åŒ–æ“ä½œç»Ÿè®¡å„ç±»åˆ«æ•°é‡
            ml_count = len(year_df[year_df['match_class'] == 'ML'])
            dl_count = len(year_df[year_df['match_class'] == 'DL'])
            llm_count = len(year_df[year_df['match_class'] == 'LLM'])
            no_match_count = len(year_df[year_df['match_class'] == 'no_match'])
            total = ml_count + dl_count + llm_count + no_match_count
            
            stats.append({
                'year': year,
                'ML': ml_count,
                'DL': dl_count,
                'LLM': llm_count,
                'æ— åŒ¹é…': no_match_count,
                'æ€»è®¡': total
            })
        
        # æ·»åŠ æ€»è®¡è¡Œ
        if stats:
            total_row = {'year': 'æ€»è®¡'}
            for col in ['ML', 'DL', 'LLM', 'æ— åŒ¹é…', 'æ€»è®¡']:
                total_row[col] = sum(row[col] for row in stats)
            stats.append(total_row)
            
        return pd.DataFrame(stats)
    
    # å¹´åº¦é¢†åŸŸåŒ¹é…ç»Ÿè®¡ï¼ˆç¬¬äºŒé˜¶æ®µæ•°æ®ï¼‰
    def yearly_domain_stats(df):
        # åªä¿ç•™ç¬¬äºŒé˜¶æ®µçš„æ•°æ®
        stage2_df = df[df['stage'] == 2]
        
        # å»é™¤åŒä¸€è®ºæ–‡å¤šæ¬¡å¤„ç†çš„æƒ…å†µï¼ˆåŸºäºtitleå»é‡ï¼‰
        stage2_df = stage2_df.drop_duplicates(subset=['title'], keep='first')
        
        if stage2_df.empty:
            return pd.DataFrame()
        
        # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„åº”ç”¨é¢†åŸŸå€¼
        all_domains = set()
        for domains in stage2_df['application_domains']:
            if domains and domains != ["None"]:
                all_domains.update(domains)
        
        # å»é‡å¹¶æ’åº
        unique_domains = sorted(all_domains)
        
        if not unique_domains:
            return pd.DataFrame()
        
        # åˆ›å»ºå¹´åº¦é¢†åŸŸç»Ÿè®¡
        years = sorted(stage2_df['year'].unique())
        stats = []
        
        for year in years:
            year_df = stage2_df[stage2_df['year'] == year]
            row = {'year': year}
            
            # ä½¿ç”¨DataFrameæ“ä½œç»Ÿè®¡é¢†åŸŸåœ¨è¯¥å¹´ä»½çš„åˆ†å¸ƒ
            for domain in unique_domains:
                # ç»Ÿè®¡æ¯å¹´ä¸­æœ‰ç‰¹å®šé¢†åŸŸçš„æ–‡ç« æ•°
                domain_count = sum(1 for domains in year_df['application_domains'] 
                                 if domains and domains != ["None"] and domain in domains)
                row[domain] = domain_count
            
            # è®¡ç®—æ€»è®¡
            row['æ€»è®¡'] = sum(row[domain] for domain in unique_domains)
            stats.append(row)
        
        # æ·»åŠ æ€»è®¡è¡Œ
        if stats:
            total_row = {'year': 'æ€»è®¡'}
            for domain in unique_domains:
                total_row[domain] = sum(row[domain] for row in stats)
            total_row['æ€»è®¡'] = sum(total_row[domain] for domain in unique_domains)
            stats.append(total_row)
        
        return pd.DataFrame(stats)
    
    st.markdown("---")
    
    # ç¬¬ä¸€é˜¶æ®µå¹´åº¦å…³é”®è¯åŒ¹é…ç»Ÿè®¡
    st.markdown("### ç¬¬ä¸€é˜¶æ®µå¹´åº¦å…³é”®è¯åŒ¹é…ç»Ÿè®¡ï¼ˆå»é‡ï¼‰")
    
    for src in ['CNKI', 'WOS']:
        st.markdown(f"#### {src} å¹´åº¦å…³é”®è¯åŒ¹é…ç»Ÿè®¡")
        src_df = df[df['source'] == src]
        if not src_df.empty:
            stats_df = yearly_match_stats(src_df, src)
            st.dataframe(stats_df)
        else:
            st.info(f"æ²¡æœ‰ {src} æ¥æºçš„æ•°æ®ã€‚")
    
    # ç¬¬äºŒé˜¶æ®µå¹´åº¦é¢†åŸŸåŒ¹é…ç»Ÿè®¡
    st.markdown("---")
    st.markdown("### ç¬¬äºŒé˜¶æ®µå¹´åº¦é¢†åŸŸåŒ¹é…ç»Ÿè®¡ï¼ˆå»é‡ï¼‰")
    
    for src in ['CNKI', 'WOS']:
        st.markdown(f"#### {src} å¹´åº¦é¢†åŸŸåŒ¹é…ç»Ÿè®¡")
        src_df = df[df['source'] == src]
        if not src_df.empty:
            stats_df = yearly_domain_stats(src_df)
            if not stats_df.empty:
                st.dataframe(stats_df)
            else:
                st.info(f"æ²¡æœ‰ {src} çš„ç¬¬äºŒé˜¶æ®µé¢†åŸŸåŒ¹é…æ•°æ®ã€‚")
        else:
            st.info(f"æ²¡æœ‰ {src} æ¥æºçš„æ•°æ®ã€‚")
    
    # ====== æ·»åŠ å¤šç»´åº¦è¶‹åŠ¿åˆ†æ ======
    st.markdown("---")
    st.markdown("### å¤šç»´åº¦è¶‹åŠ¿åˆ†æ")
    
    # åªç­›é€‰ç¬¬äºŒé˜¶æ®µæ•°æ®
    trend_df = df[df['stage'] == 2].copy()
    
    # åªä¿ç•™æœ‰åº”ç”¨é¢†åŸŸçš„æ•°æ®ï¼ˆæ’é™¤é¢†åŸŸä¸ºç©ºæˆ–ä¸ºNoneçš„æ•°æ®ï¼‰
    trend_df = trend_df[trend_df['application_domains'].apply(lambda x: bool(x) and x != ["None"])]
    
    if trend_df.empty:
        st.warning("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ç¬¬äºŒé˜¶æ®µæ•°æ®ç”¨äºè¶‹åŠ¿åˆ†æ")
    else:
        # å®šä¹‰æŠ€æœ¯æ–¹æ³•çš„é¢œè‰²å’Œæ ·å¼
        method_props = {
            "machine learning": {
                "color": '#E74C3C',   # æœºå™¨å­¦ä¹  çº¢è‰²
                "marker": "circle",
                "line": "solid",
                "name": "æœºå™¨å­¦ä¹ "
            },
            "deep learning": {
                "color": '#2980B9',   # ç¥ç»ç½‘ç»œ è“è‰²
                "marker": "square",
                "line": "dash",
                "name": "æ·±åº¦å­¦ä¹ "
            },
            "LLMs": {
                "color": '#27AE60',   # å¤§è¯­è¨€æ¨¡å‹ ç»¿è‰²
                "marker": "diamond",
                "line": "dot",
                "name": "å¤§è¯­è¨€æ¨¡å‹"
            }
        }
        
        # ç¡®å®šæ‰€æœ‰å¯èƒ½çš„å¹´ä»½èŒƒå›´
        min_year = int(trend_df['year'].min())
        max_year = int(trend_df['year'].max())
        years_range = list(range(min_year, max_year + 1))
        
        # æ·»åŠ æ•°æ®ç­›é€‰æ§ä»¶
        st.markdown("#### æ•°æ®ç­›é€‰")
        
        # æŒ‰æ¥æºã€åº”ç”¨é¢†åŸŸå’ŒæŠ€æœ¯æ–¹æ³•è¿›è¡Œç­›é€‰
        filter_col1, filter_col2, filter_col3 = st.columns(3)
        
        with filter_col1:
            # è·å–æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ¥æº
            all_sources = sorted(trend_df['source'].unique().tolist())
            selected_sources = st.multiselect(
                "æ•°æ®æ¥æº",
                all_sources,
                default=all_sources,
                key="trend_sources"
            )
        
        with filter_col2:
            # è·å–æ‰€æœ‰å¯èƒ½çš„æŠ€æœ¯æ–¹æ³•
            all_methods = sorted(trend_df['method'].unique().tolist())
            selected_methods = st.multiselect(
                "æŠ€æœ¯æ–¹æ³•",
                all_methods,
                default=all_methods,
                key="trend_methods"
            )
        
        with filter_col3:
            # è·å–æ‰€æœ‰å¯èƒ½çš„åº”ç”¨é¢†åŸŸ
            all_domains = set()
            for domains in trend_df['application_domains']:
                if domains and domains != ["None"]:
                    all_domains.update(domains)
            all_domains = sorted(all_domains)
            
            # é»˜è®¤é€‰æ‹©å‰3ä¸ªåº”ç”¨é¢†åŸŸï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            default_domains = all_domains[:3] if len(all_domains) >= 3 else all_domains
            
            selected_domains = st.multiselect(
                "åº”ç”¨é¢†åŸŸ",
                all_domains,
                default=default_domains,
                key="trend_domains"
            )
        
        # æ·»åŠ å›¾è¡¨æ˜¾ç¤ºè®¾ç½®ï¼ˆç§»åˆ°ç­›é€‰ä¹‹åï¼Œå›¾è¡¨ç”Ÿæˆä¹‹å‰ï¼‰
        st.markdown("#### å›¾è¡¨æ˜¾ç¤ºè®¾ç½®")
        chart_col1, chart_col2 = st.columns(2)
        
        with chart_col1:
            font_size = st.slider(
                "å­—ä½“å¤§å°",
                min_value=10,
                max_value=100,
                value=50,
                step=5,
                key="trend_chart_font_size",
                help="è°ƒæ•´å›¾è¡¨ä¸­æ‰€æœ‰æ–‡å­—çš„å¤§å°"
            )
        
        with chart_col2:
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯æ˜¾ç¤ºå½“å‰å­—ä½“å¤§å°
            st.info(f"å½“å‰å­—ä½“å¤§å°: {font_size}px")
            # æ·»åŠ å­—ä½“æ•ˆæœé¢„è§ˆ
            st.markdown(f'<p style="font-size: {font_size}px; margin: 0;">å­—ä½“æ•ˆæœé¢„è§ˆ</p>', unsafe_allow_html=True)
            # æ·»åŠ åˆ·æ–°æŒ‰é’®
            if st.button("ğŸ”„ åˆ·æ–°å›¾è¡¨", 
                        help="å¦‚æœå­—ä½“å¤§å°æ²¡æœ‰æ›´æ–°ï¼Œç‚¹å‡»æ­¤æŒ‰é’®åˆ·æ–°å›¾è¡¨",
                        key=f"refresh_chart_{font_size}"):
                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                if hasattr(st, '_component_cache'):
                    st._component_cache.clear()
                st.rerun()
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if selected_sources and selected_methods and selected_domains:
            # ç­›é€‰æ•°æ®æºå’ŒæŠ€æœ¯æ–¹æ³•
            filtered_df = trend_df[
                trend_df['source'].isin(selected_sources) & 
                trend_df['method'].isin(selected_methods)
            ]
            
            # ç­›é€‰åº”ç”¨é¢†åŸŸï¼ˆä¸€ç¯‡è®ºæ–‡å¯èƒ½å¯¹åº”å¤šä¸ªé¢†åŸŸï¼‰
            filtered_df = filtered_df[filtered_df['application_domains'].apply(
                lambda domains: any(domain in selected_domains for domain in domains)
            )]
            
            # æ˜¾ç¤ºç­›é€‰åçš„æ•°æ®æ•°é‡
            st.info(f"ç­›é€‰åå…±æœ‰ {len(filtered_df)} æ¡æ•°æ®")
        else:
            filtered_df = pd.DataFrame()  # åˆ›å»ºç©ºDataFrame
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ•°æ®æ¥æºã€æŠ€æœ¯æ–¹æ³•å’Œåº”ç”¨é¢†åŸŸ")
        
        # åˆ›å»ºå¹´ä»½èŒƒå›´æ»‘å—æ§ä»¶
        st.markdown("#### è®¾ç½®æŠ€æœ¯æ–¹æ³•çš„å¹´ä»½èŒƒå›´")
        year_controls = {}
        
        # ä¸ºæ¯ç§æ–¹æ³•æ·»åŠ å¹´ä»½èŒƒå›´æ»‘å—
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**æœºå™¨å­¦ä¹ **")
            ml_min_year = st.slider(
                "èµ·å§‹å¹´ä»½",
                min_value=min_year,
                max_value=max_year,
                value=min_year,
                key="ml_min_year"
            )
            ml_max_year = st.slider(
                "ç»“æŸå¹´ä»½",
                min_value=ml_min_year,
                max_value=max_year,
                value=max_year,
                key="ml_max_year"
            )
            year_controls["machine learning"] = (ml_min_year, ml_max_year)
        
        with col2:
            st.markdown("**æ·±åº¦å­¦ä¹ **")
            dl_min_year = st.slider(
                "èµ·å§‹å¹´ä»½",
                min_value=min_year,
                max_value=max_year,
                value=min_year,
                key="dl_min_year"
            )
            dl_max_year = st.slider(
                "ç»“æŸå¹´ä»½",
                min_value=dl_min_year,
                max_value=max_year,
                value=max_year,
                key="dl_max_year"
            )
            year_controls["deep learning"] = (dl_min_year, dl_max_year)
        
        with col3:
            st.markdown("**å¤§è¯­è¨€æ¨¡å‹**")
            llm_min_year = st.slider(
                "èµ·å§‹å¹´ä»½",
                min_value=min_year,
                max_value=max_year,
                value=min_year,
                key="llm_min_year"
            )
            llm_max_year = st.slider(
                "ç»“æŸå¹´ä»½",
                min_value=llm_min_year,
                max_value=max_year,
                value=max_year,
                key="llm_max_year"
            )
            year_controls["LLMs"] = (llm_min_year, llm_max_year)
        
        if not filtered_df.empty:    
            # æŒ‰æŠ€æœ¯æ–¹æ³•ç»Ÿè®¡å„å¹´åº¦è®ºæ–‡æ•°é‡
            method_data = {}
            for method in method_props.keys():
                if method not in selected_methods:
                    continue  # è·³è¿‡æœªè¢«é€‰æ‹©çš„æ–¹æ³•
                    
                min_year, max_year = year_controls[method]
                # ç­›é€‰æ–¹æ³•å’Œå¹´ä»½èŒƒå›´å†…çš„æ•°æ®
                method_df = filtered_df[(filtered_df['method'] == method) & 
                                       (filtered_df['year'] >= min_year) & 
                                       (filtered_df['year'] <= max_year)]
                
                # æŒ‰å¹´ä»½åˆ†ç»„ç»Ÿè®¡æ•°é‡
                if not method_df.empty:
                    year_counts = method_df.groupby('year').size().reset_index(name='count')
                    # ç¡®ä¿æ‰€æœ‰å¹´ä»½éƒ½æœ‰æ•°æ®ï¼ˆå¡«å……ç¼ºå¤±å¹´ä»½ä¸º0ï¼‰
                    full_years = pd.DataFrame({'year': range(min_year, max_year + 1)})
                    year_counts = pd.merge(full_years, year_counts, on='year', how='left').fillna(0)
                    # æŒ‰å¹´ä»½æ’åº
                    year_counts = year_counts.sort_values('year')
                    
                    # è®¡ç®—ç´¯è®¡æ•°é‡
                    year_counts['cumulative_count'] = year_counts['count'].cumsum()
                    
                    # è½¬æ¢ä¸ºå­—å…¸å½¢å¼ï¼Œæ–¹ä¾¿åç»­å¤„ç†
                    method_data[method] = {
                        'years': year_counts['year'].tolist(),
                        'counts': year_counts['cumulative_count'].tolist()  # ä½¿ç”¨ç´¯è®¡æ•°é‡
                    }
            
            # è£å‰ªæ•°æ®ï¼Œå»é™¤å‰é¢å…¨æ˜¯0çš„å¹´ä»½
            for method, data in method_data.items():
                if data['counts']:  # ç¡®ä¿æœ‰æ•°æ®
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªéé›¶å€¼çš„ç´¢å¼•
                    first_non_zero = next((i for i, count in enumerate(data['counts']) if count > 0), len(data['counts']))
                    # è£å‰ªæ•°æ®
                    if first_non_zero > 0:
                        data['years'] = data['years'][first_non_zero:]
                        data['counts'] = data['counts'][first_non_zero:]
                    
                    # å¯¹æ•°åˆ»åº¦å¤„ç†ï¼šç¡®ä¿æ‰€æœ‰å€¼éƒ½å¤§äº0ï¼ˆæ›¿æ¢0ä¸ºä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼‰
                    data['counts'] = [max(count, 0.1) for count in data['counts']]
            
            # åˆ›å»ºè¶‹åŠ¿å›¾
            if any(data['counts'] for method, data in method_data.items() if data['counts']):
                import plotly.graph_objects as go
                from scipy import interpolate
                import numpy as np
                
                # åˆ›å»ºå›¾è¡¨
                fig = go.Figure()
                
                # å¯¹æ¯ç§æ–¹æ³•æ·»åŠ æŠ˜çº¿
                for method, data in method_data.items():
                    if not data['counts']:
                        continue  # è·³è¿‡æ²¡æœ‰æ•°æ®çš„æ–¹æ³•
                    
                    years = data['years']
                    counts = data['counts']
                    props = method_props[method]
                    
                    # å¦‚æœæ•°æ®ç‚¹æ•°é‡è¶³å¤Ÿï¼Œä½¿ç”¨æ ·æ¡æ’å€¼ç”Ÿæˆå¹³æ»‘æ›²çº¿
                    if len(years) > 2:
                        try:
                            # åˆ›å»ºæ’å€¼å‡½æ•°
                            x_array = np.array(years)
                            y_array = np.array(counts)
                            
                            # å¯¹å¹´ä»½èŒƒå›´åˆ›å»ºæ›´åŠ å¯†é›†çš„ç‚¹ï¼Œä»¥å®ç°å¹³æ»‘æ•ˆæœ
                            x_dense = np.linspace(min(years), max(years), 100)
                            
                            # ä½¿ç”¨æ ·æ¡æ’å€¼ - ä¿®æ”¹å‚æ•°ä»¥é¿å… "m > k must hold" é”™è¯¯
                            # æ ¹æ®æ•°æ®ç‚¹æ•°é‡ç¡®å®šæ ·æ¡çš„é˜¶æ•°(k)
                            # æ ·æ¡çš„é˜¶æ•°kå¿…é¡»æ»¡è¶³ æ•°æ®ç‚¹æ•°é‡ m > k
                            k = min(3, len(years) - 1)  # é»˜è®¤ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡ï¼Œä½†å¦‚æœæ•°æ®ç‚¹å¤ªå°‘åˆ™é™ä½é˜¶æ•°
                            
                            if len(x_array) != len(set(x_array)):
                                # å¦‚æœæœ‰é‡å¤çš„xå€¼ï¼Œä½¿ç”¨æ›´ç®€å•çš„çº¿æ€§æ’å€¼
                                y_smooth = np.interp(x_dense, x_array, y_array)
                            else:
                                # ä½¿ç”¨æ ·æ¡æ’å€¼ï¼Œsæ˜¯å¹³æ»‘å‚æ•°ï¼Œåœ¨æ•°æ®ç‚¹è¾ƒå°‘æ—¶å¢åŠ å¹³æ»‘åº¦
                                s = 0 if len(years) > 4 else 0.1  # æ•°æ®ç‚¹è¾ƒå°‘æ—¶å¢åŠ å¹³æ»‘åº¦
                                tck = interpolate.splrep(x_array, y_array, k=k, s=s)
                                y_smooth = interpolate.splev(x_dense, tck, der=0)
                            
                            # ç¡®ä¿æ’å€¼åçš„yå€¼ä¸ä¸ºè´Ÿ
                            y_smooth = np.maximum(y_smooth, 0)
                            
                            # æ·»åŠ å¹³æ»‘æ›²çº¿
                            fig.add_trace(go.Scatter(
                                x=x_dense, 
                                y=y_smooth,
                                mode='lines',
                                name=props['name'],
                                line=dict(
                                    color=props['color'],
                                    dash=props['line'],
                                    width=3,
                                    shape='spline'
                                ),
                                hovertemplate='%{x}å¹´: %{y:.0f}ç¯‡ç´¯è®¡è®ºæ–‡<extra></extra>'
                            ))
                        except Exception as e:
                            # å¦‚æœæ’å€¼å¤±è´¥ï¼Œç›´æ¥è¿æ¥åŸå§‹æ•°æ®ç‚¹
                            st.warning(f"{props['name']}çš„æ ·æ¡æ’å€¼å¤±è´¥: {str(e)}ï¼Œå°†ä½¿ç”¨ç®€å•è¿çº¿ã€‚")
                            fig.add_trace(go.Scatter(
                                x=years, 
                                y=counts,
                                mode='lines',
                                name=props['name'],
                                line=dict(
                                    color=props['color'],
                                    dash=props['line'],
                                    width=3
                                ),
                                hovertemplate='%{x}å¹´: %{y:.0f}ç¯‡ç´¯è®¡è®ºæ–‡<extra></extra>'
                            ))
                    else:
                        # æ•°æ®ç‚¹å¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œæ ·æ¡æ’å€¼ï¼Œç›´æ¥è¿æ¥åŸå§‹ç‚¹
                        fig.add_trace(go.Scatter(
                            x=years, 
                            y=counts,
                            mode='lines',
                            name=props['name'],
                            line=dict(
                                color=props['color'],
                                dash=props['line'],
                                width=3
                            ),
                            hovertemplate='%{x}å¹´: %{y:.0f}ç¯‡ç´¯è®¡è®ºæ–‡<extra></extra>'
                        ))
                    
                    # æ·»åŠ åŸå§‹æ•°æ®ç‚¹
                    fig.add_trace(go.Scatter(
                        x=years, 
                        y=counts,
                        mode='markers',
                        name=f"{props['name']} (åŸå§‹æ•°æ®)",
                        marker=dict(
                            color=props['color'],
                            size=10,
                            symbol=props['marker'],
                            line=dict(width=2, color='white')
                        ),
                        showlegend=False,
                        hovertemplate='%{x}å¹´: %{y}ç¯‡ç´¯è®¡è®ºæ–‡<extra></extra>'
                    ))
                
                # è®¾ç½®å›¾è¡¨å¸ƒå±€
                title_text = "æŠ€æœ¯æ–¹æ³•å‘å±•è¶‹åŠ¿"
                if len(selected_sources) < len(all_sources):
                    sources_str = ", ".join(selected_sources)
                    title_text += f"ï¼ˆæ•°æ®æ¥æº: {sources_str}ï¼‰"
                
                fig.update_layout(
                    # ç§»é™¤æ ‡é¢˜
                    # title=title_text,
                    xaxis_title="å‘è¡¨å¹´ä»½",
                    xaxis=dict(
                        title=dict(font=dict(size=font_size)),  # è®¾ç½®Xè½´æ ‡é¢˜å­—ä½“
                        tickfont=dict(size=font_size)           # è®¾ç½®Xè½´åˆ»åº¦å­—ä½“
                    ),
                    yaxis=dict(
                        type="log",  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦
                        title="ç´¯è®¡è®ºæ–‡æ•°é‡ (å¯¹æ•°åˆ»åº¦)",
                        title_font=dict(size=font_size),        # è®¾ç½®Yè½´æ ‡é¢˜å­—ä½“
                        tickfont=dict(size=font_size),          # è®¾ç½®Yè½´åˆ»åº¦å­—ä½“
                        showgrid=True,  # æ˜¾ç¤ºç½‘æ ¼çº¿
                        gridwidth=1,    # ç½‘æ ¼çº¿å®½åº¦
                        gridcolor='rgba(200, 200, 200, 0.3)',  # ç½‘æ ¼çº¿é¢œè‰²
                        exponentformat="none",  # ä¸ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•
                        tickmode="auto",
                        nticks=10,      # åˆ»åº¦æ•°é‡
                        tickformat=",d"  # æ•°å­—æ ¼å¼åŒ–ï¼Œæ·»åŠ åƒä½åˆ†éš”ç¬¦
                    ),
                    legend=dict(
                        orientation="h",
                        yanchor="top",
                        y=1.1,
                        xanchor="center",
                        x=0.5,
                        font=dict(size=font_size),              # è®¾ç½®å›¾ä¾‹å­—ä½“
                        title=dict(
                            font=dict(
                                size=font_size  # ä½¿ç”¨æ»‘å—æ§åˆ¶çš„å­—ä½“å¤§å°
                            )
                        )
                    ),
                    legend_title="",
                    height=600,
                    # æé«˜å›¾è¡¨åˆ†è¾¨ç‡ï¼Œä»¥å¯¼å‡ºé«˜æ¸…å›¾ç‰‡
                    width=600,  # è®¾ç½®è¾ƒå¤§çš„å®½åº¦
                    hovermode="x unified",
                    hoverlabel=dict(
                        font_size=font_size,                    # è®¾ç½®æ‚¬æµ®æç¤ºå­—ä½“å¤§å°
                        font_family="Arial"
                    ),
                    # å¢åŠ å›¾åƒè´¨é‡
                    template="plotly_white",  # ä½¿ç”¨é«˜è´¨é‡ç™½è‰²æ¨¡æ¿
                    font=dict(
                        family="Arial, sans-serif",
                        size=font_size  # ä½¿ç”¨æ»‘å—æ§åˆ¶çš„å­—ä½“å¤§å°
                    ),
                    margin=dict(l=80, r=80, t=50, b=80)  # å¢åŠ è¾¹è·
                )
                
                # ç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒçš„é…ç½®
                high_res_config = {
                    "toImageButtonOptions": {
                        "format": "png",  # å›¾åƒæ ¼å¼
                        "filename": "æŠ€æœ¯æ–¹æ³•è¶‹åŠ¿åˆ†æ",
                        "height": 800,  # é«˜åˆ†è¾¨ç‡é«˜åº¦
                        "width": 800,   # é«˜åˆ†è¾¨ç‡å®½åº¦
                        "scale": 2       # ç¼©æ”¾å› å­ (æ›´é«˜ = æ›´æ¸…æ™°)
                    },
                    "displaylogo": False,  # ç§»é™¤Plotly logo
                    "modeBarButtonsToAdd": ["downloadImage"]  # çªå‡ºæ˜¾ç¤ºä¸‹è½½æŒ‰é’®
                }
                
                # æ˜¾ç¤ºé«˜åˆ†è¾¨ç‡å›¾è¡¨ï¼ˆåŒæ—¶æä¾›ä¸‹è½½åŠŸèƒ½ï¼‰
                st.markdown(f"### æŠ€æœ¯æ–¹æ³•è¶‹åŠ¿åˆ†æå›¾ (å½“å‰å­—ä½“å¤§å°: {font_size}px)")
                st.caption("ğŸ’¡ æç¤ºï¼šå¯ä»¥é€šè¿‡ä¸Šæ–¹çš„å­—ä½“å¤§å°æ»‘å—è°ƒæ•´å›¾è¡¨æ–‡å­—å¤§å°ï¼Œç‚¹å‡»å³ä¸Šè§’çš„ç›¸æœºå›¾æ ‡ä¸‹è½½é«˜æ¸…PNGå›¾ç‰‡")
                st.plotly_chart(
                    fig, 
                    use_container_width=True, 
                    config=high_res_config,
                    key=f"trend_chart_font_{font_size}"  # ä½¿ç”¨å­—ä½“å¤§å°ä½œä¸ºkeyå¼ºåˆ¶é‡æ–°æ¸²æŸ“
                )
                
                # æ·»åŠ å¯¹æ•°åˆ»åº¦è¯´æ˜
                st.caption("æ³¨ï¼šYè½´ä½¿ç”¨å¯¹æ•°åˆ»åº¦ï¼Œå¯ä»¥æ›´å¥½åœ°å±•ç¤ºä¸åŒæŠ€æœ¯æ–¹æ³•çš„å¢é•¿è¶‹åŠ¿ï¼Œç‰¹åˆ«æ˜¯å½“æ•°é‡å·®å¼‚è¾ƒå¤§æ—¶ã€‚")
                
                # æ˜¾ç¤ºåŸå§‹æ•°æ®è¡¨æ ¼
                with st.expander("æŸ¥çœ‹åŸå§‹æ•°æ®"):
                    # åˆå¹¶æ‰€æœ‰æ–¹æ³•çš„æ•°æ®åˆ°ä¸€ä¸ªDataFrame
                    all_data = []
                    for method, data in method_data.items():
                        if data['years'] and data['counts']:
                            for year, count in zip(data['years'], data['counts']):
                                all_data.append({
                                    'year': year,
                                    'method': method_props[method]['name'],
                                    'count': count
                                })
                    
                    if all_data:
                        data_df = pd.DataFrame(all_data)
                        # åˆ›å»ºé€è§†è¡¨
                        pivot_df = data_df.pivot(index='year', columns='method', values='count').fillna(0)
                        
                        # æ˜¾ç¤ºè¡¨æ ¼
                        st.markdown("**å„æŠ€æœ¯æ–¹æ³•æŒ‰å¹´åº¦ç´¯è®¡å‘è¡¨è®ºæ–‡æ•°é‡**")
                        st.dataframe(pivot_df)
                        
                        # æä¾›ä¸‹è½½é“¾æ¥
                        csv_data = pivot_df.to_csv()
                        st.download_button(
                            label="ä¸‹è½½CSVæ ¼å¼æ•°æ®",
                            data=csv_data,
                            file_name="method_cumulative_trends.csv",
                            mime="text/csv"
                        )
                    else:
                        st.info("æ²¡æœ‰æ•°æ®å¯æ˜¾ç¤º")
            else:
                st.warning("æ‰€é€‰æ¡ä»¶ä¸‹æ²¡æœ‰è¶‹åŠ¿æ•°æ®")
        else:
            st.warning("è¯·é€‰æ‹©ç­›é€‰æ¡ä»¶ä»¥æ˜¾ç¤ºè¶‹åŠ¿å›¾")
    
    # æ·»åŠ ç¼“å­˜ç®¡ç†æŒ‰é’®
    st.markdown("---")
    with st.expander("ç¼“å­˜ç®¡ç†"):
        st.info("å½“å‰ç»Ÿè®¡æ•°æ®å·²ç¼“å­˜åˆ°ç¡¬ç›˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æŒ‰é’®æ‰‹åŠ¨ç®¡ç†ç¼“å­˜")
        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("æŸ¥çœ‹ç¼“å­˜çŠ¶æ€"):
                if os.path.exists(stats_cache_file):
                    cache_size = os.path.getsize(stats_cache_file) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
                    cache_time = datetime.fromtimestamp(os.path.getmtime(stats_cache_file))
                    current_hash = get_current_data_hash()
                    with open(stats_hash_file, 'r') as f:
                        saved_hash = f.read().strip()
                    is_valid = saved_hash == current_hash
                    
                    status_msg = f"ç¼“å­˜æ–‡ä»¶å­˜åœ¨\nå¤§å°: {cache_size:.2f}MB\nåˆ›å»ºæ—¶é—´: {cache_time}\nç¼“å­˜çŠ¶æ€: {'æœ‰æ•ˆ' if is_valid else 'éœ€è¦æ›´æ–°'}"
                    if is_valid:
                        st.success(status_msg)
                    else:
                        st.warning(status_msg)
                else:
                    st.warning("ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨")
        with col2:
            if st.button("å¼ºåˆ¶æ›´æ–°ç¼“å­˜"):
                # ä¸»åŠ¨æ¸…é™¤ç¼“å­˜æ–‡ä»¶
                try:
                    if os.path.exists(stats_cache_file):
                        os.remove(stats_cache_file)
                    if os.path.exists(stats_hash_file):
                        os.remove(stats_hash_file)
                    # æ¸…é™¤Streamlitç¼“å­˜
                    st.cache_data.clear()
                    st.success("ç¼“å­˜å·²æ¸…é™¤ï¼Œæ­£åœ¨é‡æ–°ç”Ÿæˆ...")
                    st.rerun()  # é‡æ–°åŠ è½½é¡µé¢ä»¥è§¦å‘ç¼“å­˜ç”Ÿæˆ
                except Exception as e:
                    st.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")
        with col3:
            if st.button("å¼ºåˆ¶æ¸…é™¤ç¼“å­˜"):
                try:
                    if os.path.exists(stats_cache_file):
                        os.remove(stats_cache_file)
                    if os.path.exists(stats_hash_file):
                        os.remove(stats_hash_file)
                    st.success("ç¼“å­˜å·²æ¸…é™¤ï¼Œä¸‹æ¬¡è®¿é—®å°†é‡æ–°ç”Ÿæˆ")
                    # æ¸…é™¤Streamlitç¼“å­˜
                    st.cache_data.clear()
                except Exception as e:
                    st.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {str(e)}")
        
        # æ·»åŠ ç¼“å­˜è¯´æ˜
        st.markdown("""
        **ç¼“å­˜è¯´æ˜ï¼š**
        - ç»Ÿè®¡åˆ†ææ•°æ®ä¼šè‡ªåŠ¨ç¼“å­˜åˆ°ç¡¬ç›˜ï¼Œæé«˜é¡µé¢åŠ è½½é€Ÿåº¦
        - åªæœ‰åœ¨åŸå§‹æ•°æ®æœ‰æ›´æ–°æ—¶æ‰ä¼šé‡æ–°ç”Ÿæˆç¼“å­˜
        - å¦‚éœ€æ‰‹åŠ¨æ›´æ–°ç¼“å­˜ï¼Œè¯·ç‚¹å‡»"å¼ºåˆ¶æ›´æ–°ç¼“å­˜"æŒ‰é’®
        - å¦‚éœ€ä¸´æ—¶ç¦ç”¨ç¼“å­˜ï¼Œè¯·ç‚¹å‡»"å¼ºåˆ¶æ¸…é™¤ç¼“å­˜"æŒ‰é’®
        """)
        
        # æ˜¾ç¤ºæœ€åä¸€æ¬¡æ•°æ®å¤„ç†æ—¶é—´ï¼ˆå¦‚æœæœ‰ç¼“å­˜ï¼‰
        if os.path.exists(stats_cache_file):
            last_process_time = datetime.fromtimestamp(os.path.getmtime(stats_cache_file))
            st.info(f"æœ€åæ•°æ®å¤„ç†æ—¶é—´: {last_process_time}")
            # æ˜¾ç¤ºå½“å‰æ•°æ®ç»Ÿè®¡è§„æ¨¡
            if 'df' in locals() and isinstance(df, pd.DataFrame):
                st.info(f"å½“å‰ç»Ÿè®¡æ•°æ®åŒ…å« {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} ä¸ªç‰¹å¾")
                # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒç»Ÿè®¡
                st.write("æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
                st.write(f"- ç¬¬ä¸€é˜¶æ®µæ•°æ®: {len(df[df['stage'] == 1])} æ¡")
                st.write(f"- ç¬¬äºŒé˜¶æ®µæ•°æ®: {len(df[df['stage'] == 2])} æ¡")
                st.write(f"- æ•°æ®æºåˆ†å¸ƒ: {df['source'].value_counts().to_dict()}")
                st.write(f"- æ–¹æ³•åˆ†å¸ƒ: {df['method'].value_counts().to_dict()}")
                st.write(f"- å¹´ä»½èŒƒå›´: {df['year'].min()} - {df['year'].max()}")
    
    # ====== æ·»åŠ æ•°æ®å¯¼å‡ºåŠŸèƒ½ ======
    st.markdown("---")
    st.subheader("æ•°æ®å¯¼å‡º")
    export_col1, export_col2 = st.columns(2)
    
    with export_col1:
        # å¯¼å‡ºç´¢å¼•æ•°æ®
        if st.button("å¯¼å‡ºç´¢å¼•æ•°æ®"):
            # åˆ›å»ºåŒ…å«ä¸¤ä¸ªé˜¶æ®µæ•°æ®çš„DataFrame
            export_df = pd.DataFrame([{
                **item['metadata'],
                'stage': 'ç¬¬äºŒé˜¶æ®µ' if item['metadata'].get('stage') == 2 else 'ç¬¬ä¸€é˜¶æ®µ',
                'relevant_keywords': ', '.join(ensure_list(item['result'].get('relevant_keywords', []))),
                'application_domains': ', '.join(ensure_list(item['result'].get('application_domains', [])))
            } for item in all_results])
            
            if not export_df.empty:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"paper_analysis_index_{timestamp}.csv"
                csv_data = export_df.to_csv(index=False)
                
                st.download_button(
                    label="ä¸‹è½½CSVæ–‡ä»¶",
                    data=csv_data,
                    file_name=filename,
                    mime="text/csv"
                )
            else:
                st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
    
    with export_col2:
        # å¯¼å‡ºå®Œæ•´ç»“æœ
        include_raw = st.checkbox("åŒ…å«åŸå§‹å“åº”", value=False)
        stage_to_export = st.radio("é€‰æ‹©å¯¼å‡ºé˜¶æ®µ:", ["å…¨éƒ¨", "ä»…ç¬¬ä¸€é˜¶æ®µ", "ä»…ç¬¬äºŒé˜¶æ®µ"])
        
        if st.button("å¯¼å‡ºå®Œæ•´ç»“æœ"):
            # æ ¹æ®é€‰æ‹©çš„é˜¶æ®µç­›é€‰æ•°æ®
            if stage_to_export == "ä»…ç¬¬ä¸€é˜¶æ®µ":
                export_results = [r for r in all_results if r['metadata'].get('stage') == 1]
            elif stage_to_export == "ä»…ç¬¬äºŒé˜¶æ®µ":
                export_results = [r for r in all_results if r['metadata'].get('stage') == 2]
            else:
                export_results = all_results
                
            if export_results:
                export_data = []
                for item in export_results:
                    export_item = {
                        **item['metadata'],
                        'stage': 'ç¬¬äºŒé˜¶æ®µ' if item['metadata'].get('stage') == 2 else 'ç¬¬ä¸€é˜¶æ®µ',
                        'relevant_keywords': ', '.join(ensure_list(item['result'].get('relevant_keywords', []))),
                        'application_domains': ', '.join(ensure_list(item['result'].get('application_domains', []))),
                        'justification': item['result'].get('justification', '')
                    }
                    
                    # å¦‚æœåŒ…å«åŸå§‹å“åº”
                    if include_raw and 'raw_response' in item['result']:
                        export_item['raw_response'] = item['result']['raw_response']
                    
                    export_data.append(export_item)
                
                export_df = pd.DataFrame(export_data)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"paper_analysis_full_{timestamp}.csv"
                csv_data = export_df.to_csv(index=False)
                
                st.download_button(
                    label="ä¸‹è½½å®Œæ•´ç»“æœCSV",
                    data=csv_data,
                    file_name=filename,
                    mime="text/csv"
                )
            else:
                st.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")

# ========== ä¸»å…¥å£ ==========
def main():
    # åˆå§‹åŒ–æ‰€æœ‰SessionçŠ¶æ€ï¼Œé˜²æ­¢æœªåˆå§‹åŒ–æŠ¥é”™
    if "to_select_keywords" not in st.session_state:
        st.session_state.to_select_keywords = []
    if "to_delete_keywords" not in st.session_state:
        st.session_state.to_delete_keywords = []
    if "keyword_lists" not in st.session_state:
        st.session_state.keyword_lists = {}
    if "selected_keywords" not in st.session_state:
        st.session_state.selected_keywords = []
    if "loaded_data" not in st.session_state:
        st.session_state.loaded_data = None
    if "show_detail_view" not in st.session_state:
        st.session_state.show_detail_view = False
    if "selected_result" not in st.session_state:
        st.session_state.selected_result = None
    if "last_loaded_source" not in st.session_state:
        st.session_state.last_loaded_source = ""
    if "last_loaded_files" not in st.session_state:
        st.session_state.last_loaded_files = []
    if "to_delete_results" not in st.session_state:
        st.session_state.to_delete_results = []
    if "confirm_clear_cache" not in st.session_state:
        st.session_state.confirm_clear_cache = False
    if "is_processing" not in st.session_state:
        st.session_state.is_processing = False
    if "processing_queue" not in st.session_state:
        st.session_state.processing_queue = []
    if "processed_items" not in st.session_state:
        st.session_state.processed_items = []
    if "display_page" not in st.session_state:
        st.session_state.display_page = {
            "unprocessed": 0,
            "processed": 0,
            "processing": 0,
            "results_list": 0,
            "cached": 0,
            "page_size": 10
        }
    if "annotation_results" not in st.session_state:
        st.session_state.annotation_results = {}
    if "system_prompt" not in st.session_state:
        st.session_state.system_prompt = ""
    if "user_prompt_template" not in st.session_state:
        st.session_state.user_prompt_template = ""
    if "prompt_examples" not in st.session_state:
        st.session_state.prompt_examples = []
    if "api_key" not in st.session_state:
        st.session_state.api_key = ""
    if "last_session_time" not in st.session_state:
        st.session_state.last_session_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    if "filter_no_match" not in st.session_state:
        st.session_state.filter_no_match = False
    if "deduplication" not in st.session_state:
        st.session_state.deduplication = True
    st.title("ğŸ“š ç»“æœæŸ¥çœ‹ä¸æ•°æ®åˆ†æ")
    page = st.sidebar.radio(
        "å¯¼èˆª",
        ["ğŸ“Š æ•°æ®åŠ è½½", "ğŸ”‘ å…³é”®è¯ç®¡ç†", "ğŸ“‹ ç»“æœæŸ¥çœ‹", "ğŸ“ˆ ç»Ÿè®¡åˆ†æ"]
    )
    if page == "ğŸ“Š æ•°æ®åŠ è½½":
        render_data_loading_page()
    elif page == "ğŸ”‘ å…³é”®è¯ç®¡ç†":
        render_keywords_management_page()
    elif page == "ğŸ“‹ ç»“æœæŸ¥çœ‹":
        render_results_view_page()
    elif page == "ğŸ“ˆ ç»Ÿè®¡åˆ†æ":
        render_statistics_page()

if __name__ == "__main__":
    main()
