{
  "paper": {
    "id": 695,
    "title": "Collaborative Evolutionary Reinforcement Learning",
    "abstract": "Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyperparameters. One reason is that most approaches use a noisy version of their operating policy to explore - thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners - typically proven algorithms like TD3 - optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient - notably solving the Mujoco Humanoid benchmark where all of its composite learners (TD3) fail entirely in isolation.",
    "year": 2019,
    "source": "WOS",
    "area": "portfolio",
    "method": "machine learning",
    "cache_key": "c26c86cb5ba1935add917c3880bdb81b",
    "relevant_keywords": [
      "machine learning",
      "reinforcement learning",
      "deep learning",
      "neural network",
      "representation learning"
    ],
    "stage1_timestamp": "2025-05-15T00:47:18.451138"
  },
  "domain_result": {
    "success": true,
    "application_domains": [
      "None"
    ],
    "justification": "该论文主要介绍了一种名为协作进化强化学习（CERL）的框架，用于解决连续控制任务中的探索和利用问题。虽然强化学习在金融领域有潜在应用，但论文中并未提及任何具体的金融应用场景或实验设计，因此无法确定其核心技术是否应用于衍生品定价、金融风险管理或投资组合管理。",
    "raw_response": "```json\n{\n  \"application_domains\": [\"None\"],\n  \"justification\": \"该论文主要介绍了一种名为协作进化强化学习（CERL）的框架，用于解决连续控制任务中的探索和利用问题。虽然强化学习在金融领域有潜在应用，但论文中并未提及任何具体的金融应用场景或实验设计，因此无法确定其核心技术是否应用于衍生品定价、金融风险管理或投资组合管理。\"\n}\n```"
  },
  "timestamp": "2025-05-20T16:28:28.602659"
}