{
  "paper": {
    "id": 63,
    "title": "Continuous control with Stacked Deep Dynamic Recurrent Reinforcement Learning for portfolio optimization",
    "abstract": "Recurrent reinforcement learning (RRL) techniques have been used to optimize asset trading systems and have achieved outstanding results. However, the majority of the previous work has been dedicated to systems with discrete action spaces. To address the challenge of continuous action and multi-dimensional state spaces, we propose the so called Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) architecture to construct a real-time optimal portfolio. The algorithm captures the up-to-date market conditions and rebalances the portfolio accordingly. Under this general vision, Sharpe ratio, which is one of the most widely accepted measures of risk-adjusted returns, has been used as a performance metric. Additionally, the performance of most machine learning algorithms highly depends on their hyperparameter settings. Therefore, we equipped SDDRRL with the ability to find the best possible architecture topology using an automated Gaussian Process (GP) with Expected Improvement (El) as an acquisition function. This allows us to select the best architectures that maximizes the total return while respecting the cardinality constraints. Finally, our system was trained and tested in an online manner for 20 successive rounds with data for ten selected stocks from different sectors of the S&P 500 from January 1st, 2013 to July 31st, 2017. The experiments reveal that the proposed SDDRRL achieves superior performance compared to three benchmarks: the rolling horizon Mean-Variance Optimization (MVO) model, the rolling horizon risk parity model, and the uniform buy-and-hold (UBAH) index. (C) 2019 Elsevier Ltd. All rights reserved.",
    "year": 2020,
    "source": "WOS",
    "area": "portfolio",
    "method": "machine learning",
    "cache_key": "3a3ce4b0ae79ae7c35364066866faf5f",
    "relevant_keywords": [
      "machine learning",
      "reinforcement learning",
      "deep learning",
      "neural network",
      "recurrent neural network",
      "long short-term memory",
      "LSTM"
    ],
    "stage1_timestamp": "2025-05-15T00:38:56.978687"
  },
  "domain_result": {
    "success": true,
    "application_domains": [
      "Portfolio Management"
    ],
    "justification": "该论文的核心技术是使用Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) 架构来构建实时最优投资组合，并通过Sharpe比率作为性能指标来优化投资组合的表现。论文明确提到其目标是解决连续动作和多维状态空间下的投资组合优化问题，并通过实验与多种投资组合优化基准进行比较。因此，该论文的主要应用领域是投资组合管理。",
    "raw_response": "```json\n{\n  \"application_domains\": [\"投资组合管理\"],\n  \"justification\": \"该论文的核心技术是使用Stacked Deep Dynamic Recurrent Reinforcement Learning (SDDRRL) 架构来构建实时最优投资组合，并通过Sharpe比率作为性能指标来优化投资组合的表现。论文明确提到其目标是解决连续动作和多维状态空间下的投资组合优化问题，并通过实验与多种投资组合优化基准进行比较。因此，该论文的主要应用领域是投资组合管理。\"\n}\n```"
  },
  "timestamp": "2025-05-20T16:33:57.213067"
}