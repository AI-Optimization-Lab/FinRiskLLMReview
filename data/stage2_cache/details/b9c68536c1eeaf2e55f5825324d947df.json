{
  "paper": {
    "id": 3163,
    "title": "SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks",
    "abstract": "Spiking neural networks (SNNs) are expected to be part of the future AI portfolio, with heavy investment from industry and government, e.g., IBMTrueNorth, Intel Loihi. While Artificial Neural Network (ANN) architectures have taken large strides, few works have targeted SNN hardware efficiency. Our analysis of SNN baselines shows that at modest spike rates, SNN implementations exhibit significantly lower efficiency than accelerators for ANNs. This is primarily because SNN dataflows must consider neuron potentials for several ticks, introducing a new data structure and a new dimension to the reuse pattern. We introduce a novel SNN architecture, SpinalFlow, that processes a compressed, time-stamped, sorted sequence of input spikes. It adopts an ordering of computations such that the outputs of a network layer are also compressed, time-stamped, and sorted. All relevant computations for a neuron are performed in consecutive steps to eliminate neuron potential storage overheads. Thus, with better data reuse, we advance the energy efficiency of SNN accelerators by an order of magnitude. Even though the temporal aspect in SNNs prevents the exploitation of some reuse patterns that are more easily exploited in ANNs, at 4-bit input resolution and 90% input sparsity, SpinalFlow reduces average energy by 1.8x, compared to a 4-bit Eyeriss baseline. These improvements are seen for a range of networks and sparsity/resolution levels; SpinalFlow consumes 5x less energy and 5.4x less time than an 8-bit version of Eyeriss. We thus show that, depending on the level of observed sparsity, SNN architectures can be competitive with ANN architectures in terms of latency and energy for inference, thus lowering the barrier for practical deployment in scenarios demanding real-time learning.",
    "year": 2020,
    "source": "WOS",
    "area": "portfolio",
    "method": "machine learning",
    "cache_key": "164318c998ae447d40b05b292aa2b2fe",
    "relevant_keywords": [
      "neural network",
      "deep learning"
    ],
    "stage1_timestamp": "2025-05-15T01:14:16.214876"
  },
  "domain_result": {
    "success": true,
    "application_domains": [
      "None"
    ],
    "justification": "该论文主要研究Spiking Neural Networks (SNNs)的硬件架构和数据流优化，以提高能效和计算效率。虽然SNNs可能在金融领域有潜在应用，但论文中并未提及任何具体的金融应用场景或研究目标，如衍生品定价、金融风险管理或投资组合管理。",
    "raw_response": "```json\n{\n  \"application_domains\": [\"None\"],\n  \"justification\": \"该论文主要研究Spiking Neural Networks (SNNs)的硬件架构和数据流优化，以提高能效和计算效率。虽然SNNs可能在金融领域有潜在应用，但论文中并未提及任何具体的金融应用场景或研究目标，如衍生品定价、金融风险管理或投资组合管理。\"\n}\n```"
  },
  "timestamp": "2025-05-20T16:10:36.170640"
}