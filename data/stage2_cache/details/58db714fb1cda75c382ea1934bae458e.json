{
  "paper": {
    "id": 678,
    "title": "The Protection of Data Sharing for Privacy in Financial Vision",
    "abstract": "The primary motivation is to address difficulties in data interpretation or a reduction in model accuracy. Although differential privacy can provide data privacy guarantees, it also creates problems. Thus, we need to consider the noise setting for differential privacy is currently inconclusive. This paper's main contribution is finding a balance between privacy and accuracy. The training data of deep learning models may contain private or sensitive corporate information. These may be dangerous to attacks, leading to privacy data leakage for data sharing. Many strategies are for privacy protection, and differential privacy is the most widely applied one. Google proposed a federated learning technology to solve the problem of data silos in 2016. The technology can share information without exchanging original data and has made significant progress in the medical field. However, there is still the risk of data leakage in federated learning; thus, many models are now used with differential privacy mechanisms to minimize the risk. The data in the financial field are similar to medical data, which contains a substantial amount of personal data. The leakage may cause uncontrollable consequences, making data exchange and sharing difficult. Let us suppose that differential privacy applies to the financial field. Financial institutions can provide customers with higher value and personalized services and automate credit scoring and risk management. Unfortunately, the economic area rarely applies differential privacy and attains no consensus on parameter settings. This study compares data security with non-private and differential privacy financial visual models. The paper finds a balance between privacy protection with model accuracy. The results show that when the privacy loss parameter epsilon is between 12.62 and 5.41, the privacy models can protect training data, and the accuracy does not decrease too much.",
    "year": 2022,
    "source": "WOS",
    "area": "financial_risk",
    "method": "deep learning",
    "cache_key": "937e876e3bfacdf220555500f559fb47",
    "relevant_keywords": [
      "deep learning",
      "neural network"
    ],
    "stage1_timestamp": "2025-05-15T01:41:29.793279"
  },
  "domain_result": {
    "success": true,
    "application_domains": [
      "None"
    ],
    "justification": "该论文主要研究的是数据隐私保护技术在金融领域的应用，特别是差分隐私和联邦学习在保护金融数据隐私方面的平衡。虽然提到了风险管理和信用评分，但这些并不是论文的核心技术或主要研究方向。论文的重点在于隐私保护与模型准确性的平衡，而非具体的金融风险管理、衍生品定价或投资组合管理。",
    "raw_response": "```json\n{\n  \"application_domains\": [\"None\"],\n  \"justification\": \"该论文主要研究的是数据隐私保护技术在金融领域的应用，特别是差分隐私和联邦学习在保护金融数据隐私方面的平衡。虽然提到了风险管理和信用评分，但这些并不是论文的核心技术或主要研究方向。论文的重点在于隐私保护与模型准确性的平衡，而非具体的金融风险管理、衍生品定价或投资组合管理。\"\n}\n```"
  },
  "timestamp": "2025-05-20T16:02:28.126569"
}