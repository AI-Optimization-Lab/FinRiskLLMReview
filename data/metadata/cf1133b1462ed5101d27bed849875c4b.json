{
  "id": 284,
  "title": "Investigation into a University Electronic Portfolio System Using Activity Theory",
  "abstract": "The last few years have seen an enormous growth of interest in e-portfolios and the benefits they can bring to learners. While it is generally agreed that e-portfolios have great potential to engage students and promote deep learning, the research that has been conducted to date focuses very little on student perceptions of value of the e-portfolio for their learning. If students do not agree or wish to use the e-portfolio as an integral part of their educational experience, then the potential impact the e-portfolio have on learning will not be realised. This paper describes the development of an e portfolio system to promote reflective skills for engineering students in a university in Malaysia. The Activity Theory is used as a lens to explain the reasons for the failed adoption of the e portfolio system.",
  "year": 2013,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cf1133b1462ed5101d27bed849875c4b",
  "timestamp": "2025-05-15T00:34:13.629657"
}