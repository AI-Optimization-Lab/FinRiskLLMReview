{
  "id": 3085,
  "title": "Short selling and firms' long-term stock return volatility: Evidence from Chinese concept stocks in Hong Kong",
  "abstract": "This paper examines the impact of short selling on the long-term volatility of firms' stock returns, highlighting the moderating effect of corporate governance quality. We analyze a sample of Chinese concept stocks listed in Hong Kong from 2015 to 2023, separating stock volatility into short- and long-term components with a GARCH-MIDAS model and differentiating between noisy and informed short selling. We provide evidence through portfolio and regression analyses that short selling increases stock return volatility, with the most significant impact observed in the long-term component, fueled in part by noisy short selling. However, this effect can be mitigated by managers who invest in improving their firms' corporate governance quality. Implications of our findings for both research and practical applications are discussed.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c1386f4e54b30c649f0a61e3508e0e6d",
  "timestamp": "2025-05-15T01:13:09.915493"
}