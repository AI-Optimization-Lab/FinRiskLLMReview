{
  "id": 3470,
  "title": "Country Portfolio and Taxation: Evidence from Japan",
  "abstract": "This study investigates the conditions under which host country portfolios are more likely to regularize corporate tax behavior. We use a sample comprising data on Japanese multinationals covering 2004 to 2014 to examine the relationship between foreign direct investment (FDI) host country portfolios and tax avoidance from the perspectives of investor protection and ethical standards. Our multivariate regression results show that the number of countries with strong investor protection/high ethical standards in the FDI host country portfolio is negatively related to tax avoidance. We also find that the effect of investor protection/ethical standards is less pronounced when the degree of indulgence is high. Further analyses show that the relationships we find are affected by disclosure requirements, local regulations, and accounting standards. These findings remain unchanged after several sensitivity checks. Overall, this study contributes to the business ethics literature by showing that FDI host country investor protection and ethical standards are external monitoring mechanisms for mitigating tax avoidance by multinationals.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ed42c6bf0d2dd07af39c88cd3bf70c69",
  "timestamp": "2025-05-15T01:17:25.516805"
}