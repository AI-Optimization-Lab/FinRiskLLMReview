{
  "id": 3667,
  "title": "A Multifaceted Approach to Stock Market Trading Using Reinforcement Learning",
  "abstract": "In the recent past, algorithmic stock market trading for financial markets has undergone significant growth and played a major role in investment decisions. Several methods have been proposed with the objective of designing optimum trading strategies to maximize profitability, economic utility, and risk-adjusted returns. Although traditional methods including mean reversion, momentum, and trend following approaches show good results, but have poor generalization and often perform well in specific time frames. Presently, Reinforcement Learning (RL) approaches are more adaptable and continually perceive the environment by making optimum trading decisions. However, it is still difficult to develop a lucrative trading approach in a complicated and dynamic stock market. The primary challenges in RL methods are effective state representation to reflect current market situations and a suitable trading reward to encourage agents to make more informed decisions. To address such challenges, this research presented a multifaceted strategy for multi-stock market trading using RL that incorporates enhanced state representation based on daily historical data, technical indicators, and fundamental indicators from balance sheets, income statements, and cash flow statements. To inform the agent about the impact of decisions taken on a day-to-day basis by considering risk, a novel reward function named PSR is also proposed. The proposed RL agent is trained in a multi-stock environment in which investors have multiple shares and trading signals are needed with the quantity of shares by using Advantage Actor-Critic (A2C), and Deep Deterministic Policy Gradient (DDPG) algorithms. Furthermore, the proposed multifaceted strategy is validated on 30 Dow Jones stocks and the proposed model outperforms the benchmark Dow Jones Industrial Average index during backtesting.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d4ded9e735bcc628d03d0e598cda497f",
  "timestamp": "2025-05-15T02:29:00.017240"
}