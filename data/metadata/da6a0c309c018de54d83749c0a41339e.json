{
  "id": 1224,
  "title": "Demand forecast improvement based on electricity load scale and the electricity demander portfolio",
  "abstract": "Demand and supply balancing is the key technology for power management and high-efficiency planning. As the smart metering device is getting popular in the Japan, USA, and EU, the more data from smart meters can be collected for the better forecast. Demand forecasts are based on the machine learning technology. As the increase of the demanders, it is known that the forecasted error will be decreased. However, there is still a certain number of error left even though we have a large number of demanders in the demander portfolio. In this research, we found a new law between individuals demand forecast and total demand forecast. It explains the reason of limitation of the forecast is caused by non-independent covariance between the demanders. Based on this result, we presented a method to reduce possible forecast error by the management of demander portfolio. This result can serve as a guideline for the power company to acquire new demanders to keep the demand and supply balance.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "da6a0c309c018de54d83749c0a41339e",
  "timestamp": "2025-05-15T00:53:09.966795"
}