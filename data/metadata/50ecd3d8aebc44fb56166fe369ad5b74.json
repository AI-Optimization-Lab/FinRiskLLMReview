{
  "id": 4412,
  "title": "Historical legacies in savings: Evidence from Romania",
  "abstract": "This study explores channels of savings persistence using a spatial regression discontinuity across an imperial border in present-day Romania. With data obtained from a lab in the field experiment and household survey, the findings suggest that imperial history influences savings behaviors today. I find no evidence that economic preferences for risk and time differ across the border, nor that these preferences are culturally transmitted. Rather, imperial history is strongly correlated with current financial access and asset choice, which affect savings accumulation. To confirm the robustness of these findings, I conduct falsification tests that arbitrarily move the border and also rule out several alternative mechanisms, including trust in financial institutions, financial literacy, and migration. I establish the external validity of the field experiment using nationally-representative data, which also suggests that savings legacies have important welfare consequences for the ability to mitigate household shocks. These findings highlight the role of history in under-saving, as well as the reinforcing nature of culture and institutions in shaping contemporary economic outcomes.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "50ecd3d8aebc44fb56166fe369ad5b74",
  "timestamp": "2025-05-15T02:36:38.579128"
}