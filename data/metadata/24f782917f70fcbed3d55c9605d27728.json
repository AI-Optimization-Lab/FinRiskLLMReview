{
  "id": 3291,
  "title": "What Drives States to Support Renewable Energy?",
  "abstract": "Why do states support electricity generation from renewable energy sources? Lyon/ Yin (2010), Chandler (2009), and Huang et al. (2007) have answered this question for the adoption of renewable portfolio standards (RPS) at the U.S. state level. This article supplements their work by testing the core hypotheses on the EU27 sample between 1990 and 2010. Furthermore, the article asks why the majority of EU states rely on feed-in-tariffs (FIT). The study conducts logistic time series cross-section regression analyses that run on a hazard model. Evidence in support of private interest theory and public interest theory is provided. (a) The existence of a solar energy association increases the probability of a state to adopt regulation. (b) Solar radiation, and (c) the unemployment rate also increase the odds. (d) Electricity market concentration decreases the probability of transition.",
  "year": 2012,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "24f782917f70fcbed3d55c9605d27728",
  "timestamp": "2025-05-15T01:15:54.662663"
}