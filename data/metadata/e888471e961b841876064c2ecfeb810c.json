{
  "id": 6307,
  "title": "Malware Classification Based on Graph Convolutional Neural Networks and Static Call Graph Features",
  "abstract": "Advanced Persistent Threats (APT) are targeted, high level cybersecurity risk factors facing governments, financial units and other organizations. The attribution of APTs - gathering information about the origin of an attack - is an important key in the process of securing an organisation's infrastructure, prioritizing the measures to be taken depending on the actor(s) targeting the organisation. In practice, an elementary step in the process of attribution is determining the family and/or author of a sample, based on the binary file and/or its dynamic analysis - i.e. a multi-class classification problem regarding the family/author label. There are numerous methods in the literature aimed to label a sample based on its control flow graph or API sequence graph. We aim to summarize the literature on these methods, and offer another method to classify malware families leveraging the static call graph of a PE executable, as well as the functions' instruction lists, using a locality-sensitive hashing method to obtain the node feature vectors. Our results are compared to recent publications in the field.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e888471e961b841876064c2ecfeb810c",
  "timestamp": "2025-05-15T02:56:37.523665"
}