{
  "id": 571,
  "title": "STAGE framework: A stock dynamic anomaly detection and trend prediction model based on graph attention network and sparse spatiotemporal convolutional network",
  "abstract": "As the financial market becomes increasingly complex, stock prediction and anomaly data detection have emerged as crucial tasks in financial risk management. However, existing methods exhibit significant limitations in handling the intricate relationships between stocks and addressing anomalous data. This paper proposes the STAGE framework, which integrates the Graph Attention Network (GAT), Variational Autoencoder (VAE), and Sparse Spatiotemporal Convolutional Network (STCN), to enhance the accuracy of stock prediction and the robustness of anomaly data detection. Experimental results show that the complete STAGE framework achieved an accuracy of 85% after 20 training epochs, which is 10% to 20% higher than models with key algorithms removed. In the anomaly detection task, the STAGE framework further improved the accuracy to 95%, demonstrating fast convergence and stability. This framework offers an innovative solution for stock prediction, adapting to the complex dynamics of real-world markets.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "89ec77e935d8078e5f09254ade07b4f8",
  "timestamp": "2025-05-15T01:40:14.167398"
}