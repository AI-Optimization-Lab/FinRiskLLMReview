{
  "id": 4058,
  "title": "Estimating loss-given default through advanced credibility theory",
  "abstract": "The New Basel Accord allows internationally active banking organizations to calculate their credit risk capital requirements using an internal ratings based approach, subject to supervisory review. One of the modeling components is the loss-given default (LGD): it represents the credit loss for a bank when extreme events occur that influence the obligor ability to repay his debts to the bank. Among researchers and practitioners the use of statistical models such as linear regression, Tobit or decision trees is quite common in order to compute LGDs as a forecasting of historical losses. However, these statistical techniques do not seem to provide robust estimation and show low performance. These results could be driven by some factors that make differences in LGD, such as the presence and quality of collateral, timing of the business cycle, workout process management and M&A activity among banks. This paper evaluates an alternative method of modeling LGD using a technique based on advanced credibility theory typically used in actuarial modeling. This technique provides a statistical component to the credit and workout experts' opinion embedded in the collateral and workout management process and improve the predictive power of forecasting. The model has been applied to an Italian Bank Retail portfolio represented by Overdrafts; the application of credibility theory provides a higher predictive power of LGD estimation and an out-of-time sample backtesting has shown a stable accuracy of estimates with respect to the traditional LGD model.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "33fda970a7b352a78232f3e51c248842",
  "timestamp": "2025-05-15T01:23:27.321902"
}