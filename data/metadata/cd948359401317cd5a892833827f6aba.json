{
  "id": 3285,
  "title": "Out-of-sample realized volatility forecasting: does the support vector regression compete combination methods",
  "abstract": "This article investigates whether the nonlinear support vector regression method under the Heterogeneous Auto-Regressive model (SVR-HAR) can compete for combination methods in terms of out-of-sample realized volatility forecasting. Empirical analyses are conducted based on the CSI 300 index high-frequency data, two new combination methods are employed and compared with the forecasting ability of the SVR method. The empirical results show that SVR-HAR models outperform individual models and all the combination methods, although the new combination methods are superior to other combination strategies. Specifically, HAR models with realized semi-variances as regressors obtains the lowest forecasting errors, confirming the strong forecasting ability of nonlinear SVR method and the realized semi-variances. The portfolio performance further confirms the highest economic value for models employing realized semi-variances and nonlinear SVR method in terms of volatility forecasting.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cd948359401317cd5a892833827f6aba",
  "timestamp": "2025-05-15T01:15:26.192171"
}