{
  "id": 1214,
  "title": "Measuring financial market integration over the long run: Is there a U-shape?",
  "abstract": "Using long time series for sovereign bond markets of fifteen industrialized economies from 1875 to 2009, I find that financial market integration by the end of the 20th century was higher than in earlier periods and exhibited a J-shaped trend with a trough in the 19205. The main reason for the higher financial integration seen today is the recent extensive globalization. Around the turn of the 20th century, countries frequently drifted apart. Conversely, in recent years, the bond markets of most countries have moved together. Both policy variables and the global market environment play a role in explaining the time variation in integration, while unexplained changes in the overall level of country risk are also empirically important. My methodology, based on principal components analysis, is immune to outliers and accounts for global and country-specific shocks and, hence, can capture trends in financial integration more accurately than standard techniques such as simple correlations. (C) 2011 Elsevier Ltd. All rights reserved.",
  "year": 2011,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d5c1c25045419fbca6cf0b0d91d3a482",
  "timestamp": "2025-05-15T02:00:30.466268"
}