{
  "id": 1348,
  "title": "A quantile regression neural network approach to estimating the conditional density of multiperiod returns",
  "abstract": "This paper presents a new approach to estimating the conditional probability distribution of multiperiod financial returns. Estimation of the tails of the distribution is particularly important for risk management tools, such as Value-at-Risk models. A popular approach is to assume a Gaussian distribution, and to use a theoretically derived variance expression which is a non-linear function of the holding period, k, and the one-step-ahead volatility forecast, delta(t+l). The new method avoids the need for a distributional assumption by applying quantile regression to the historical returns from a range of different holding periods to produce quantile models which are functions of k and delta(t+l). A neural network is used to estimate the potentially non-linear quantile models. Using daily exchange rates, the approach is compared to GARCH-based quantile estimates. The results suggest that the new method offers a useful alternative for estimating the conditional density. Copyright (C) 2000 John Wiley & Sons, Ltd.",
  "year": 2000,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "698be2bfcb556b7db34bcca9e9d27038",
  "timestamp": "2025-05-15T02:01:44.844505"
}