{
  "id": 5118,
  "title": "The Shapley value of regression portfolios",
  "abstract": "By viewing portfolio optimization as a cooperative game played by the assets minimizing risk for a given return, investors can compute the exact value each security adds to the common payoff of the game. This is known the Shapley value that imputes the contribution of each asset, by looking at all the possible portfolios in which securities might participate. In this paper I use the Shapley value to decompose the risk and return of optimal portfolios that result from minimizing ordinary least squares. These regression portfolios are identical to tangency portfolios obtained by maximizing the Sharpe ratio of holdings on the mean-variance efficient frontiers. The Shapley value of individual assets is computed using the statistics resulting from the regressions. The value imputation prices assets by their comprehensive contribution to portfolio risk and return. This procedure allows investors to make unbiased decisions when analyzing the inherent risk of their holdings. By running OLS regressions, the Shapley value is calculated for asset allocation using Ibbotson's aggregate financial data for the years 1926-2019.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2a013174be8208ab86df0d794ddbea67",
  "timestamp": "2025-05-15T02:44:43.809030"
}