{
  "id": 1420,
  "title": "Neural networks for extracting implied risk-neutral probability density surface of stock index options",
  "abstract": "How to extract the implied risk-neutral probability density surface from market prices of stock index options is an important research topic. However, not only the observable option prices are discrete, insufficient, and noisy, but also the implied probability distribution is nonstationary. Therefore, extracting an implied probability distribution accurately from option prices is not an easy task. To overcome the difficulties coming from market reality, a multilayer feedforward neural network is applied. Based on a theory derived by Breeden and Litzenberger, the risk-neutral probability density surface implicit in option prices is then extracted. Market prices of S&P 500 Index options, SPX, are used to investigate performances of the proposed method. Empirical tests show that the extracted risk-neutral probability density surface is consistent with market data accurately and predicts future option price structure precisely. Thus, the proposed neural network technique is suitable for a variety of financial applications.",
  "year": 2005,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c8922036f424120b1add8c09e8a99464",
  "timestamp": "2025-05-15T02:03:00.615602"
}