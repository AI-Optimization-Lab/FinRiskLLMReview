{
  "id": 2393,
  "title": "Transformers and attention-based networks in quantitative trading: a comprehensive survey",
  "abstract": "Since the advent of the transformer neural network architecture, there has been a rapid adoption and investigation of its applicability in various domains, such as computer vision, speech processing, and natural language processing, with the latter most notably exemplified by the rise of Large Language Models. These accomplishments have also led to increased interest in other network architectures that rely on attention mechanisms, one of the building blocks of transformers. Transformers and other attention-based networks are being applied to the quantitative analysis, management, and trading of financial assets, be it for price movement prediction, discovery of trading strategies, portfolio optimization, and risk management. The applications range across different asset categories, including equity markets, foreign exchange pairs, cryptocurrencies, and futures markets. This survey aims to provide a comprehensive overview of the applications of attention-based networks within the field of quantitative analysis, management, and trading of financial assets. After a brief overview of transformers and attention mechanisms, we analyze the existing applications of these architectures for quantitative finance in a taxonomy of four specializations: Alpha Seeking, Risk Management, Portfolio Construction, and Execution. After comparing the literature in light of the research problems, modeling approaches, and complementary results, we discuss current challenges and research opportunities.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e7586c98605fd68e00451a596e06f489",
  "timestamp": "2025-05-15T02:14:50.322112"
}