{
  "id": 3330,
  "title": "Cross-sectional anomalies and conditional asset pricing models based on investor sentiment: evidence from the Chinese stock market",
  "abstract": "This study examines a comprehensive set of 30 cross-sectional anomalies in the Chinese A-share market to investigate whether incorporating investor sentiment as conditioning information enhances the explanatory power of asset pricing models. Utilizing a long-short portfolio strategy and Fama-MacBeth cross-sectional regression, we find that trading-based anomalies outnumber accounting-based anomalies in the Chinese market. Our results demonstrate that conditional models significantly outperform their unconditional counterparts. Notably, investor sentiment is crucial for capturing the size anomaly when excluding observations from the COVID-19 pandemic period. Additionally, it substantially improves the ability of conditional Fama-French three-factor models to capture individual anomalies and enhances the return-prediction accuracy of conditional CAPMs. We suggest further investigating high-frequency investor sentiment-based conditional models to anticipate stock price fluctuations during extraordinary public health events.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fe82ec1b553957cd185b0315233c5f57",
  "timestamp": "2025-05-15T01:15:54.856211"
}