{
  "id": 2722,
  "title": "Do LBMA gold price follow random-walk?",
  "abstract": "The present study attempted to analyse the random-walk characteristics of the gold spot price of the London Bullion Market Association (LBMA) by using several linear and nonlinear models. The research collects two decades of daily data from 3rd February 2000 to 2nd October 2020. Econometric tests such as serial correlation test, unit-root tests, multiple variance ratio (MVR), and the BDS test were applied to examine the linear and nonlinear dependence of return series. Further, we employed all the tests from ARCH family to examine the volatility clustering of the gold return series. The results of serial correlation and the unit-root test suggest that the gold return is stationary, therefore does not follow the random-walk benchmark, and hence the gold market is inefficient. EGARCH results indicate that the positive news has a more significant impact on the gold return than the negative news. The findings have important implications for the efficient portfolio investments, and better hedging opportunities for the investors.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "285482bd8afaa8353e69e35ef3063872",
  "timestamp": "2025-05-15T01:09:43.375147"
}