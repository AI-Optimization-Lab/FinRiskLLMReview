{
  "id": 2600,
  "title": "Lucky factors",
  "abstract": "Identifying the factors that drive the cross-section of expected returns is challenging for at least three reasons. First, the choice of testing approach (time series versus cross-sectional) will deliver different sets of factors. Second, varying test portfolio sorts changes the impor-tance of candidate factors. Finally, given the hundreds of factors that have been proposed, test multiplicity must be dealt with. We propose a new method that makes measured progress in addressing these key challenges. We apply our method in a panel regression setting and shed some light on the puzzling empirical result that the market factor drives the bulk of the variance of stock returns, but is often knocked out in cross-sectional tests. In our setup, the market factor is not eliminated. Further, we bypass arbitrary portfolio sorts and instead execute our tests on individual stocks with no loss in power. Finally, our bootstrap implementation, which allows us to impose the null hypothesis of no cross-sectional explanatory power, naturally controls for the multiple testing problem. (c) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "26bea0c446ff61215c253e2b3f1086fc",
  "timestamp": "2025-05-15T01:08:41.843384"
}