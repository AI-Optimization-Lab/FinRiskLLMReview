{
  "id": 1768,
  "title": "Portfolio optimization based on GARCH-EVT-Copula forecasting models",
  "abstract": "This study uses GARCH-EVT-copula and ARMA-GARCH-EVT-copula models to perform out-of-sample forecasts and simulate one-day-ahead returns for ten stock indexes. We construct optimal portfolios based on the global minimum variance (GMV), minimum conditional value-at-risk (Min-CVaR) and certainty equivalence tangency (CET) criteria, and model the dependence structure between stock market returns by employing elliptical (Student-t and Gaussian) and Archimedean (Clayton, Frank and Gumbel) copulas. We analyze the performances of 288 risk modeling portfolio strategies using out-of-sample back-testing. Our main finding is that the CET portfolio, based on ARMA-GARCH-EVT-copula forecasts, outperforms the benchmark portfolio based on historical returns. The regression analyses show that GARCH-EVT forecasting models, which use Gaussian or Student-t copulas, are best at reducing the portfolio risk. (C) 2018 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5b9e64e5acababd4d4e46fa0c2c2e36c",
  "timestamp": "2025-05-15T00:59:22.968282"
}