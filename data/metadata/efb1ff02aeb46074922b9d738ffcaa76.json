{
  "id": 286,
  "title": "Text-mining approach with Black-Litterman model: A case study of Armenian pension funds",
  "abstract": "Pension fund managers face challenges in acquiring, analysing, and synthesising external information to make informed investment decisions. Text-mining and sentiment analysis can aid fund managers integrate contrasting news, financial data, and market speculation into their decision rubric. Considering Armenia pension funds as the research object, we perform sentiment analysis on Yahoo Finance data, transform the information into numerical data, add confidence weights, and apply the Black-Litterman model using supervised deep learning to predict how information affects portfolio management. Our method using the Black-Litterman model is practical to quantify how external information impacts on fund portfolio management, and using the method leads to an increase in portfolio performance.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "efb1ff02aeb46074922b9d738ffcaa76",
  "timestamp": "2025-05-15T00:34:13.642182"
}