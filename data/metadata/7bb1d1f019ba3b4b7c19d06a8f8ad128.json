{
  "id": 660,
  "title": "A Systematic Literature Review of Volatility and Risk Management on Cryptocurrency Investment: A Methodological Point of View",
  "abstract": "In this study, we explore the research published from 2009 to 2021 and summarize what extant literature has contributed in the last decade to the analysis of volatility and risk management in cryptocurrency investment. Our samples include papers published in journals ranked across different fields in ABS ranked journals. We conduct a bibliometric analysis using VOSviewer software and perform a literature review. Our findings are presented in terms of methodologies used to model cryptocurrencies' volatility and also according to their main findings pertaining to volatility and risk management in those assets and using them in portfolio management. Our research indicates that the models that consider the Markov-switching regime seem to be more consensual among the authors, and that the best machine learning technique performances are hybrid models that consider the support vector machines (SVM). We also argue that the predictability of volatility, risk reduction, and level of speculation in the cryptocurrency market are improved by the leverage effects and the volatility persistence.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7bb1d1f019ba3b4b7c19d06a8f8ad128",
  "timestamp": "2025-05-15T00:46:24.433062"
}