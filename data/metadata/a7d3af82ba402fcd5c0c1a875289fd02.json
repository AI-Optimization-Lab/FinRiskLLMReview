{
  "id": 2827,
  "title": "Measurement of common risks in tails: A panel quantile regression model for financial returns",
  "abstract": "We investigate how to measure common risks in the tails of return distributions using the recently proposed panel quantile regression model for financial returns. By exploring how volatility crosses all quantiles of the return distribution and using a fixed effects estimator, we can control for otherwise unobserved heterogeneity among financial assets. Direct benefits are revealed in a portfolio value-at-risk application, where our modeling strategy performs significantly better than several benchmark models. In particular, our results show that the panel quantile regression model for returns consistently outperforms all competitors in the left tail. Sound statistical performance translates directly into economic gains. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a7d3af82ba402fcd5c0c1a875289fd02",
  "timestamp": "2025-05-15T02:19:46.627655"
}