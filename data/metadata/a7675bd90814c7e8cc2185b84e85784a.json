{
  "id": 1787,
  "title": "Decomposing and reconstructing dynamic risks in the crude oil market based on the VMD and Lempel-Ziv algorithms",
  "abstract": "Crude oil markets have become increasingly uncertain. To study them, we first employ the decomposition-ensemble framework based on the variational mode decomposition (VMD) and Lempel-Ziv algorithms to assess the crude oil dual attributes. Three steps are involved: 1) conditional autoregressive value at risk measures the crude oil risk; 2) they are decomposed by the VMD algorithm into submodes; 3) the Lempel-Ziv algorithm is applied to analyze the crude oil risk for each, thereby identifying the oil commodity or oil financial risks. The results of the empirical analysis reveal significantly different amplitudes for the high-and low-frequency crude oil risk. By summarizing the crude oil risk components, we also conclude that the mean value for the oil commodity risk is 0.04, while that for the oil financial risk is 0. What is more, the oil commodity risk is highly related to downward trends in oil prices, while the oil financial risk exerts the same clustering effect as oil returns.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a7675bd90814c7e8cc2185b84e85784a",
  "timestamp": "2025-05-15T02:07:25.714345"
}