{
  "id": 1456,
  "title": "Predictive Modeling for Default Risk Using A Multilayered Feedforward Neural Network with Bayesian Regularization",
  "abstract": "In this study we propose a multilayered feedforward neural network (MFNN) with Bayesian Regularization, and apply it to the credit risk evaluation problem domain using a real world data set from a financial services company in England. We choose the MFNN because of its broad applicability to many problem domains of relevance to business: principally prediction, classification, and modelling. We employ two different methods to determine their prowess in identifying the true positives, that is, defaulters. We analyzed the effect of making the number of observed bad equal the number of observed good in the data by over sampling of the minority class (bad obligors) by resampling without replacement, and compare this to the dimensionality reduction of the input vector space using Principal Component Analysis. Overall results indicate that using the Receiver Operating Characteristic as a measure of discriminatory power, over sampling of the minority class has been found to be effective in identifying the true positives.",
  "year": 2013,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7053f0846d944aaa5585de7a66972de9",
  "timestamp": "2025-05-15T02:03:00.816334"
}