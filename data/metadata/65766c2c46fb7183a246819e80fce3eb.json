{
  "id": 2825,
  "title": "How can consumers without credit history benefit from the use of information processing and machine learning tools by financial institutions?",
  "abstract": "This research aims to enhance the predictability of creditworthiness among marginalized consumers affected by the widespread adoption of AI frameworks. We utilize ensemble methods to handle the imbalanced dataset used for evaluating the credit risk of consumers with sparse or non-existent credit histories. To promote fairness in the Machine Learning (ML) model, we employed the disparate impact remover-a recognized bias mitigation tool to minimize group bias. Three strategies were employed to tackle dataset imbalance: oversampling, undersampling, and class weight adjustment. Our findings reveal that adjusting the class weight proved most effective in sustaining commendable performance, demonstrating higher accuracy and F-1 scores surpassing 80% in most experiments. While the application of the disparate impact remover might compromise the ML model's predictive capabilities, our results underscore the necessity of deliberating over the use of potentially bias-sensitive, unprotected features. Recognizing the critical nature of this trade-off for financial decision-makers, we delve into its implications.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "65766c2c46fb7183a246819e80fce3eb",
  "timestamp": "2025-05-15T02:19:46.617500"
}