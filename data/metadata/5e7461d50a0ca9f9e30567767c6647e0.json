{
  "id": 3260,
  "title": "Make it Happen Summer School: Experiential Learning to Develop Novice Socially-Engaged Artists",
  "abstract": "This article evaluates Make it Happen Summer School according to Bernstein's writing on classification, framing, and recontextualisation. The project was a collaboration between a university and an Arts Council England National Portfolio Organisation (NPO) that aimed to develop a curriculum for creative practitioners so they could learn to propose successful socially-engaged arts projects for funding and commissions. NPOs are arts organisations that are funded by the UK Government and the UK's National Lottery via the Arts Council England. This article draws upon quantitative and qualitative data collected in relation to this project to evaluate the curriculum and pedagogy. In order to do this, models developed from Bernstein's work guide the analysis of the findings. It was found that while many gained powerful knowledge from the project, some did not. The processes of recontextualisation demonstrated how ideologies based on accountability and performativity shaped the curriculum reflecting the depoliticisation of socially-engaged practice.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5e7461d50a0ca9f9e30567767c6647e0",
  "timestamp": "2025-05-15T01:15:26.104051"
}