{
  "id": 274,
  "title": "Risk spillover analysis of China's financial sectors based on a new GARCH copula quantile regression model",
  "abstract": "This study employs a new GARCH copula quantile regression model to estimate the conditional value at risk for systemic risk spillover analysis. To be specific, thirteen copula quantile regression models are derived to capture the asymmetry and nonlinearity of the tail dependence between financial returns. Using Chinese stock market data over the period from January 2007 to October 2020, this paper investigates the risk spillovers from the banking, securities, and insurance sectors to the entire financial system. The empirical results indicate that (i) three financial sectors contribute significantly to the financial system, and the insurance sector displays the largest risk spillover effects on the financial system, followed by the banking sector and subsequently the securities sector; (ii) the time-varying risk spillovers are much larger during the global financial crisis than during the periods of the banking liquidity crisis, the stock market crash and the COVID-19 pandemic. Our results provide important implications for supervisory authorities and portfolio managers who want to maintain the stability of China's financial system and optimize investment portfolios.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "10ff5a301718454faf866bff7f68086b",
  "timestamp": "2025-05-15T01:49:04.536890"
}