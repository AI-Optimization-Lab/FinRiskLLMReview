{
  "id": 1224,
  "title": "A Novel RMS-Driven Deep Reinforcement Learning for Optimized Portfolio Management in Stock Trading",
  "abstract": "Algorithmic stock trading has improved tremendously, with Reinforcement Learning (RL) algorithms being more adaptable than classic approaches like mean reversion and momentum. However, challenges remain in adequately depicting market events and generating suitable rewards to influence the trading decisions of an agent in a dynamic environment. This study proposed an improved stock market trading framework termed the RMS-Driven Deep Reinforcement Learning (DRL) model for optimal portfolio management. The research attempts to give a more comprehensive view of the market and, as a result, enhance trading decisions by including consumer information and incorporating news sentiments into the model, in addition to data from typical earnings reports. More specifically, three kinds of DRL models are presented, combined with data from stock earnings reports, Max Drawdown rewards, and sentiment indicators (RMS), known as PPO_RMS, A2C_RMS, and DDPG_RMS, respectively. The findings of this research indicate that the integrated model, mainly the DDPG_RMS effectively outperforms the baseline <^>DJI index in many risk-return analyses in ratios showing better risk management, and profitability. The proposed stock trading model generates a maximum cumulative return of 27% with a Sharpe ratio of 0.66, showing an appropriate trade-off between risk and return. This approach, which incorporates sentiment analysis and Max Drawdown rewards, significantly enhances the model's performance in adapting to changing market conditions. Therefore, the results emphasize the appropriateness of integrating sentiment indices with traditional financial data to enhance a trader's performance while also offering essential information to aid in the development of improved trading tactics in continuously changing financial markets.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "014343a5b6ac654f21eaf1b83a545851",
  "timestamp": "2025-05-15T02:00:30.494894"
}