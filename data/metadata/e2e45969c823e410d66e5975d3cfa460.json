{
  "id": 3951,
  "title": "Measuring Asset Market Linkages: Nonlinear Dependence and Tail Risk",
  "abstract": "Traditional measures of dependence in time series are based on correlations or periodograms. These are adequate in many circumstances but, in others, especially when trying to assess market linkages and tail risk during abnormal times (e.g., financial contagion), they might be inappropriate. In particular, popular tail dependence measures based on exceedance correlations and marginal expected shortfall (MES) have large variances and also contain limited information on tail risk. Motivated by these limitations, we introduce the (tail-restricted) integrated regression function, and we show how it characterizes conditional dependence and persistence. We propose simple estimates for these measures and establish their asymptotic properties. We employ the proposed methods to analyze the dependence structure of some of the major international stock market indices before, during, and after the 2007-2009 financial crisis. Monte Carlo simulations and the application show that our new measures are more reliable and accurate than competing methods based on MES or exceedance correlations for testing tail dependence. for this article are available online.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e2e45969c823e410d66e5975d3cfa460",
  "timestamp": "2025-05-15T02:31:28.481378"
}