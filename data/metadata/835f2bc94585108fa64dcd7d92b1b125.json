{
  "id": 3424,
  "title": "VaR calculation by binary response models",
  "abstract": "The original Risk-Metrics method is underpinned by the assumption that daily asset returns are conditional Gaussian independently identically distributed (iid) random variables with a mean of zero. In this paper, a new method to calculate Value at Risk (VaR) was suggested to overcome the shortcoming of Risk-Metrics by employing binary response models to compute probability forecasts of the portfolio return by exceeding a grid of candidate quantile values. From those values, the VaR quantile value was selected. The proposed model was called BRV (Binary Response VaR method). Consistent application of BRV to the Dow Jones Industrial Average (INDEXDJX: DJI) and Dow Jones U.S. Marine Transportation Index (DJUSMT) time series proved that it was more accurate than the Risk-Metric system. This method not only worked similar to quantile regression but had the advantage that conventional maximum likelihood methods could be used for parameter estimation and inference. The BRV method was the best performing method for computing the daily VaR at both the 95% and 99% confidence levels over the period 02/01/06-31/12/08. The BRV and the QR (quantile regression) methods performed similarly, but the BRV method had the practical advantage that conventional maximum likelihood (ML) technique could be used for parameter estimation and robust inference.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "835f2bc94585108fa64dcd7d92b1b125",
  "timestamp": "2025-05-15T01:16:55.558821"
}