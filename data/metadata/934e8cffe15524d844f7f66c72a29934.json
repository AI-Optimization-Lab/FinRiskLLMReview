{
  "id": 670,
  "title": "Lagrange Programming Neural Networks for Sparse Portfolio Design",
  "abstract": "Although there were some works on analog neural networks for sparse portfolio design, the existing works do not allow us to control the number of the selected assets and to adjust the weighting between the risk and return. This paper proposes a Lagrange programming neural network (LPNN) model for sparse portfolio design, in which we can control the number of selected assets. Since the objective function of the sparse portfolio design contains a non-differentiable l(1)-norm term, we cannot directly use the LPNN approach. Hence, we propose a new formulation based on an approximation of the l(1)-norm. In the theoretical side, we prove that state of the proposed LPNN network globally converges to the nearly optimal solution of the sparse portfolio design. The effectiveness of the proposed LPNN approach is verified by the numerical experiments. Simulation results show that the proposed analog approach is superior to the comparison analog neural network models.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "934e8cffe15524d844f7f66c72a29934",
  "timestamp": "2025-05-15T00:46:24.482304"
}