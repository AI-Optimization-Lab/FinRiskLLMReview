{
  "id": 2438,
  "title": "Early Warning Models of Banking Crises: VIX and High Profits",
  "abstract": "We built a logistic regression Early Warning Models (EWM) for banking crises in a panel of 47 countries based on data from 1970-2014 using candidate variables that cover macro and financial market indicators. We find that VIX, a proxy of global risk-premium, has a strong signalling properties and that low VIX (low price of risk) increases likelihood of crisis. It does not only mean that stability leads to instability, but that this tends to be a global rather than a domestic phenomenon. We also find that particularly high contribution of financial sector to GDP growth often precedes crises, suggesting that such instances are primarily driven by excessive risk taking by financial sector and may not necessarily be sustainable. Other variables that feature prominently include credit and residential prices. Models using multiple variable clearly outperform single variable models, with probability of correct signal extraction exceeding 0.9. Our setting includes country-specific information without using country-specific effects in a regression, which allows for direct application of EWM we obtain to any country, including these that have not experienced a banking crisis.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3a09544fa63e215d567d6eae0d2f1b82",
  "timestamp": "2025-05-15T02:15:27.023069"
}