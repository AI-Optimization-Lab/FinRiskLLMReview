{
  "id": 270,
  "title": "RETRACTED: Exchange Rate Forecasting Based on Deep Learning and NSGA-II Models (Retracted Article)",
  "abstract": "Today, the global exchange market has been the world's largest trading market, whose volume could reach nearly 5.345 trillion US dollars, attracting a large number of investors. Based on the perspective of investors and investment institutions, this paper combines theory with practice and creatively puts forward an innovative model of double objective optimization measurement of exchange forecast analysis portfolio. To be more specific, this paper proposes two algorithms to predict the volatility of exchange, which are deep learning and NSGA-II-based dual-objective measurement optimization algorithms for the exchange investment portfolio. Compared with typical traditional exchange rate prediction algorithms, the deep learning model has more accurate results and the NSGA-II-based model further optimizes the selection of investment portfolios and finally gives investors a more reasonable investment portfolio plan. In summary, the proposal of this article can effectively help investors make better investments and decision-making in the exchange market.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "88d5d0dee86bf554274ab18f8ac3dedf",
  "timestamp": "2025-05-15T00:34:13.539170"
}