{
  "id": 1530,
  "title": "A Hybrid Multi-objective Programming based System Portfolio Selection by SoSCR",
  "abstract": "With military requirements, practitioners often face weapon system portfolio selection problems at the equipment R&D stage. Recently, the system of SoS contribution rate (SoSCR) always serves as one of the critical criteria to evaluate the role of a system, facilitating with the weapon system portfolio selection. In this paper, we establish a SoSCR value evaluation model considering two aspects of static capability contribution rate and dynamic connectivity contribution rate, which provide a more comprehensive estimate. Then, based on the value evaluation model, the classification optimization selection strategy is used to obtain the optimal system portfolio. Finally, an illustrative example is presented to verify the feasibility of the proposed model.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e625dab732319d707221aa27e1240b04",
  "timestamp": "2025-05-15T00:56:39.113570"
}