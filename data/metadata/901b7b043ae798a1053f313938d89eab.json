{
  "id": 1326,
  "title": "Computer simulation of investment efficiency function model based on GMM method and artificial intelligence",
  "abstract": "Traditional investment efficiency evaluation methods mostly focus on single-stage investment portfolio evaluation, and there is no comprehensive theoretical analysis method in multi-stage investment evaluation. Therefore, the construction of a scientific and reasonable multi-stage investment portfolio evaluation system is an urgent need for the theoretical and practical industries. In order to improve the efficiency of investment efficiency model evaluation, based on the GMM method and machine learning algorithm, this paper constructs an investment efficiency model based on the GMM method, and uses the characteristics of multi-scale Gaussian convolution to establish the MGCGMM model corresponding to the GMM model. The experimental results provided in this paper show that the MGC algorithm and the MGC-EM algorithm have higher estimation accuracy than the traditional algorithm EM algorithm. The intuitive meaning of this model can be understood as the mathematical expectation of the random variable function induced by the kernel function, which provides a way to directly estimate the GMM model using sample data. According to the case analysis, the model constructed in this paper has certain effects.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "901b7b043ae798a1053f313938d89eab",
  "timestamp": "2025-05-15T00:54:24.677702"
}