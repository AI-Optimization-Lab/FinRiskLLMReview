{
  "id": 2281,
  "title": "On improving GARCH volatility forecasts for Bitcoin via a meta-learning approach",
  "abstract": "Modelling the volatility of Bitcoin, the cryptocurrency with the largest market share, has recently attracted considerable attention from researchers, practitioners and investors in financial markets and portfolio management. For this purpose, a wide variety of GARCH-type models have been employed. However, there is no consensus in the literature on which specification arising from the volatility equation and the assumed error distribution is better in an out-of-sample performance. This study tries to fill this gap by comparing the forecasting performances of 110 GARCH-type models for Bitcoin volatility. Furthermore, it proposes a new combining method based on support vector machines (SVM). This method effectively selects the set of superior models to perform meta-learning. The results indicate that the best performing GARCH specification depends on the loss function chosen, and the proposed method leads to more accurate volatility forecasts than those of the best GARCH-type models and other combining methods investigated. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "53451e2e484efd04918b144c8852f17b",
  "timestamp": "2025-05-15T01:05:04.526898"
}