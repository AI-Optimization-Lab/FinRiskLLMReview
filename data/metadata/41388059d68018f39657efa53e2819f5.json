{
  "id": 237,
  "title": "Bayesian Network Model of China's Financial Risk Under COVID-19 Based on Grey Clustering",
  "abstract": "The panic caused by COVID-19 and the stagnation of business activities induced the continuous breeding of China's financial risks. This paper considers the COVID-19 and economic indexes as nodes to establish the Bayesian topology of financial risk. The liquidity, sovereign, and stock market risks are mainly considered to evaluate the financial risk. Based on the risk characteristics, the central interval trapezoidal possibility functions are designed, then the grey clustering model is used to classify the financial risk into four different levels. The possibility distribution of financial risk levels under different COVID-19 index levels is inferenced through the Bayesian network. Finally, each node's monthly time series data from October 2019 to May 2021 is used to learn by NETICA software, and the conditional probability of each node and the possibility of financial risk are deduced. It is concluded that liquidity risk and sovereign risk are more sensitive to COVID-19, while the stock market risk is not very sensitive to it.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "41388059d68018f39657efa53e2819f5",
  "timestamp": "2025-05-15T01:48:30.739449"
}