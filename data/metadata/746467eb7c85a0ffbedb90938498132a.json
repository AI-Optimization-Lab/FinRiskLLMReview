{
  "id": 2097,
  "title": "The public production of medicines in Brazil",
  "abstract": "The paper aims to contribute as a reflection on the public production of medicines in Brazil. Public producers present themselves as strategic in Brazil, either as price regulators, in meeting the demands of the Ministry of Health (MoH) on neglected products and those at risk of shortage to SUS. The study used the official bases of the MoH, National Health Surveillance Agency (ANVISA) and Website of Official Pharmaceutical Laboratories (OPL). Thirty-three OPL were identified, 16 with active production of drug registration at ANVISA. For the remaining 17 LFOs, no one identified active portfolio in the bases surveyed. There are 80% of the OPL portfolio concentrated in the first level of the Anatomical Therapeutic Chemical Classification, such as alimentary tract and metabolism, blood and blood forming organs, cardiovascular system, anti-infective for systemic use and nervous system. The OPL participation in the health complex is 48.6% of its portfolio dedicated to the strategic component, 30.6% for primary care and 20.7% for the specialized. It concludes the relevance of the OPL for the Brazilian health policy, with the better realignment of their potential in the face of technological advancement, health legislation, drug dependence and new treatment protocols.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "746467eb7c85a0ffbedb90938498132a",
  "timestamp": "2025-05-15T01:03:03.618626"
}