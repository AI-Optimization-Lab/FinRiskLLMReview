{
  "id": 2598,
  "title": "Long horizon predictability: An asset allocation perspective",
  "abstract": "Consider investors with a 10-year investment horizon who rebalance their portfolio at the monthly frequency. Should they use information from monthly returns, 10-year returns or intermediate returns to build their optimal portfolios? When stock and bond returns are i.i.d., the frequency of returns is not relevant. However, when they are predictable, it is. Using a new estimation approach and before correcting for overlapping observations, we show that the positive impact of predictability on investors' welfare is stronger for longer prediction horizons and the more so as the investment horizon enlarges. This welfare improvement is achieved by adopting realistic portfolio positions. When we correct for the persistence in the predictive regression residuals due to overlapping observations, our results are preserved for short to medium investment horizons although the added value of long horizon predictability is reduced. Our results are robust to various checks and also hold out-of-sample. Overall, short to medium term investors should exploit long horizon predictability even though they rebalance their portfolios at high frequency. (C) 2019 Elsevier B.V. All rights reserved.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "52c93e1277f92ecea6790f9e33e568b2",
  "timestamp": "2025-05-15T01:08:41.830842"
}