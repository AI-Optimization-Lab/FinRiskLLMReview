{
  "id": 3656,
  "title": "Role of political risk to achieve carbon neutrality: Evidence from Brazil",
  "abstract": "The current research assesses the impact of political risk on carbon dioxide (CO2) emissions in Brazil while controlling the role of financial development, GDP growth, trade openness, and technological innovation. In doing so, the quarterly dataset from 1990 to 2018 is utilized with Bayer and Hanck cointegration, dynamic ordinary least square (DOLS) and canonical correlation regression (CCR), and frequency-domain causality tests. The cointegration test revealed a long-run association amongst the variables of interest. Furthermore, the outcomes from the DOLS and CCR revealed that increasing financial development, technological innovation, trade openness, and real growth increase CO2 emissions while a better political environment reduces environmental pollution.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0bf545a444ce443bb376ea11ab1a37ab",
  "timestamp": "2025-05-15T02:28:29.431490"
}