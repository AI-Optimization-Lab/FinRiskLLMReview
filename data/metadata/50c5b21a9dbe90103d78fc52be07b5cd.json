{
  "id": 2542,
  "title": "Tail risk spillovers between Shanghai oil and other markets",
  "abstract": "This paper uses daily returns data from January 2011 to December 2022 to analyse the tail risk spillovers between Shanghai oil and a sample of stock and commodities markets. The computed CAViaR measures each market's tail risk and analyses the connectedness network using the TVP-VAR method. The tail risk spillover network estimates reveal clustering-as stocks and commodities within the same category are vigorously connected. Shanghai oil's links to the global markets remain limited, but it is a net risk receiver. As such, Shanghai oil is a lesser player in the global financial system than WTI or Brent. Nevertheless, (tail risk) connectedness between Shanghai oil and sample markets soars during crises such as the shale oil revolution, the COVID-19 pandemic, and the Russia -Ukraine war. Among the crises, COVID-19 has had the most potent effect on our markets-with connectedness indicators exceeding 70%. The spillover size and shape differ considerably by crisis events. Thus, Shanghai oil futures may be viewed as a novel financial market that permits both domestic and international investors to access the Chinese crude oil market and diversify their investment risk.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "50c5b21a9dbe90103d78fc52be07b5cd",
  "timestamp": "2025-05-15T02:16:32.683558"
}