{
  "id": 482,
  "title": "The Network of Mutual Funds: A Dynamic Heterogeneous Graph Neural Network for Estimating Mutual Funds Performance",
  "abstract": "Mutual funds are interconnected to each other through multiple types of links, including but not limited to co-investment, advisors, firms, and managers. These connections enable information flow among network entities, influence investment decisions, and ultimately impact mutual fund managers' performance. In this paper, we propose a dynamic graph neural network approach to model these heterogeneous relationships and their contributions to mutual fund performance. Using the graph attention mechanism, our model learns latent embedding for mutual funds and their invested assets dynamically in each month and then uses the embedding to estimate future returns. Empirical analysis confirms that the proposed method outperforms the state-of-the-art DeepWalk model by 10%. Furthermore, this study also reveals the importance of networks in mutual fund performance. The inclusion of network connections in a feedforward machine learning model significantly increases the performance of the model by 118%. Finally, portfolio analysis and regression estimation on next month's excess return show that the proposed approach has a significant economic contribution over current benchmark approaches.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "31c727f441045d6fb7402858aff8461a",
  "timestamp": "2025-05-15T00:43:47.426626"
}