{
  "id": 1655,
  "title": "A STUDY OF HYBRID GENETIC-FUZZY MODELS FOR IPO STOCK SELECTION",
  "abstract": "In this paper, we present a study of hybrid genetic-fuzzy models for effective IPO stock selection. This class of models employs a stock scoring mechanism using IPO fundamental variables and applies fuzzy membership functions to re-scale the scores properly. The scores are then used to obtain the relative rankings of IPO's and top-ranked IPO's can be selected to form a portfolio. On top of the stock scoring model, a genetic algorithm is used for optimization of model parameters and feature selection for input variables simultaneously. We will show that the investment returns provided by our methodology significantly outperform the benchmark. Based upon the promising results obtained, we expect that this hybrid genetic-fuzzy methodology can advance the research in machine learning for finance and provide an effective solution to stock selection for IPO's in practice.",
  "year": 2012,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d31da6f818fd8bc403ee575e51821686",
  "timestamp": "2025-05-15T00:58:18.274037"
}