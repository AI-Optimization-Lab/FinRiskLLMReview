{
  "id": 6392,
  "title": "Tail risk of electricity futures",
  "abstract": "This paper compares the in-sample and out-of-sample performance of several models for computing the tail risk of one-month and one-year electricity futures contracts traded in the NordPool, French, German, and Spanish markets in 2008-2017. As measures of tail risk, we use the one-day-ahead Value-at-Risk (VaR) and the Expected Shortfall (ES). With VaR, the AR (1) GARCH (1,1) model with Student-t distribution is the best-performing specification with 88% cases in which the Fisher test accepts the model, with a success rate of 94% in the left tail and of 81% in the right tail. The model passes the test of model adequacy in the 100% of the cases in the NordPool and German markets, but only in the 88% and 63% of the cases in the Spanish and French markets. With ES, this model passes the test of model adequacy in 100% of cases in all markets. Historical Simulation and Quantile Regression based approaches misestimate tail risks. The right hand tail of the returns is more difficult to model than the left-hand tail and therefore financial regulators and the administrators of futures markets should take these results into account when setting additional regulatory capital requirements and margin account regulations to short positions. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3d9f31744b39a1daa0c3e8a0efe4aed6",
  "timestamp": "2025-05-15T02:57:38.772612"
}