{
  "id": 2007,
  "title": "Intelligent BiLSTM-Attention-IBPNN Method for Anomaly Detection in Financial Auditing",
  "abstract": "Anomaly detection is a fundamental requirement in financial auditing, its detecting results can be used to correct the defects and predict risks for audited enterprise. However, with the auditing data becoming very huge, the anomaly detection error probabilities and material misstatement risk will be significantly increased. In this case, it is essential to develop an intelligent anomaly detection technology to address above problems. For these reasons, this paper develops a new intelligent anomaly detection method that combines the advantages of bidirectional long-short term memory (BiLSTM), improved backpropagation neural network (IBPNN) and an attention mechanism, also it possesses the strong abilities of nonlinear predicting, long time series feature extracting and important information attention. Furthermore, we present a correlation analysis algorithm to process the various types of huge financial auditing data, which can effectively remove the irrelevant information and discover the correlation relationships in financial auditing data before the BiLSTM-Attention-IBPNN method runs on it. The experimental results proved that our proposed method has better performances and evaluation results in anomaly detection compared with the state-of-the-art methods, also significantly improves the anomaly detection quality and efficiency for financial auditing.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d58a29d8c61357aa304132e09d3dcd65",
  "timestamp": "2025-05-15T02:09:48.451567"
}