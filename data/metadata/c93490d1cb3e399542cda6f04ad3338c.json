{
  "id": 2394,
  "title": "SENSITIVITY ANALYSIS IN APPLICATIONS WITH DEVIATION, RISK, REGRET, AND ERROR MEASURES",
  "abstract": "The envelope formula is obtained for optimization problems with positively homogeneous convex functionals defined on a space of random variables. Those problems include linear regression with general error measures and optimal portfolio selection with the objective function being either a general deviation measure or a coherent risk measure subject to a constraint on the expected rate of return. The obtained results are believed to be novel even for Markowitz's mean variance portfolio selection but are far more general and include explicit envelope relationships for the rates of return of portfolios that minimize lower semivariance, mean absolute deviation, deviation measures of GP-type and semi -L-P type, and conditional value-at-risk. In each case, the envelope theorem yields explicit estimates for the absolute value of the difference between deviation/risk of optimal portfolios with the unperturbed and perturbed asset probability distributions in terms of a norm of the perturbation.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c93490d1cb3e399542cda6f04ad3338c",
  "timestamp": "2025-05-15T01:06:38.027328"
}