{
  "id": 2041,
  "title": "What Went Wrong? Identifying Risk Factors for Popular Negative Consequences in AI",
  "abstract": "The technologies that we have come to know as artificial intelligence (AI), such as machine learning, deep learning, computer vision, and natural language processing, are becoming general-purpose tools that significantly impact organizational and societal economic and social structures. However, that impact has not been entirely positive. We have already seen many projects where undesirable or negative consequences of AI systems have harmed their respective organizations in social, financial, and legal spheres. In this study, we examine common intended objectives and risk factors that lead to negative consequences in AI. Using a qualitative approach, we propose a unifying theoretical framework for negative consequences in AI projects. We analyzed 840 quotes from key informants about 30 unique AI projects using multiple news articles for each project. We identified intended objectives for implementing AI systems that lead to negative consequences through various linking risk factors.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e3f027e699155650353827e71d2560f8",
  "timestamp": "2025-05-15T02:10:25.789799"
}