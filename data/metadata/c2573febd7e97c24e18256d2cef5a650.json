{
  "id": 3320,
  "title": "Decomposing value globally",
  "abstract": "This paper utilizes an international context and revisits the findings which argue that the positive relation between book-to-market ratio and future equity returns is driven by historical changes in firm size in the US. After confirming these results in the US setting both in the original and a more recent sample period, we find that they do not hold in regions outside the US. In the international sample, book-to-market ratio has a significantly positive relation with future equity returns even after changes in firm size are controlled for in regression analyses. This positive relation is again visible when the orthogonal component of book-to-market ratio (which is independent of changes in firm size) is used as a sorting variable in portfolio analyses. Country-level analyses confirm the findings from regional analyses.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c2573febd7e97c24e18256d2cef5a650",
  "timestamp": "2025-05-15T01:15:54.774459"
}