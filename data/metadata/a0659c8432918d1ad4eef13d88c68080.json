{
  "id": 146,
  "title": "World energy futures market efficiency and its determinants; evidence from white noise test based on block-wise wild bootstrap approach",
  "abstract": "The study examines the time-varying efficiency of nine energy futures traded across different exchanges with varied trade settlement methods and pricing currency using the novel white noise test based on block-wsie wild bootstrap approach. The advantage of using this approach is that it relaxes the assumption of serial independence and the martingale difference sequence. Time-varying efficiency is observed for all nine energy futures consistent with the Adoptive Market Hypothesis (AMH). The three most (least) efficient energy futures based on efficiency ratio are Coal, Gasoline and NaturalGas (WTI, Brent and OmanCrude). Quantile regression analysis shows that the higher the level of illiquidity furthers the level of efficiency towards inefficiency (high level of efficiency) in the case of a less (more) efficient market. A mixed impact of volatility is observed across energy futures. No differential impact is found due to exchange, method of settlement and pricing currency. The robustness of the results is investigated, policy implications are discussed, and further research is recommended.",
  "year": 2024,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a0659c8432918d1ad4eef13d88c68080",
  "timestamp": "2025-05-15T01:28:35.413071"
}