{
  "id": 4672,
  "title": "Investors' attention and overpricing of IPO: an empirical study on China's growth enterprise market",
  "abstract": "In China, the Growth Enterprise Market (GEM) is a brand new market that provides an additional way of financing for firms with good growth potential. Whereas there has been a massive amount of stocks breaking on the first trading day since 2010, the risk of IPO's overpricing in China's GEM has drawn more and more attention in recent years. Based on the theory of behavioral finance and limited attention, investors' attention may be a quite indicative determinant to the IPO overpricing. Accordingly, we collected data from Internet including online stock forums and search engines, then built multiple investors' attention metrics that were distinct to the traditional metrics from the offline stock market. In the empirical study, we built regression models to dig out the determinants of IPO's overpricing and found that the hybrid model containing both online metrics and financial metrics outperformed the others considerably. The adjusted R-square of the hybrid model containing both online metrics and financial metrics is as high as 82.8%, in contrast to 18.2% for the model containing only the financial metrics and 59.3% for that containing only investors' attention.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7e4a797c79cbb3ff8a82648643ecc417",
  "timestamp": "2025-05-15T02:39:57.423607"
}