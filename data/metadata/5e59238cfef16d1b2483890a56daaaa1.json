{
  "id": 112,
  "title": "Robust portfolio management: A novel multi-task learning model fusing predicted returns and residual data under the framework of Mean-VaR",
  "abstract": "We investigate how to build a robust portfolio by introducing a novel multi-task learning model that fuses predicted returns and residual data to assess the portfolio risk under the decision-making framework of Mean-VaR. A common way to build a portfolio is to predict the return of assets and then allocate weights according to the predicted return and corresponding risk. However, predicting asset returns accurately in financial markets remains a challenge. To improve prediction accuracy and, more importantly, effectively reduce risk in the portfolio, we adopt the multi-task learning anomaly detection (MTLAD). In this model, predicting asset returns using deep learning model (long short-term memory, LSTM) is the main task, and anomaly detection is the auxiliary task. We then combine the predicted returns and residual data to evaluate the risk measure when allocating the asset weights. Furthermore, we perform an extensive numerical investigation based on data in the Chinese financial market. Results obtained show that our robust portfolio management approach has great potential compared with multiple benchmarks.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5e59238cfef16d1b2483890a56daaaa1",
  "timestamp": "2025-05-15T01:33:49.079589"
}