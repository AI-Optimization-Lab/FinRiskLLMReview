{
  "id": 4763,
  "title": "Forecasting Extreme Returns in Financial Markets: A Discrete Duration Framework",
  "abstract": "We introduce a new dynamic peaks-over-threshold (POT) model for predicting both the timing and the size of extreme losses in financial markets. The novelty of our approach lies in treating the times at which the magnitude of loss exceeds a sufficiently large threshold as a realization of a discrete random variable. The conditional hazard function with respect to the time in-between consecutive extreme losses - and hence, the risk of an extreme loss over the next time unit - is described using two lifetime distributions: the discrete Weibull and the discrete Burr. To consider the clustering of extreme losses, the scale parameters of these discrete distributions vary with time and have the functional form of autoregressive conditional duration (ACD) models. Accordingly, the probability of an extreme loss over the next unit of time depends on times of extreme losses in the past and the period that has elapsed since the last such event. We demonstrate how to predict the value at risk (VaR) from the discrete-duration POT model and empirically confirm that this new approach provides a good alternative to the ACD-POT models outlined in the literature.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "835d2207c71fe4387882bd53484d25a4",
  "timestamp": "2025-05-15T02:41:00.770275"
}