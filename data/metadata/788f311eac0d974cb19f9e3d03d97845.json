{
  "id": 727,
  "title": "A short-term capacity trading method for semiconductor fabs with partnership",
  "abstract": "This paper presents a capacity trading method for two semiconductor fabs that have established a capacity-sharing partnership. A fab that is predicted to have insufficient capacity at some workstations in a short-term period (e.g. one week) could purchase tool capacity from its partner fab. The population of such a capacity-trading portfolio may be quite huge. The proposed method involves three modules. We first use discrete-event simulation to identify the trading population. Secondly, some randomly sampled trading portfolios with their performance measured by simulation are used to develop a neural network, which can efficiently evaluate the performance of a trading portfolio. Thirdly, a genetic algorithm (GA) embedded with the developed neural network is used to find a near-optimal trading portfolio from the huge trading population. Experiment results indicate that the proposed trading method outperforms two other bench-marked methods in terms of number of completed operations, number of wafer outs, and mean cycle time. (c) 2006 Elsevier Ltd. All rights reserved.",
  "year": 2007,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "788f311eac0d974cb19f9e3d03d97845",
  "timestamp": "2025-05-15T00:47:18.620468"
}