{
  "id": 1445,
  "title": "Network analysis of pension funds investments",
  "abstract": "In this paper, we analyze the Italian pension funds and their declared benchmarks, which are market indexes. Within this perspective, the amounts invested in accord to the declared benchmarks can be analyzed like as a portfolio of benchmarks. We aim at understanding whether the pension funds investments are in line with the optimal portfolios which can be built through the declared benchmarks. To achieve the results, we set up a portfolio optimization problem building two networks of pension funds: one based on the (Pearson) correlation, and the other measuring the tail correlation. For each network, we use the local clustering coefficients to describe the level of connectivity, and we insert it in the risk function. This approach allows us to consider the network measures directly in the portfolio optimization model. We compare the results with the classical Markowitz setting, and we find a new efficient frontier overperforming the Markowitz one. A comparison among the performances of pension funds and their declared portfolio of benchmarks is also reported. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6da5c61f0abeabccb02f7f001ab9419f",
  "timestamp": "2025-05-15T00:56:08.252547"
}