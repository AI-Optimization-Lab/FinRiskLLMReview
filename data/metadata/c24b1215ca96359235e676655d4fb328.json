{
  "id": 2133,
  "title": "Does inter-region portfolio diversification pay more than the international diversification?",
  "abstract": "Recent literature supports the view that returns of most of the international equity markets are significantly integrated. However, diversification based across different regions remains a focus of attention for the investment community. We examine the presence of returns integration among BRICS, Latin American, and emerging and frontier Asian equity markets by utilizing panel co-integration and panel regression, i.e., fully modified ordinary least square (FMOLS), to examine portfolio diversification opportunities. Our sampling of data comprises of daily stock return from 1st September 1997 to 30th November 2018. Our findings highlight that within the region, portfolio diversification does not provide optimal returns. Furthermore, empirical results from the vector error correction model (VECM) suggest the benefits of portfolio diversification in the short-run. In particular, not all equity markets are significantly linked with gold, oil, and forex markets. Our empirical analysis reveals that within the region, equity-commodity portfolios may lead to greater diversification gains. (C) 2021 Board of Trustees of the University of Illinois. Published by Elsevier Inc. All rights reserved.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c24b1215ca96359235e676655d4fb328",
  "timestamp": "2025-05-15T01:03:03.811774"
}