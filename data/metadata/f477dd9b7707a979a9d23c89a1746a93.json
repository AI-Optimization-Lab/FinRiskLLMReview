{
  "id": 2263,
  "title": "Research on the Application Methods of Large Language Model Interpretability in FinTech Scenarios",
  "abstract": "In the field of FinTech, large language models have increasingly been widely applied, such as in credit risk assessment, investment strategy analysis, market trend prediction. However, the interpretability of these models has always been a focal point of concern in both industry and academia. Although neural network-based large models exhibit excellent performance in terms of prediction accuracy, their internal decision-making processes often resemble a black box, making them difficult to interpret. This issue is particularly acute in FinTech scenarios, where financial decisions involve significant economic interests and legal liabilities. Therefore, this paper will investigate the application of interpretability of large models in FinTech scenarios. Firstly, we will review and summarize the existing research progress on the interpretability of neural networks. Then, we will conduct experimental analysis tailored to the specific characteristics and needs of FinTech scenarios. Finally, based on the results of the experimental research, we will explore effective application methods of large-scale neural network models in the field of FinTech. We anticipate that these studies will serve as valuable references for decision-makers, researchers, and practitioners in the field of FinTech.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f477dd9b7707a979a9d23c89a1746a93",
  "timestamp": "2025-05-15T02:13:31.717221"
}