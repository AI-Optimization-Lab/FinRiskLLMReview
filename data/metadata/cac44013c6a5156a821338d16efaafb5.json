{
  "id": 1896,
  "title": "Risk Measurement of the National Social Security Fund Portfolio Based on the GARCH-VaR Model",
  "abstract": "National Social Security Fund is a strategic fund, the key is how to control risk, based on increasing the value. Because the traditional method of risk measurement requires more stringent assumptions in computing variance of the portfolio, as well as other deficiencies, in this paper, we use GARCH-VaR model to do risk measurement and related inspections of National Social Security Fund, in accordance with return's characteristics such as spikes, thick tail, clustering. The paper uses Shanghai and Shenzhen stock markets' top ten stocks heavily held by the social security fund as samples for analysis, ranging from April 1, 2008 to September 30, 2010. The results of risk measurement show that the social security fund equity portfolio faced greater downside risk in 2008 and market risk have had greater impact on stock returns of social security fund. Thus China must improve the capital market as soon as possible to avoid a greater possibility of losing more on social security fund.",
  "year": 2012,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cac44013c6a5156a821338d16efaafb5",
  "timestamp": "2025-05-15T01:01:06.061413"
}