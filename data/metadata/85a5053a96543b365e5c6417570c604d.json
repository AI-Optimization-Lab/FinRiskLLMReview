{
  "id": 2178,
  "title": "Polynomial response surface-based transformation function for the performance improvement of low-fidelity models for concrete gravity dams",
  "abstract": "The behavior of concrete gravity dams under seismic loading is a complex engineering problem dependent on a wide range of variables. Probabilistic methods can be used to evaluate the capacity of an individual or a portfolio of dams to withstand seismic events. However, due to the high number of re-evaluations required by such methods, simplified models that may not fully capture the complexity of the problem are frequently adopted in the evaluations. For a portfolio of dams that requires geometric uncertainty to be included in the list of controlled variables, the number of re-evaluations increases even further. This is a common engineering problem, where the cost-performance trade-off must be evaluated for every project. To address this issue, this study proposes a machine learning-based transformation function that improves the results obtained with a simplified method by converting low-fidelity data into high-fidelity data. The proposed procedure is applied to analyze the seismic response of a dam-reservoir-foundation system considering three approaches for geometric uncertainty, with increasing complexity. The final function takes as input low-fidelity observations, as well as geometric, material and seismic parameters, and outputs improved observations, with accuracy levels comparable to those obtained with a high-fidelity model but at a much lower cost. The sliding factor of safety resulting from a pseudostatic analysis is taken as the low-fidelity observation, and the sliding displacement from a nonlinear finite element analysis is selected as the high-fidelity observation. The resulting transformation function is then used to generate fragility curves for a well-documented case study dam, and the results using the proposed methodology and from a traditional fragility analysis are compared. It is observed that the proposed methodology to generate transformation functions is capable of correlating methods with radically different hypotheses, precision levels and even different outputs.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "85a5053a96543b365e5c6417570c604d",
  "timestamp": "2025-05-15T01:03:36.735684"
}