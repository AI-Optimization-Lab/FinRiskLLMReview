{
  "id": 3720,
  "title": "Revisiting time-varying dynamics in stock market forecasting: A multi-source sentiment analysis approach with large language model",
  "abstract": "This paper presents the Heterogeneous Dynamic Seemingly Unrelated Regression with Dynamic Linear Models (HD-SURDLM), an innovative framework for stock return prediction that combines cutting-edge sentiment analysis with dynamic financial modeling. The model integrates sentiment data from 2.5 million Twitter posts and various news sources, utilizing state-of-the-art sentiment analysis tools such as VADER, TextBlob, and RoBERTa. HD-SURDLM refines Gibbs sampling for enhanced numerical stability and efficiency while capturing cross-sectional dependencies across multiple assets such as a portfolio. The model consistently outperforms traditional methods like LSTM, Random Forest, and RNN in forecasting accuracy. Empirical results show a 1.02% improvement in 1-day horizon forecasts, a 0.42% gain for 20-day predictions, and a 0.36% increase for 50-day forecasts. By effectively merging public sentiment with dynamic asset modeling, HD-SURDLM offers substantial improvements in short- and long-term prediction accuracy. Its capacity to capture both crosssectional insights and temporal dynamics makes it an invaluable tool for investors, traders, and financial institutions navigating sentiment-driven markets. HD-SURDLM not only enhances predictive accuracy but also provides a robust decision-support system for financial stakeholders.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3d4d4f050a48092324a79707dfa47af3",
  "timestamp": "2025-05-15T01:20:03.484488"
}