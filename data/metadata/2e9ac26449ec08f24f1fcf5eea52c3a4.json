{
  "id": 5460,
  "title": "Mixed-frequency quantile regressions to forecast value-at-risk and expected shortfall",
  "abstract": "Although quantile regression to calculate risk measures is widely established in the financial literature, when considering data observed at mixed-frequency, an extension is needed. In this paper, a model is built on a mixed-frequency quantile regressions to directly estimate the Value-at-Risk (VaR) and the Expected Shortfall (ES) measures. In particular, the low-frequency component incorporates information coming from variables observed at, typically, monthly or lower frequencies, while the high-frequency component can include a variety of daily variables, like market indices or realized volatility measures. The conditions for the weak stationarity of the daily return process are derived and the finite sample properties are investigated in an extensive Monte Carlo exercise. The validity of the proposed model is then explored through a real data application using two energy commodities, namely, Crude Oil and Gasoline futures. Results show that our model outperforms other competing specifications, on the basis of some popular VaR and ES backtesting test procedures.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2e9ac26449ec08f24f1fcf5eea52c3a4",
  "timestamp": "2025-05-15T02:47:43.098020"
}