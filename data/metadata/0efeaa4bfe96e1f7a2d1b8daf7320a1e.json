{
  "id": 3901,
  "title": "Learning Dynamic Dependencies With Graph Evolution Recurrent Unit for Stock Predictions",
  "abstract": "Investment decisions and risk management require understanding the time-varying dependencies between stocks. Graph-based learning systems have emerged as a promising approach for predicting stock prices by leveraging interfirm relationships. However, existing methods rely on a static stock graph predefined from finance domain knowledge and large-scale data engineering, which overlooks the dynamic dependencies between stocks. In this article, we present a novel framework called graph evolution recurrent unit (GERU), which uses a dynamic graph neural network to automatically learn the evolving dependencies from historical stock features, leading to better predictions. Our approach consists of three parts: first, we develop an adaptive dynamic graph learning (ADGL) module to learn latent dynamic dependencies from stock time series. Second, we propose a clustered ADGL (clu-ADGL) to handle large-scale time series by reducing time and memory complexity. Third, we combine the ADGL/clu-ADGL with a graph-gated recurrent unit to model the temporal evolutions of stock networks. Extensive experiments on real-world datasets show that our proposed methods outperform existing methods in predicting stock movements, capturing meaningful dynamic dependencies and temporal evolution patterns from the financial market, and achieving outstanding profitability in portfolio construction.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0efeaa4bfe96e1f7a2d1b8daf7320a1e",
  "timestamp": "2025-05-15T02:30:58.384337"
}