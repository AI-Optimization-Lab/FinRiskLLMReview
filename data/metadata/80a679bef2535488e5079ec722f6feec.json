{
  "id": 2332,
  "title": "Eliciting Risk Preferences Experimentally versus Using a General Risk Question. Does Financial Literacy Bridge the Gap?",
  "abstract": "The study investigates the stability of financial risk preference choices elicited from subjects by way of two methods, namely: experimentally elicited incentivized revealed risk preferences (IRRP) and (self-reported) perceived willingness to take a financial risk (PWTFR). The research further examines whether financial literacy (a human capital aspect) helps in reducing the gap between IRRP and PWTFR choices made by subjects. A total of 193 university students (where 53% were female) participated in the study. The subjects completed IRRP choices from four multiple price list (MPL) risk preference tasks and a financial literacy questionnaire. There is a tendency to anchor at extremes of risk-seeking behavior when subjects self-report their PWTFR choices. A paired t-test analysis of the two methods shows that the average responses from the two methods are significantly different. A random effect (RE) panel regression shows that an increase in financial literacy narrows the gap between IRRP and PWTFR choices. The study's findings show that responses by subjects from a PWTFR general risk question (GRQ) and IRRP experiment are unstable and inconsistent. What people say in a survey does not always translate into what they do when faced with a risk preference choice dilemma. Financial literacy helps individuals to predict their risk attitudes more precisely.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "80a679bef2535488e5079ec722f6feec",
  "timestamp": "2025-05-15T02:14:19.578880"
}