{
  "id": 768,
  "title": "Recognizing the pattern of beta coefficient based on rough sets and improved SVM",
  "abstract": "Systematic risk (Beta) which is presented by beta is the avoidless risk on the stock market Beta is calculated by linear analysis between the prices of stocks and the security index of stock market However, many studies have showed there are stronger relationships between beta and financial ratios. Therefore, a hybrid intelligent system is applied to recognize the clusters of beta (systematic risk), combining rough set approach and improved SVM. We can get reduced information table with no information loss by rough set approach. And then, this reduced information is used to develop classification rules and train SVM. At the same time, in order to improve the general recognizing ability of SVM, we make use of the particle swarm algorithm to optimize the SVM, and obtain appropriate parameters. The rationale of our hybrid system is using rules developed by rough sets for an object that matches any of the rules and SVM for one that does not match any of them. The effectiveness of our methodology was verified by experiments comparing BP neural networks with our approach.",
  "year": 2007,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "765312457b87b171ffd501e258a104f2",
  "timestamp": "2025-05-15T01:55:07.982794"
}