{
  "id": 3129,
  "title": "Bayesian Value-at-Risk with product partition models",
  "abstract": "In this paper we propose a novel Bayesian methodology for Value-at-Risk computation based on parametric Product Partition Models. Value-at-Risk is a standard tool for measuring and controlling the market risk of an asset or portfolio, and is also required for regulatory purposes. Its popularity is partly due to the fact that it is an easily understood measure of risk. The use of Product Partition Models allows us to remain in a Normal setting even in the presence of outlying points, and to obtain a closed-form expression for Value-at-Risk computation. We present and compare two different scenarios: a product partition structure on the vector of means and a product partition structure on the vector of variances. We apply our methodology to an Italian stock market data set from Mib30. The numerical results clearly show that Product Partition Models can be successfully exploited in order to quantify market risk exposure. The obtained Value-at-Risk estimates are in full agreement with Maximum Likelihood approaches, but our methodology provides richer information about the clustering structure of the data and the presence of outlying points.",
  "year": 2012,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "773e669a9d75f3894f067f75a60f2901",
  "timestamp": "2025-05-15T01:13:45.278725"
}