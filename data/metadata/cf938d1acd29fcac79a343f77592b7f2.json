{
  "id": 2481,
  "title": "What makes nonprofit organizations (NPOs) talk transparently about their connections with businesses on Twitter? Insights from nonprofit-business network portfolios and resource dependencies",
  "abstract": "Drawing on network portfolio literature and resource dependence theory, this study investigates how a non- profit's N2B partnership portfolio configurations (i.e., size and industry diversification), reliance on individual donations, and reliance on government grants influence the nonprofit's transparency in disclosing N2B part- nerships on Twitter. We manually coded the level of transparency reflected in 911 tweets sent by 81 leading COVID-19 NPOs mentioning 501 companies from March 1 to July 19, 2020. Social network analysis and regression models were performed to answer the research inquiries. Findings indicate that maintaining a large number of business connections is associated with lowered transparency in N2B communication on Twitter, whereas keeping diverse connections with different business industries relates to increased transparency in N2B communication. NPOs with a stronger reliance on government grants signaled more transparency in N2B parentships on Twitter, but the reliance on individual donations did not influence N2B transparency signaling.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cf938d1acd29fcac79a343f77592b7f2",
  "timestamp": "2025-05-15T01:07:03.115658"
}