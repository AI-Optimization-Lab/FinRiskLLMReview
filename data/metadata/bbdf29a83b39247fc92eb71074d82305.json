{
  "id": 784,
  "title": "Cryptocurrency returns prediction using candlestick patterns analysis and multi-layer deep LSTM neural networks",
  "abstract": "Financial markets are characterised by their dynamic, non-linear, and fluctuating nature. Analysing financial time series in these contexts is a complex and challenging task. Candlestick patterns are recognised as among the most widely used financial tools and offer invaluable insights into market sentiment and psychology. However, manual analysis of these patterns presents significant challenges. Therefore, leveraging machine learning methods becomes a necessity for overcoming these challenges. In this study, a four-step framework was introduced in which the data preparation process is executed on the price data of the 20 cryptocurrencies. Forty-eight candlestick patterns were extracted alongside returns. Employing the long shortterm memory (LSTM) neural network, structured with multiple layers, each specialising in a specific cryptocurrency, enables individualised prediction of market returns. Evaluation of model accuracy and sensitivity is conducted via the confusion matrix, and two distinct trading strategies assess the capital portfolio. The research findings underscore the profitability of the proposed model across all scenarios. Candlestick patterns serve as powerful tools for understanding market sentiments and identifying shifts in market trends. However, their standalone efficacy is limited. Integrating them with other technical analysis tools facilitates more informed decision-making and fosters a deeper understanding of market dynamics.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "bbdf29a83b39247fc92eb71074d82305",
  "timestamp": "2025-05-15T00:47:50.437930"
}