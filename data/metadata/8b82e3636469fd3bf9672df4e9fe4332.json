{
  "id": 2523,
  "title": "RESPONSES OF TURKISH CONSUMERS TO PRODUCT RISK INFORMATION IN THE CONTEXT OF NEGATIVE EWOM",
  "abstract": "This study explores the risk-based effects of negative electronic word-of-mouth (neWOM) perception (financial and performance risks) on electronic word-of-mouth (eWOM) credibility and purchase intention for promotion- and prevention-focused consumers. In this experimental study, a survey was conducted in which 344 people from Turkey participated. Automobiles were chosen as the subject of the survey because they are in the high-risk product category. The results from regression analysis show that the neWOM perception containing financial risk information plays a more significant role than the one containing performance risk in terms of the effect on eWOM credibility for both promotion- and prevention-focused consumers. Moreover, the neWOM containing performance risk has no effect on the eWOM credibility for promotion-focused consumers. When exposed to neWOM, the purchase intentions of both consumers are even more negatively affected by the neWOM information containing performance risk than the one with financial risk content The results and future predictions are discussed.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8b82e3636469fd3bf9672df4e9fe4332",
  "timestamp": "2025-05-15T02:16:32.591851"
}