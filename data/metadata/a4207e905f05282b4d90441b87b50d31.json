{
  "id": 96,
  "title": "Boosting Financial Market Prediction Accuracy With Deep Learning and Big Data: Introducing the CCL Model",
  "abstract": "The accuracy of financial market trend prediction directly impacts investors' decisions and financial institutions' risk management, making it a focal point of research in the financial domain. While traditional statistical methods lay the foundation for market forecasting, they still face limitations in handling complex, nonlinear, and non-stationary financial time series data with accuracy and adaptability. Time series analysis plays a pivotal role in financial market prediction, revealing historical patterns and future trends within the data. Addressing the limitations of existing methods, this study proposes a deep learning-based CEEMDAN-CNN-LSTM model. Leveraging the CEEMDAN decomposition method to capture the multi-frequency components of time series, CNN extracts spatial features, and LSTM learns temporal dependencies. Experimental results demonstrate that the performance of the CEEMDAN-CNN-LSTM model surpasses traditional models on standardized evaluation metrics such as RMSE, MAE, and MAPE across four major financial datasets.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a4207e905f05282b4d90441b87b50d31",
  "timestamp": "2025-05-15T01:33:49.047843"
}