{
  "id": 385,
  "title": "Global Sensitivity Analysis of Various Numerical Schemes for the Heston Model",
  "abstract": "The pricing of financial options is usually based on statistical sampling of the evolution of the underlying under a chosen model, using a suitable numerical scheme. It is widely accepted that using low-discrepancy sequences instead of pseudorandom numbers in most cases increases the accuracy. It is important to understand and quantify the reasons for this effect. In this work, we use Global Sensitivity Analysis in order to study one widely used model for pricing of options, namely the Heston model. The Heston model is an important member of the family of the stochastic volatility models, which have been found to better describe the observed behaviour of option prices in the financial markets. By using a suitable numerical scheme, like those of Euler, Milstein, Kahl-Jackel, Andersen, one has the flexibility needed to compute European, Asian or exotic options. In any case the problem of evaluating an option price can be considered as a numerical integration problem. For the purposes of modelling and complexity reduction, one should make the distinction between the model nominal dimension and its effective dimension. Another notion of average dimension has been found to be more practical from the computational point of view. The definitions and methods of evaluation of effective dimensions are based on computing Sobol' sensitivity indices. A classification of functions based on their effective dimensions is also known. In the context of quantitative finance, Global Sensitivity Analysis (GSA) can be used to assess the efficiency of a particular numerical scheme. In this work we apply GSA based on Sobol sensitivity indices in order to assess the interactions of the various dimensions in using the above mentioned schemes. We observe that the GSA offers useful insight on how to maximize the advantages of using QMC in these schemes.",
  "year": 2020,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "62c29a45ca8a45666d68e8052df84e7e",
  "timestamp": "2025-05-15T01:31:13.206950"
}