{
  "id": 3764,
  "title": "Multilevel Monte-Carlo for computing the SCR with the standard formula and other stress tests",
  "abstract": "This paper studies the multilevel Monte-Carlo estimator for the expectation of a maximum of conditional expectations. This problem arises naturally when considering many stress tests and appears in the calculation of the interest rate module of the standard formula for the SCR. We obtain theoretical convergence results that complement the recent work of Giles and Goda (2019) and give some additional tractability through a parameter that somehow describes regularity properties around the maximum. We then apply the MLMC estimator to the calculation of the SCR at future dates with the standard formula for an ALM savings business on life insurance. We compare it with estimators obtained with Least Squares Monte-Carlo or Neural Networks. We find that the MLMC estimator is computationally more efficient and has the main advantage to avoid regression issues, which is particularly significant in the context of projection of a balance sheet by an insurer due to the path dependency. Last, we discuss the potential of this numerical method and analyse in particular the effect of the portfolio allocation on the SCR at futuredates. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "656a80cc9510bfbe8d99791ea2a31889",
  "timestamp": "2025-05-15T01:20:36.091641"
}