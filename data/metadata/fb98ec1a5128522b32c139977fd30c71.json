{
  "id": 3341,
  "title": "CVAR HEDGING USING QUANTIZATION-BASED STOCHASTIC APPROXIMATION ALGORITHM",
  "abstract": "In this paper, we investigate a method based on risk minimization to hedge observable but nontradable source of risk on financial or energy markets. The optimal portfolio strategy is obtained by minimizing dynamically the conditional value-at-risk (CVaR) using three main tools: a stochastic approximation algorithm, optimal quantization, and variance reduction techniques (importance sampling and linear control variable), as the quantities of interest are naturally related to rare events. As a first step, we investigate the problem of CVaR regression, which corresponds to a static portfolio strategy where the number of units of each tradable assets is fixed at time 0 and remains unchanged till maturity. We devise a stochastic approximation algorithm and study its a.s. convergence and weak convergence rate. Then, we extend our approach to the dynamic case under the assumption that the process modeling the nontradable source of risk and financial assets prices is Markovian. Finally, we illustrate our approach by considering several portfolios in connection with energy markets.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fb98ec1a5128522b32c139977fd30c71",
  "timestamp": "2025-05-15T02:25:13.733809"
}