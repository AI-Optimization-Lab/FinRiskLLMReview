{
  "id": 436,
  "title": "A2GAN: A Deep Reinforcement-Based Learning Algorithm for Risk-Aware in Finance",
  "abstract": "Financial data suffer from missing, unlabeled and unbalanced data, thus weakening the performance of decision-making systems. In addition, the aim of financial institutions is not only to find decision-making models that achieve high scores for the standard metrics (e.g., AUC, accuracy, F-score) but to reduce the risk from miss-classification cases. This paper addresses these problems by proposing a novel framework inspired by reinforcement learning, specifically actor-critic, for decision-making and implementing generative adversarial networks for imputing missing data, as well as utilizing the unlabeled dataset. Moreover, by taking advantage of reinforcement learning, the trained model is calibrated using a customizable reward function, which can be designed for different purposes of financial institutions. We evaluate the framework via real-world financial datasets that only have a small amount of labeled data and exhibit missing data. Our experiment shows promising results where the financial risk is dramatically reduced without too much sacrifice on standard metrics.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3e83d1f0729540ff01da3504db630b93",
  "timestamp": "2025-05-15T01:50:52.641321"
}