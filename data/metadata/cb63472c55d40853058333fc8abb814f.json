{
  "id": 2000,
  "title": "Predicting Hospital Readmission of Diabetics using Deep Forest",
  "abstract": "Diabetes can cause a variety of complications, which also leads to a high rate of repeated admission of patients with diabetes, which greatly increases the pain and financial burden of patients. Higher readmission rates also reduce hospital evaluation and operational efficiency. Therefore, it is urgent to screen out high-risk readmission patients in advance and introduce adjuvant treatment to reduce the probability of readmission. In this study, we propose a deep learning model combining wavelet transform and deep forest to hospital readmission of the diabetic. The proposed model has been tested with real clinical records and compared with several prevalent approaches to patient prediction. The experimental results show that the feature representation transformed by wavelet transform may well represent the original features and the deep forest is able to outperform the state-of-the-art approaches to classify diabetics.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cb63472c55d40853058333fc8abb814f",
  "timestamp": "2025-05-15T02:09:48.424943"
}