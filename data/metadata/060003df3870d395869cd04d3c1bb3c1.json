{
  "id": 3078,
  "title": "Carbon trading price forecasting based on parameter optimization VMD and deep network CNN-LSTM model",
  "abstract": "To meet carbon peak and neutrality targets, accurate carbon trading price forecasting is very important for enterprises making emission reduction decisions. By fusing convolutional neural network (CNN) and long short-term memory network (LSTM), the CNN-LSTM model is constructed. After variational mode decomposition (VMD), several intrinsic mode functions (IMFs) components are obtained and input into the CNN-LSTM model, thus constructing the combined sooty tern optimization algorithm (STOA)-VMD-CNN-LSTM forecasting model. To test this model, the carbon trading prices of the carbon emission trading markets of Hubei, Guangdong and Shenzhen were forecast. The prediction performance of the STOA-VMD-CNN-LSTM model is compared with ARIMA, BP, CNN and LSTM benchmark models and models combining different decomposition technologies. The international carbon trading price (EUR and CER) is used for prediction. Compared with other methods, the developed model makes fewer errors and achieves superior performance. Several important implications are provided for investors and risk managers involved in carbon financial products.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "060003df3870d395869cd04d3c1bb3c1",
  "timestamp": "2025-05-15T02:22:25.082158"
}