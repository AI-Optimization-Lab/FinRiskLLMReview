{
  "id": 2410,
  "title": "Time-varying relative risk aversion: Theoretical mechanism and empirical evidence",
  "abstract": "This paper explores the issue of understanding time-varying relative risk aversion with household-level data on two classical portfolio choice problems. First, we derive an analytic form solution to a parsimonious portfolio choice model with the preference given by Greenwood, Hercowitz and Huffman (1988, GHH), and then, the solution identifies four partial equilibrium effects in our model with the GHH preference on risky shares through two channels and two net effects whose signs hinge on the value of a key structural parameter. Based on household-level data, our empirical results from both mean and quantile regression models show clearly that wealth negatively affects risky shares and the estimated effects are statistically significant and robust, which is in line with the theory. Finally, we show that the GHH preference alone is not sufficient in explaining how risky shares respond to labor income in the household-level data.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9210004c49c7d81f72d89a3d7e518f2b",
  "timestamp": "2025-05-15T01:06:38.078529"
}