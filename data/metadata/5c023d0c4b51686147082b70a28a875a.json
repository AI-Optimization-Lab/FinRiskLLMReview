{
  "id": 554,
  "title": "MODELLING CONDITIONAL VOLATILITY IN STOCK INDICES: A COMPARISON OF THE ARMA-EGARCH MODEL VERSUS NEURONAL NETWORK BACKPROPAGATION",
  "abstract": "The analysis of conditional volatility is a key factor to correctly assess the risk of several financial assets such as shares, bonds or index as well as derivatives (futures and options). The econometric models from the GARCH family are traditionally the most widely used to predict conditional volatility. As an alternative to the econometric models, neural networks can be employed to this end. This paper compares the econometric model ARMA-EGARCH with the neuronal network Backpropagation. Both methodologies have been applied on diverse international stock indices. The main conclusion to be stressed is that the neuronal network can significantly better predict conditional volatility than the econometric model.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5c023d0c4b51686147082b70a28a875a",
  "timestamp": "2025-05-15T01:39:38.318459"
}