{
  "id": 1161,
  "title": "A reduced variance unsupervised ensemble learning algorithm based on modern portfolio theory",
  "abstract": "Unsupervised ensemble learning or consensus clustering has gained popularity due to its ability to combine multiple clustering solutions into a single solution that is robust and often performs better than the individual ones. There have been several approaches to consensus clustering including voting and weighted voting algorithmic schemes. Although there have been several algorithms for adjusting the weights of a consensus clustering all of them are tuned based on some performance characteristic associated with clustering accuracy. In this paper, we propose a method for incorporating weights by taking into consideration the intra algorithmic variability i.e. algorithms that provide solutions with very different performance upon multiple runs. The methodology is inspired by modern portfolio theory and more specifically from Markowitz model for asset allocation where one is trying to identify the most efficient portfolio through the solution of a convex optimization problem. Here, efficiency is defined as the minimum amount of risk for an expected return. We apply this method to different datasets and compare with respect to performance and robustness. The proposed scheme appears to achieve competitive average performance with very low variability.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "06bdda5f5d702836ebe230eae429fb2d",
  "timestamp": "2025-05-15T00:52:33.857657"
}