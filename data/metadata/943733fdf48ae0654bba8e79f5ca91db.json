{
  "id": 1105,
  "title": "On the Classification of Industrial SAT Families",
  "abstract": "The success of portfolio approaches in SAT solving relies on the observation that different SAT solving techniques perform better on different SAT instances. The Algorithm Selection Problem faces the problem of choosing, using a prediction model, the best algorithm from a predefined set, to solve a particular instance of a problem. Using Machine Learning techniques, this prediction is performed by analyzing some features of the instance and using an empirical hardness model, previously built, to select the expected fastest algorithm to solve such instance. Recently, there have been some attempts to characterize the structure of industrial SAT instances. In this paper, we use some structural features of industrial SAT instances to build some classifiers of industrial SAT families of instances. Namely, they are the scale-free structure, the community structure and the self-similar structure. We measure the effectiveness of these classifiers by comparing them to other sets of SAT instances features commonly used in portfolio SAT solving approaches. Finally, we evaluate the performance of this set of structural features when used in a real portfolio SAT solver.",
  "year": 2015,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "943733fdf48ae0654bba8e79f5ca91db",
  "timestamp": "2025-05-15T00:52:00.339382"
}