{
  "id": 6337,
  "title": "Association Between Length of Stay and Readmission for COPD",
  "abstract": "Objectives: Recent financial penalties for high risk-adjusted chronic obstructive pulmonary disease (COPD) readmissions are causing hospitals to search for ways to reduce COPD readmissions. Although some have advocated for increasing the length of stay (LOS) as a method to decrease readmissions, the association between LOS and readmission is unclear. Our primary objective was to examine the association between LOS and readmission among patients admitted for COPD. Study Design: We conducted an observational study of 33,558 veterans admitted to 130 Veterans Affairs hospitals for COPD from October 1, 2008, to September 30, 2011. Methods: We used multivariable regression to separately examine the associations of patient and hospital LOS with 30-day all-cause readmission. Results: At the patient level, compared with short LOS (<3 days), a longer LOS was associated with increased risk for readmission. The adjusted odds ratio was 1.39 (95% confidence interval [CI], 1.18-1.63) for medium LOS (3-4 days) and 2.03 (95% CI, 1.72-2.40) for long LOS (>4 days). On the hospital level, there was no association between LOS and readmission. Conclusions: On a patient level, a longer LOS for COPD hospitalizations was associated with higher risk for readmission, which is likely confounded by the severity of the illness. On a hospital level, LOS was not associated with readmission. These findings imply that, independent of other transitional care practices, altering the hospital LOS may not influence the risk of readmission.",
  "year": 2017,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a0779a643111899a1c44d93cc1389248",
  "timestamp": "2025-05-15T02:57:07.697765"
}