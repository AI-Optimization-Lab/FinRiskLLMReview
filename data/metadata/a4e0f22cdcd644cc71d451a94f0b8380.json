{
  "id": 483,
  "title": "Forecasting returns with machine learning and optimizing global portfolios: evidence from the Korean and US stock markets",
  "abstract": "This study employs a variety of machine learning models and a wide range of economic and financial variables to enhance the forecasting accuracy of the Korean won-U.S. dollar (KRW/USD) exchange rate and the U.S. and Korean stock market returns. We construct international asset allocation portfolios based on these forecasts and evaluate their performance. Our analysis finds that the Elastic Net and LASSO regression models outperform traditional benchmark models in predicting exchange rate and stock market returns, as evidenced by their superior out-of-sample R-squared values. We also identify the key factors crucial for improving the accuracy of forecasting the KRW/USD exchange rate and stock market returns. Furthermore, a machine learning-driven global portfolio that accounts for exchange rate fluctuations demonstrated superior performance. Global portfolios constructed using LASSO (Sharpe ratio = 3.45) and Elastic Net (Sharpe ratio = 3.48) exhibit a notable performance advantage over traditional benchmark portfolios. This suggests that machine learning models outperform traditional global portfolio construction methods.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a4e0f22cdcd644cc71d451a94f0b8380",
  "timestamp": "2025-05-15T00:43:47.428119"
}