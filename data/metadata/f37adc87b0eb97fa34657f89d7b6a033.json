{
  "id": 5897,
  "title": "Molecular Taxonomy and Signatures of Breast Cancer in 2017",
  "abstract": "Breast cancers are best classified according to their level of hormone receptors and HER2 gene expression. Molecular classification has modified this simplistic taxonomy highlighting multiple profiles with different prognosis. It is in this context, and given the need to use targeted therapies, that molecular signatures were developed. Although they differ in methods (qRT-PCR, micro-array, or derivatives), molecular signatures endorse the same objectives: calculate a prognostic score based on the levels of gene expression involved in carcinogenesis, and, if possible, predict response to treatment. Applicable mainly to luminal ER-positive tumors, molecular signatures have proven their prognostic value in large prospective clinical trials and experts now look forward to integrate them in the therapeutic decision, which is currently based on clinico-pathological criteria. Furthermore, compared to the cost of chemotherapy, molecular signatures provide a real financial benefit and help to equilibrate the risk-benefit balance by reducing the use of aggressive treatments that are sometimes ineffective.",
  "year": 2017,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f37adc87b0eb97fa34657f89d7b6a033",
  "timestamp": "2025-05-15T02:52:31.490948"
}