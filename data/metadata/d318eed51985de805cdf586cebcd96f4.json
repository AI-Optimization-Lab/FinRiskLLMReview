{
  "id": 658,
  "title": "Sparse Bayesian models: Bankruptcy-predictors of choice?",
  "abstract": "Making inferences and choosing appropriate responses based on incomplete, uncertainty and noisy data is challenging in financial settings particularly in bankruptcy detection. In an increasingly globalized economy, bankruptcy results both in huge economic losses and tremendous social impact. While early prediction for a bankruptcy, if done appropriately, is of great importance to banks, insurance firms, creditors, and investors, the need of substantially more accurately predicting models becomes crucial. This problem has been approached by various methods ranging from statistics to machine learning, however they find a class decision estimate rather than a probabilistic confidence of the class distribution. In this paper we show that sparse Bayesian models also known as Relevance Vector Machine (RVMs) are superior to the state-of-the-art machine learning algorithms such as Support Vector Machines (SVMs) therefore leading to predictors of choice. The advantage of RVM approach is that the classifier can yield a decision function that is much sparser than the SVM while maintaining its detection accuracy. This can lead to significant reduction in the computational complexity of the decision function, thereby making it more suitable for realtime applications. Preliminary experiments on Coface Data set (French credit risk provider) show that RVM classifiers outperform SVM, lead to more sparse and accurate prediction models.",
  "year": 2006,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d318eed51985de805cdf586cebcd96f4",
  "timestamp": "2025-05-15T01:53:24.982093"
}