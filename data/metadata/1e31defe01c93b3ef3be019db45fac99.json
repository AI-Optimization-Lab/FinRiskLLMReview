{
  "id": 3637,
  "title": "Beta, size and value effects on the JSE, 1994-2007",
  "abstract": "This paper builds on the observations of Van Rensburg and Robertson (2003a), who found persistent size and price-earnings effects in the cross-section of returns on the JSE, but surprisingly found that beta had, if anything, an inverse relationship with returns. Based on stock returns from January 1994 to October 2007, this portfolio-based study finds support for these earlier findings. However, when betas are estimated by the Dimson Aggregated Coefficients method with a lead and lag of at least three months, to compensate for the weaknesses of Ordinary Least Squares regression in the face of thin trading, the relationship between beta and return loses its statistical significance. We are left with the conclusion that beta has no predictive power for returns on the JSE, invalidating the CAPM, at least as it is commonly applied, based on a market proxy of the All-Share Index. We find further that the size premium is concentrated in the smallest stocks on the JSE, with no significant difference in returns between the four largest quintiles, and tentative evidence that it has been reducing over time.",
  "year": 2011,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1e31defe01c93b3ef3be019db45fac99",
  "timestamp": "2025-05-15T01:19:00.824281"
}