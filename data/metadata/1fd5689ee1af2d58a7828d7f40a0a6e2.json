{
  "id": 447,
  "title": "Predicting Churn of Credit Card Customers Using Machine Learning and AutoML",
  "abstract": "Nowadays, a major concern for most retail banks is the risk that originates from customer fluctuation and that increases the cost of almost every financial product. In this work, the authors compared different approaches and algorithms to predict the relevant features that affect the customer churn, which means we can find ways to reduce the customer churn and create financial inclusion. This research was conducted by applying different machine learning techniques like decision tree classifier, random forest classifier, AdaBoost classifier, extreme gradient boosting, and balancing data with random under-sampling and random oversampling. The authors have also implemented AutoML to further compare different models and improve the accuracy of the model to predict customer churn. It was observed that applying AutoML highest accuracy model gave the accuracy of 97.53% in comparison to that of the decision tree classifier, which was 93.48% with the use of low processing power. Important features were `total transaction amount' and `total transaction count' to predict customer churn for a given dataset.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1fd5689ee1af2d58a7828d7f40a0a6e2",
  "timestamp": "2025-05-15T01:50:52.675800"
}