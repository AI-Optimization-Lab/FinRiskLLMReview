{
  "id": 3323,
  "title": "Conditional VaR estimation using Pearson's type IV distribution",
  "abstract": "This paper presents a new value at risk (VaR) estimation model for equity returns time series and tests it extensively on Stock Indices of 14 countries. Two most important stylized facts of such series are volatility clustering, and non-normality as a result of fat tails of the return distribution. While volatility clustering has been extensively studied using the GARCH model and its various extensions, the phenomenon of non-normality has not been comprehensively explored, at least in the context of VaR estimation. A combination of extreme value theory (EVT) and GARCH has been explored to analyze financial data showing non-normal behavior. This paper proposes a combination of the Pearson's Type IV distribution and the GARCH (1, 1) approach to furnish a new method with superior predictive abilities. The approach is back tested for the entire sample as well as for a holdout sample using rolling windows. (C) 2007 Elsevier B.V. All rights reserved.",
  "year": 2008,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "11c73ed9613d32b8badf5d504d61378e",
  "timestamp": "2025-05-15T02:25:13.646685"
}