{
  "id": 3090,
  "title": "Hybrid quantile regression estimation for time series models with conditional heteroscedasticity",
  "abstract": "Estimating conditional quantiles of financial time series is essential for risk management and many other financial applications. For time series models with conditional heteroscedasticity, although it is the generalized auto-regressive conditional heteroscedastic (GARCH) model that has the greatest popularity, quantile regression for this model usually gives rise to non-smooth non-convex optimization which may hinder its practical feasibility. The paper proposes an easy-to-implement hybrid quantile regression estimation procedure for the GARCH model, where we overcome the intractability due to the square-root form of the conditional quantile function by a simple transformation. The method takes advantage of the efficiency of the GARCH model in modelling the volatility globally as well as the flexibility of quantile regression in fitting quantiles at a specific level. The asymptotic distribution of the estimator is derived and is approximated by a novel mixed bootstrapping procedure. A portmanteau test is further constructed to check the adequacy of fitted conditional quantiles. The finite sample performance of the method is examined by simulation studies, and its advantages over existing methods are illustrated by an empirical application to value-at-risk forecasting.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "aaba52ddd43d234578f6916334315e37",
  "timestamp": "2025-05-15T02:22:25.139668"
}