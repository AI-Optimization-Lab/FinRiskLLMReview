{
  "id": 3238,
  "title": "A roadmap for achieving scalable, safe, and low-cost direct air carbon capture and storage",
  "abstract": "Direct air carbon capture and storage (DACCS) involves a set of approaches for capturing CO2 directly from the air and its subsequent long-term storage. DACCS is at an early stage of technical development and currently faces a variety of challenges, including high cost and energy requirements. Building on publicly available data, this paper provides: (i) an overview and classification of DACCS systems, (ii) a harmonization of technical and economic performance of direct air capture technologies, (iii) a comprehensive list of technical- and infrastructure-based obstacles to scaling DACCS systems, and (iv) a roadmap and list of priority initiatives for research, development, demonstration, and deployment of DACCS. Our intent is to drive progress against high-impact priority actions, with a focus on accelerating research, development, and deployment of safe, scalable, and low cost DACCS as a component of the broader carbon dioxide removal portfolio. A roadmap that delineates the major hurdles and essential RD&D actions to enable large-scale DACCS deployment.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "80c31e5a566f3b2df2f7016e5217fcc4",
  "timestamp": "2025-05-15T01:14:55.078310"
}