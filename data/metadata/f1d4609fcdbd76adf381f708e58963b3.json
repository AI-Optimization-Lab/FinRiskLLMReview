{
  "id": 6084,
  "title": "Fraud Detection Based on Credit Review Texts with Dual Channel Memory Networks",
  "abstract": "With the rapid development of the automotive finance market in China, fraudulent behaviors present new characteristics such as intellectualization and high concealment. Graph neural networks and memory networks have strong capabilities for processing the textual data containing massive complex associations, providing a new perspective for fraud detection. During the operation of an automotive finance company, a large amount of credit review texts with recording the customers' multidimensional data are accumulated. These texts contain information that is helpful for risk management, but have not been well explored. In order to effectively identify fraud risks, we propose a fraud detection method based on credit review texts with dual-channel memory network, which combines graph and text memory networks. By utilizing pre-trained language models, the text data for credit review text is encoded into semantic vectors. The graph memory network module and the text memory network module are then employed to extract graph features and text features corresponding to the credit review text. Finally, the generated results from the three modules are fused and input into a classification network to obtain the final determination of financial fraud risk. Comparative experiments with baseline models demonstrate the validity of our model in fraud detection.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f1d4609fcdbd76adf381f708e58963b3",
  "timestamp": "2025-05-15T02:54:38.798361"
}