{
  "id": 6758,
  "title": "Predicting stock market reactions to financial news using naive Bayesian classifiers",
  "abstract": "Recently most research trying to build automatic trading systems have focused on various forms of Technical Analysis (TA) even though it has generally been met by skepticism by the academics. In this paper we argue that what really influences the market is news, and that little research trying to deal with this problem using soft computational techniques has been done. We use Naive Bayesian classifiers for text classification and try to predict the future trends of the Swedish All-Share index using the titles of financial news. Our results show that Naive Bayesian Classifiers are well suited for this task and that they can successfully distinguish between positive and negative news. Even though we label all articles in a day as having the same effect on the market the predictions are quite good, but they could probably be significantly improved if applied on intraday data with more accurate labeling. We also found that using single words as features in the classifier results in a higher overall accuracy, but using pairs of words increased recall of negative news, which is good for risk aversive investment policies.",
  "year": 2003,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "da2138cc82e4994e824f8f11a0d0798f",
  "timestamp": "2025-05-15T03:01:03.698057"
}