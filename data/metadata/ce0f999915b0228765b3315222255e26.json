{
  "id": 1494,
  "title": "A Quantile Regression approach for the analysis of the diversification in non-life premium risk",
  "abstract": "This paper concerns the study of the diversification effect involved in a portfolio of non-life policies priced via traditional premium principles when individual pure premiums are calculated via Quantile Regression. Our aim is to use Quantile Regression to estimate the individual conditional loss distribution given a vector of rating factors. To this aim, we make a comparison of the outcomes obtained via Quantile Regression with the widely used industry standard method based on generalized linear models. Then, considering a specific premium principle, we calculate individual pure premium by means of a specific functional of the conditional loss distribution, the standard deviation. We determine the portfolio risk margin according to the Solvency 2 framework and then we allocate it over each policy in a way consistent with his/her riskiness. Indeed, considering a portfolio of heterogeneous policies, we determine the individual reduction of the safety loading, due to the diversification, and we measure the risk contribution of each individual.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ce0f999915b0228765b3315222255e26",
  "timestamp": "2025-05-15T00:56:38.936398"
}