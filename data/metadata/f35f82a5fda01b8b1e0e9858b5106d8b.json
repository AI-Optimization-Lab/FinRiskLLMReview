{
  "id": 1624,
  "title": "Computing capital-at-risk by multivariate maxima of moving maxima processes",
  "abstract": "To determine an efficient VaR(Value-at-Risk) or CaR(Capital-at-Risk) the difficulty is to model the extreme events. In many circumstances, extremal observations appear to be clustered in time. Nor univariate nor multivariate extreme value theory is adequate to describe this kind of clustering of extreme events in a time series. In this paper, we extend the estimation of the M4 process developed by Zhang ([14]) from the simple b(1d)'(x) functions to b(dd)'(x) functions. We use MSCI (Morgan & Stanley Capital International) equity indexes returns modeled by M4 (multivariate maxima of moving maxima process) to compute CaR of the market portfolio composed of five markets: China Index, Germany Index, Hong Kong Index, Japan Index and USA Index. Thus, by providing a tool going beyond the difficulties of modeling the extreme financial market falls in n dimensions, we find that portfolio return decrease over a certain VaR and the CaR is be constant if VaR is over same value of VaR. This is opposite to general portfolio theory attesting that more risky markets have higher gain.",
  "year": 2005,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f35f82a5fda01b8b1e0e9858b5106d8b",
  "timestamp": "2025-05-15T00:57:46.885954"
}