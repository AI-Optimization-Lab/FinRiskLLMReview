{
  "id": 1015,
  "title": "On integrating unsupervised and supervised classification for credit risk evaluation",
  "abstract": "Credits granting are very important parts of banks' activities, as they may give big profits, but there is a big risk connected with making decisions in this area and mistakes may be very costly for financial institutions. The main idea in credit risk evaluation investigations consists of building classification rules that assign properly bank customers as good or bad payers. In the paper, the system based on combination of unsupervised and supervised classification is proposed. In the first step, by using clustering algorithm, clients are segmented into groups with similar features. In the second step, decision trees are built and classification rules, for each group of clients, are defined. To avoid redundancy, different attributes are taken into account during each kind of classification. The proposed approach allows for using different rules within the same data set, and for defining more accurately clients with high risk. The system was tested on the real credit-risk data sets. Some exemplary results concerning different groups of clients are presented.",
  "year": 2007,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0434070c833b08654c16bb15bba898e4",
  "timestamp": "2025-05-15T01:58:08.436819"
}