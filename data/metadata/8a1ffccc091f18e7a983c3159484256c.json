{
  "id": 3234,
  "title": "A portfolio return definition coherent with the investors' preferences",
  "abstract": "In this paper, we deal with the portfolio selection problem from the point of view of different non-satiable investors: namely, risk-averse, risk-seeking, neither risk-averse nor risk-seeking. In particular, using a well-known ordering classification, we first identify different definitions of return according to the investors' preferences. The new definitions of return are based on the conditional expected value between the random wealth assessed at different times. Secondly, we propose an estimator of the conditional expected value between random variables, and we prove that it is consistent. Using the new estimator of the conditional expected value, we are able to forecast the investors' behaviour by comparing the wealth sample path obtained by taking their different preferences into account. We then examine three alternative performance measures based on dynamic and static definitions of risk applied to the new return definitions. Finally, we compare the ex-post wealth obtained by optimizing the performance measures on the US stock market during the decade 2004-2014.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8a1ffccc091f18e7a983c3159484256c",
  "timestamp": "2025-05-15T01:14:55.062048"
}