{
  "id": 2962,
  "title": "Reactive global minimum variance portfolios with k-BAHC covariance cleaning",
  "abstract": "We introduce a covariance cleaning method which works well in the very high-dimensional regime, i.e. when there are many more assets than data points per asset. This opens the way to unconditional reactive portfolio optimization when there are not enough points to calibrate dynamical conditional covariance models, which happens, for example, when new assets appear in a market. The method is a k-fold boosted version of the Bootstrapped Average Hierarchical Clustering cleaning procedure for correlation and covariance matrices. We apply this method to global minimum variance portfolios and find that k should increase with the calibration window length. We compare the performance of k-BAHC with other state-of-the-art covariance cleaning methods, including dynamical conditional covariance (DCC) with non-linear shrinkage. Generally, we find that our method yields better Sharpe ratios after transaction costs than competing unconditional covariance filtering methods, despite requiring a larger turnover. Finally, k-BAHC yields better Global Minimum Variance portfolios with long-short positions than DCC in a non-stationary investment universe.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "15c5f69ff2ef8750d81ecfcbd3d394a6",
  "timestamp": "2025-05-15T01:12:09.177839"
}