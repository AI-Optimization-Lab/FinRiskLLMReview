{
  "id": 2240,
  "title": "The dynamic connectedness between private equities and other high-demand financial assets: A portfolio hedging strategy during COVID-19",
  "abstract": "In view of the need for portfolio diversification, we investigate the interlinkages between a private equity ETF and a set of high-demand asset classes including bonds, equities, crude oil, gold, commodities, currency, Bitcoin, and shipping within a spillover framework. For this objective, we apply the enhanced modification of the Diebold and Yilmaz approach for the period 1 January 2010 to 31 January 2023. The empirical findings indicate a modest degree of connectedness among the investigated markets, whereas volatility spillovers showed acceleration during tumultuous periods. In addition, we assess the capacity of private equities for hedging, for the whole sample period and during COVID-19 infectious disease, in order to suggest investors for potential portfolio restructures. Results demonstrate that the short position in the volatility of private equity ETF can result in strong hedging effectiveness for investors holding long positions in Bitcoin, shipping, bonds, and crude oil. JEL Classification: C32, C58, G11, G15",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5cacbcbebffeb84fdde0efe1952b0021",
  "timestamp": "2025-05-15T01:05:04.348737"
}