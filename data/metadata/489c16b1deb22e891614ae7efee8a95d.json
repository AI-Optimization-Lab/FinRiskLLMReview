{
  "id": 1191,
  "title": "Implementation of Quantum Annealing: A Systematic Review",
  "abstract": "Quantum annealing is a quantum computing approach widely used for optimization and probabilistic sampling problems. It is an alternative approach designed due to the limitations of gate-based quantum computing models. The method is observed to have a significant impact on different fields such as machine learning, graphics, routing, scheduling, computational chemistry, computational biology, security, portfolio, and others despite the fact that it is relatively new. This research provides a systematic review of research development trends in the field of quantum annealing and analyzes how it has been implemented in different problem domains. The results are expected to serve as the basis to identify the opportunities and challenges of research related to its implementation. The main contribution of this systematic review is to summarize different implementations of quantum annealing. It is also to analyze the prospect and opportunities in one of the problem domains with the greatest interest which is machine learning.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "489c16b1deb22e891614ae7efee8a95d",
  "timestamp": "2025-05-15T00:53:09.835324"
}