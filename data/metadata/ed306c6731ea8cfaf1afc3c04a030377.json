{
  "id": 599,
  "title": "Batch mode active learning framework and its application on valuing large variable annuity portfolios",
  "abstract": "In practice, the valuation of a large volume variable annuity contracts relies on Monte Carlo simulation which is quite computationally intensive. To build a more efficient valuation process, statistical models have been used within a data mining framework that consists of two subsequent stages: the data sampling stage to create a set of representative contracts, and the regression modeling stage to make predictions for the remaining contracts in the portfolio. In this article, we work with a new data mining framework based on active learning, in which we iteratively update the regression model efficiently by selecting the most informative representatives. Our metrics take into consideration both the ambiguity and the diversity of the prediction, which allow us to propose two methods that fit well in this active learning framework. Experimental results demonstrate the effectiveness of the proposed active learning approaches over the random sampling as well as the two-stage data mining framework. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ed306c6731ea8cfaf1afc3c04a030377",
  "timestamp": "2025-05-15T00:45:39.074602"
}