{
  "id": 3772,
  "title": "How marketers influence product platform decisions: A configurational approach",
  "abstract": "This study identifies the political behavior marketers use to ensure the market fit of a new product platform. Drawing on two political NPD literature streams and configuration theory, the authors explain how (i) soft and hard influence tactics, (ii) reason, (iii) marketers' experience (with product and portfolio decisions), and (iv) the type of product platform (smart vs. conventional), together determine marketers' political effectiveness in gaining support for design modifications from the development team. The framework and hypotheses are tested using data from a sample of 100 influence attempts by marketers. A fuzzy set qualitative comparative analysis reveals no single, optimal solution but three influence recipes. First, we find a generic solution for both smart and conventional product platforms that combines marketer experience with reason and coalition building. Second, we note two additional solutions for smart product platforms: one consisting of marketer experience with coalition building and another one combining marketer experience with reason and assertiveness. Regression results reveal a positive relationship between marketers' influence behavior and platform performance in the marketplace, in support of a complementary and functional role of marketers' political behaviors in innovation management.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fef196156b84d4c418bbcbe091505970",
  "timestamp": "2025-05-15T01:20:36.103160"
}