{
  "id": 3656,
  "title": "An empirical analysis of the cardinality constrained expectile-based VaR portfolio optimization problem",
  "abstract": "Expectiles are asymmetric generalizations of mean that are extensively employed by statisticians in regression analysis. In the last decade, the coherence and elicitability characteristics of expectiles have attracted attention of the researchers in risk management field. Recently, expectile has been recommended as an alternative risk measure to value-at-risk (VaR) and conditional value-at-risk (CVaR). As an analogy to VaR and CVaR, expectile is defined as a risk measure called expectile-based value-at-risk (EVaR). In this study, EVaR optimization model is extended with a set of practical constraints such as no short-selling, target return, proportional bounds, and portfolio cardinality constraints. The ex-ante and ex-post risk-adjusted return performances of the proposed model are compared with those of CVaR model by using historical data of the stocks listed in the BIST 100 and the S&P 100 indices. Furthermore, we perform an extensive numerical investigation to reveal the impact of important parameters on the performances of the models. The obtained results show the potential benefits of using EVaR model in practical investment decisions.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "bce2c90edb313b2e9abb915a0ce433d7",
  "timestamp": "2025-05-15T01:19:26.158921"
}