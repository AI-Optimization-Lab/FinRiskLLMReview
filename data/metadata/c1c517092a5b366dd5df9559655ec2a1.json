{
  "id": 1631,
  "title": "Deep reinforcement learning based on transformer and U-Net framework for stock trading",
  "abstract": "An effective stock-trading strategy offers investors as much profit and as little risk as possible. Capturing volatility trends from historical stock prices and determining trading strategies is extremely challenging. This study proposes an end-to-end model called DRL-UTrans for learning a single stock trading strategy that combines deep reinforcement learning, transformer layers, and a U-Net architec-ture. In particular, the transformer layer captures complex dynamic patterns in financial markets. The model structure based on the U-Net architecture contains multiple skip connections used to combine long-and short-term features. The input of the model is a windowed stock price sequence, and the output consists of a trading action and action weight. The benefit of having two outputs is that the agent can control the share of buys and sells to reduce investment risk. In addition, a reward function that is sensitive to market volatility is proposed to feed back the market state. Finally, trading data for 10 stocks is extracted from a real financial market to validate the proposed model. The results show that DRL-UTrans has a higher profitability compared with the seven baseline approaches; further, it is effective in sensing market volatility and hedging market risk when encountering stock crashes.(c) 2022 Published by Elsevier B.V.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c1c517092a5b366dd5df9559655ec2a1",
  "timestamp": "2025-05-15T02:05:42.513041"
}