{
  "id": 156,
  "title": "监管压力、资本调整与风险承担——基于寿险业联立门限回归模型的研究",
  "abstract": "本文基于单期期权定价理论模型,构建关于资本、承保风险和投资风险调整的联立方程,并利用2009—2013年我国寿险公司的面板数据,研究监管门限效应的存在性,以及不同监管压力下资本调整和风险承担行为的异质性特征。本文突破了以往以监管标准值为行动门槛的研究模式,首次将考虑内生性的门限回归模型应用于我国寿险公司的研究中。研究结果表明,寿险公司的投资风险调整行为不存在门限效应,资本和承保风险调整行为存在不同于监管标准值的行动门槛168.3%和534.0%。通过比较各监管压力区间的回归结果发现,监管高压区的公司既没有过度承担风险的行为,也没有及时增加资本或者减少风险的行为;监管中压区的公司能够受到监管压力的良性驱动,公司趋于采用资本维持策略抵御承保风险;监管低压区的公司能够在一定程度上意识到投资风险,它们趋于采用资本维持策略应对投资风险,而采用资本缓冲策略应对承保风险。整体而言,寿险公司的资本调整和风险承担行为尚未形成良性互动机制。",
  "year": 2015,
  "source": "CNKI",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2c30072160004930215df47343721ee7",
  "timestamp": "2025-05-14T22:13:31.305145"
}