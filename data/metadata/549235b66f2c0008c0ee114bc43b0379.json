{
  "id": 2584,
  "title": "The heterogeneity of institutional investor activists and their counterintuitive tactical interactions",
  "abstract": "Purpose Following the traditions of stakeholder salience theory, this paper aims to contend that some institutional investor activists and tactics have more power, legitimacy and urgency than others. Design/methodology/approach The author undertakes an empirical test of a saliency table looking at the effects of institutional investor heterogeneity on portfolio firm responses using ordinal logistic regression. Findings This study found heterogeneity for institutional investor type to drive firm responses but not tactic type raising the importance of the attributes of each type of investor activist. The author found a rank ordering of public pension plans, hedge funds and then private multiemployer funds in saliency to portfolio firms. In addition, the use of proxy-based tactics did not help or hurt each investor type. Both findings challenge prior empirical work. Originality/value The rank ordering based upon the heterogeneity of institutional investor activists and their tactical interactions are tested providing empirical evidence of the most influential activist investors and tactics in one study, which is rare in the literature.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "549235b66f2c0008c0ee114bc43b0379",
  "timestamp": "2025-05-15T01:08:10.956336"
}