{
  "id": 2006,
  "title": "A Large-Scale Empirical Study of Aligned Time Series Forecasting",
  "abstract": "Automated Machine Learning (AutoML) tools for time series forecasting represent a frontier in both academic and industrial research, addressing the need for efficient, accurate predictions in various domains. This study focuses on the development of Automated Time Series Forecasting (AutoTS), specifically in the domains of demand forecasting and traffic prediction. Through a comprehensive empirical evaluation and comparative analysis, this study investigates the performance of 35 time series forecasting methods and scrutinizes the forecast quality and time series characteristics. In the experiments, we used six real and one synthetic datasets with 325 time series in total. The forecasting for four distinct horizons is studied. From our large-scale empirical study, we draw the following main conclusions: boosting-based methods, which are often overlooked, have a strong performance; the global modeling approach is promising because it provides competitive performance with a small computational cost; meta-learning via portfolio selection performs better than one based on meta-features. We hope that our empirical study will pave the way for more efficient AutoML systems for time series forecasting.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "566e0d75c800ca36e8e737cf1e4823c6",
  "timestamp": "2025-05-15T01:02:10.186058"
}