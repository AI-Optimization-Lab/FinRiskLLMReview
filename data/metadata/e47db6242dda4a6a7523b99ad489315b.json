{
  "id": 429,
  "title": "Grid-based Urban Fire Prediction Using Extreme Gradient Boosting (XGBoost)",
  "abstract": "Fires in urban areas lead to enormous financial and human losses because cities have high densities of people and buildings. Although a recent advanced IoT technology improves early fire detection, it is crucial to predict fire risk to manage and prevent urban fires. We propose a method of predicting urban fires using extreme gradient boosting (XGBoost), which is based on grid-based data, to consider the characteristics of urban fires occurring in local areas. Before model training, we conducted a correlation analysis and variance inflation factor (VIF) analysis to remove variables with a strong correlation between independent variables. Furthermore, oversampling and feature selection techniques were applied to improve the model's performance. Experimental results revealed that the overall accuracy of XGBoost was 81.25%, the F1-score was 86.43%, and the area under the curve (AUC) was 84.59%. XGBoost performed better than baseline models, such as the support vector machine (SVM) and logistic regression. The results of this study show that it can be used for local area management and the prevention of urban fires.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e47db6242dda4a6a7523b99ad489315b",
  "timestamp": "2025-05-15T01:50:52.622797"
}