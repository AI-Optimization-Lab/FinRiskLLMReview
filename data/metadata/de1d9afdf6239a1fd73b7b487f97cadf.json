{
  "id": 2175,
  "title": "When are extreme daily returns not lottery? At earnings announcements!",
  "abstract": "Using a sample of U.S. stocks over the period 1973-2015, we find that quarterly earnings announcements account for more than 18% of the total maximum daily returns in the top MAX portfolio. Maximum daily returns as triggered by earnings announcements do not entail lower future returns. Both portfolio and regression analyses show that the MAX phenomenon completely disappears when conditioning MAX returns on earnings announcements. We further show that earnings announcement MAX returns do not indicate a probability of future large short-term upward returns. Excluding earnings announcement MAX returns in constructing the lottery demand factor results in not only a larger lottery demand premium but also superior factor model performance. (C) 2018 Published by Elsevier B.V.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "de1d9afdf6239a1fd73b7b487f97cadf",
  "timestamp": "2025-05-15T01:03:36.726015"
}