{
  "id": 1696,
  "title": "An application of two-stage quantile regression to insurance ratemaking",
  "abstract": "Two-part models based on generalized linear models are widely used in insurance rate-making for predicting the expected loss. This paper explores an alternative method based on quantile regression which provides more information about the loss distribution and can be also used for insurance underwriting. Quantile regression allows estimating the aggregate claim cost quantiles of a policy given a number of covariates. To do so, a first stage is required, which involves fitting a logistic regression to estimate, for every policy, the probability of submitting at least one claim. The proposed methodology is illustrated using a portfolio of car insurance policies. This application shows that the results of the quantile regression are highly dependent on the claim probability estimates. The paper also examines an application of quantile regression to premium safety loading calculation, the so-called Quantile Premium Principle (QPP). We propose a premium calculation based on quantile regression which inherits the good properties of the quantiles. Using the same insurance portfolio data-set, we find that the QPP captures the riskiness of the policies better than the expected value premium principle.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "521b1f07f6204fd4e7c2c0e6062d338b",
  "timestamp": "2025-05-15T00:58:50.586034"
}