{
  "id": 532,
  "title": "A Denoising Autoencoder Approach for Credit Risk Analysis",
  "abstract": "Credit risk evaluation is a key consideration in financial activities. Financial institutions such as banks rely on credit risk analysis for determining the potential risk involved in financial activities and then decide the degree of involvement in such activities as well as the appropriate interest rate and the amount of capital that should be reserved. The recent development of machine learning has provided powerful tools for computer-aided credit risk analysis, and neural networks are one of the most promising approaches. However, conventional artificial neural networks involve multiple layers of neurons which then become a universal function that can approximate any function. Therefore, it will learn from not only the information in the training data set but also from the noise in it. It is critical to remove the noise in order to improve the accuracy and efficiency of such algorithms. In this paper, a denoising autoencoder approach is proposed for the training process for neural networks. The denoising-autoencoder-based neural network model is then applied to credit risk analysis, and the performance is evaluated.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "17a7694a6fc8314b491f6ff36263929a",
  "timestamp": "2025-05-15T01:52:12.240172"
}