{
  "id": 432,
  "title": "Using BERT to Predict the Brazilian Stock Market",
  "abstract": "Stock market prediction considering financial news is still an open challenge in the literature. Generally, related works consider the sentiment present in news with the following approaches: (1) lexicons; or (2) machine learning algorithms. However, both strategies are subject to errors introduced by human factors. While the lexical approach needs a specific dictionary for the financial domain, algorithms based on machine learning need a database manually labeled by experts. In this sense, this work aims to propose an approach to forecast the Brazilian stock market using news headlines preprocessed through BERT (Bidirectional Encoders Representations from Transformers), stock prices, technical indicators, and a Multilayer Perceptron neural network. Thus, the prediction is carried out directly with the embeddings of the financial news alongside technical analysis indicators and the financial time series, avoiding human intervention in the process. Our results are promising, with the developed approach overcoming the baselines Buy & Hold and Moving Average Crossover, considering both profitability and risk during the investment process.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fb5a37adb47e9fcd5bc09a90245e7fed",
  "timestamp": "2025-05-15T01:50:52.630637"
}