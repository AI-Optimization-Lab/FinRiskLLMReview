{
  "id": 858,
  "title": "Inverse reinforcement learning by expert imitation for the stochastic linear-quadratic optimal control problem",
  "abstract": "This article studies inverse reinforcement learning (IRL) for the linear-quadratic stochastic optimal control problem, where two agents are considered. A learner agent lacks knowledge of the expert agent's cost function, but it reconstructs an underlying cost function by observing the expert agent's states and controls, thereby imitating the expert agent's optimal feedback control. We initially present a model-based IRL method, which consists of a policy correction and a policy update from the policy iteration in reinforcement learning, as well as a cost function weight reconstruction informed by the inverse optimal control. Afterward, under this scheme, we propose a model-free off-policy IRL method, which requires no system identification, only collecting behavior data from the learner agent and expert agent once during the iteration process. Moreover, the proofs of the method's convergence, stability, and non-unique solutions are given. Finally, a numerical example and an inverse mean-variance portfolio optimization example are provided to validate the effectiveness of the presented method.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e98678630201d516d3e3965690742268",
  "timestamp": "2025-05-15T00:49:03.138604"
}