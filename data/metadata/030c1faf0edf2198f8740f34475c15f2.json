{
  "id": 2908,
  "title": "A hybrid volatility forecasting framework integrating GARCH, artificial neural network, technical analysis and principal components analysis",
  "abstract": "Measurement, prediction, and modeling of currency price volatility constitutes an important area of research at both the national and corporate level. Countries attempt to understand currency volatility to set national economic policies and firms to best manage exchange rate risk and leverage assets. A relatively new technological invention that the corporate treasurer has to turn to as part of the overall financial strategy is cryptocurrency. One estimate values the total market capitalization of cryptocurrencies at $557 billion USD at the beginning of 2018. While the overall size of the market for cryptocurrency is significant, our understanding of the behavior of this instrument is only beginning. In this article, we propose a hybrid Artificial Neural Network-Generalized AutoRegressive Conditional Heteroskedasticity (ANN-GARCH) model with preprocessing to forecast the price volatility of bitcoin, the most traded and largest by market capitalization of the cryptocurrencies. (C) 2018 Elsevier Ltd. All rights reserved.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "030c1faf0edf2198f8740f34475c15f2",
  "timestamp": "2025-05-15T02:20:16.989704"
}