{
  "id": 2494,
  "title": "Monthly income patterns and earnings management in banking",
  "abstract": "This study makes use of 38,070 loan portfolio-month observations in the 2009-2017 recessionary period and investigates whether monthly loan portfolio income distribution patterns can indicate earnings management in banking. We find that discontinuous monthly income patterns throughout the year based on Gini estimations are clear telltale signs of reported income manipulation detected via a Regression Discontinuity Design. We highlight that banks manipulate reported income by varying their loan loss provisions and reversals throughout the year. Specifically, they increase provisioning in the early months and release them towards year-end. This monthly-based strategy is employed by bank managers to influence the annual financial numbers reported in year-end public financial statements. The study contributes to understanding how banks internally manage provisioning on-average within a year, revealing a key mechanism of earnings management in the banking sector. The study fills a significant research gap in the existing literature on bank earnings management, primarily focused on quarterly earnings research.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "858170bdd3ce0e592a213a911f0b3636",
  "timestamp": "2025-05-15T01:07:35.661093"
}