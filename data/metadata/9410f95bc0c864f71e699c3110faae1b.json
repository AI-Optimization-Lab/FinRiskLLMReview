{
  "id": 1165,
  "title": "Customer Churn Prediction in the Iranian Banking Sector",
  "abstract": "in the financial system such as the banking sector, customers are valuable, and losing them is very expensive as customer churn is a major challenge facing banks. In this paper, we present a time series Deep Neural Networks (DNNs)-based approach for customer retention, in which a dataset has been collected from retail banking customers in the Republic Islamic of Iran. The dataset consists of real daily transactional data of about 50,000 customers in Pasargad bank in the months of November and December 2021. The goal of this study is to perform a highly churned customer predictor, attempting to observe the customer information in 30 days and predict the customer behavior in the next 30 days. Also, unlike other research in this field where the labels of customers are already determined, we present a new definition of the churned banking customer to label the data. Then, the data is cleaned, preprocessed, and prepared to import to a BiLSTM neural network. The proposed model has shown a significant superiority over Traditional Machine Learning Techniques. This paper can guide researchers in the field of banking and artificial intelligence, providing business knowledge to managers in the banking sector to reduce the risk of losing their customers.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9410f95bc0c864f71e699c3110faae1b",
  "timestamp": "2025-05-15T01:59:56.246991"
}