{
  "id": 5151,
  "title": "Reducing duration of inpatient treatment results in a higher readmission rate of alcohol addicted patients",
  "abstract": "Objective During the last years there has been a tendency to shorten the duration of inpatient treatment of alcohol addicted patients. This is mostly due to financial reasons. This study aimed at falsifying the hypothesis that there was a decrease in the duration of inpatient treatment and an increase in the readmission rate of alcohol addicted patients. Methods We analyzed a cohort of 6341 treatment episodes between 1995 and 2002 by using regression models for the length of stay, clinical improvement according to Clinical Global Impression scale and risk for readmission within 365 days after discharge. Results There was a highly significant decrease in duration of inpatient treatment and a successive increase in readmission rates between 1995 and 2002. The most prominent increase in readmission rates could be seen in patients with 3-14 days of inpatient treatment. Moreover, clinical improvement was correlated with length of stay. Conclusions Further studies need to clarify the relevant variables contributing to this increase in readmission rates after shortening length of stay and the respective financial consequences.",
  "year": 2007,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4b5dce4d9fbde7c96a7f80472f36d426",
  "timestamp": "2025-05-15T02:44:43.970363"
}