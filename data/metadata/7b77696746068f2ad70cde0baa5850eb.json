{
  "id": 3074,
  "title": "Stock return predictability in emerging markets: Does the choice of predictors and models matter across countries?",
  "abstract": "This study aims to examine return predictability in 24 emerging markets disaggregated in different regions. We propose four specifications, including a benchmark model. Then, an augmented model appropriate for each country, including a large set of potential factors, is evaluated. Furthermore, a dynamic multifactor model is investigated for all countries. Finally, we relax the symmetric hypothesis in asset return predictability based on a non-parametric non-linear approach: the projection pursuit regression model. Our study reveals three main findings. First, we reject all previous findings supporting a standard model of asset return predictability that is valuable for all countries, as we show that each country has specific domestic factors (both macroeconomic and financial) useful to predict future returns. Second, our empirical framework shows that asset return predictability might be robustly modelled based on non-linear specification based on the projection pursuit regression model. Our findings' explanatory power of out-of-sample estimations is economically relevant. Our results are useful for investors and policy-makers for portfolio diversification and regulation policies.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7b77696746068f2ad70cde0baa5850eb",
  "timestamp": "2025-05-15T01:13:09.890984"
}