{
  "id": 537,
  "title": "Forecasting stock market for an efficient portfolio by combining XGBoost and Hilbert-Huang transform",
  "abstract": "Portfolio formation in financial markets is the task of not taking non-necessary risks. Quantitative investment powered by machine learning has opened many new opportunities for more insight generation from financial data resulting in the explosion of ideas to enhance the performance of investments in the stock markets. In this research, we propose a first-introduced model called HHT-XGB to predict the changing trends in the next close price of stocks under the study. The proposed model combines Hilbert-Huang Transform (HHT) as the feature engineering part and the extreme gradient boost (XGBoost) as the Close price trend classifier. The classification output is a sequence of ups and downs used to optimize the stocks' portfolio weights with the best trading performance The performance of the portfolios optimized under this study proves that our novel combination of HHT with classification performs 99.8% better than forming the portfolio using raw financial data. The back-testing process suggests that the HHT-XGB strategy outperforms the benchmark strategies even with the poor-performing markets.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7d57158a1b4016717e923bfdd84d6eee",
  "timestamp": "2025-05-15T00:44:21.137658"
}