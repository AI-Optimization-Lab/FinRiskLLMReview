{
  "id": 3628,
  "title": "Fast and efficient nested simulation for large variable annuity portfolios: A surrogate modeling approach",
  "abstract": "The nested-simulation is commonly used for calculating the predictive distribution of the total variable annuity (VA) liabilities of large VA portfolios. Due to the large numbers of policies, inner-loops and outer-loops, running the nested-simulation for a large VA portfolio is extremely time consuming and often prohibitive. In this paper, the use of surrogate models is incorporated into the nested-simulation algorithm so that the relationship between the inputs and the outputs of a simulation model is approximated by various statistical models. As a result, the nested-simulation algorithm can be run with much smaller numbers of different inputs. Specifically, a spline regression model is used to reduce the number of outer-loops and a model-assisted finite population estimation framework is adapted to reduce the number of policies in use for the nested-simulation. From simulation studies, our proposed algorithm is able to accurately approximate the predictive distribution of the total VA liability at a significantly reduced running time. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4b5f1bfa90ae590ac3d4a1a92e1f62a0",
  "timestamp": "2025-05-15T01:19:00.782423"
}