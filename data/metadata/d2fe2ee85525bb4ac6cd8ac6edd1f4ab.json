{
  "id": 2070,
  "title": "Pure robust versus robust portfolio unbiased-Credibility and asymptotic optimality",
  "abstract": "Empirical credibility estimation, which is a credibility counterpart of empirical Bayes estimation, lacks robustness due to the sensitivity of estimators to outlier events. In this paper we combine robust statistics with empirical linear Bayes estimation and derive robust asymptotic optimality based on Norberg's (1980) proposal. Robust portfolio-unbiased empirical regression credibility is derived and its asymptotic optimality is proved, under not very restrictive assumptions. The asymptotic optimality of pure robust credibility estimators is also proved. The superiority of the pure robust credibility estimation against the robust portfolio-unbiased credibility estimation is presented and verified with numerical results. (C) 2013 Elsevier B.V. All rights reserved.",
  "year": 2013,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d2fe2ee85525bb4ac6cd8ac6edd1f4ab",
  "timestamp": "2025-05-15T01:02:32.888816"
}