{
  "id": 631,
  "title": "Does energy consumption follow asymmetric behavior? An assessment of Ghana's energy sector dynamics",
  "abstract": "The study answered the following questions: First, does energy evolves in different regimes by transitioning over a finite set of unobserved states ? Second, does energy consumption follow an asymmetric behavior over energy boom and energy scarcity ? and, Third, are there unobserved factors underpinning energy crisis ? We employed Markov-switching dynamic regression to examine the asymmetric effect, NIPALS regression to examine energy determinants and neural network analysis for prediction. The neural network model suggests a 99% prediction of energy consumption by the predictor variables. It was evident that energy consumption evolves in two states by transitioning over a finite set of unobserved states. The 11.6% growth in energy consumption is expected to occur in 4.1 years while energy crisis is expected to last for 3.7 years. Technological advancement and the development of green energy through foreign direct investment are essential to improve energy sector portfolio. (C) 2018 Elsevier B.V. All rights reserved.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a74db11de01e8a1b0c8f68e495f1b159",
  "timestamp": "2025-05-15T00:45:39.187857"
}