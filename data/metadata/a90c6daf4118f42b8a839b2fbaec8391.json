{
  "id": 780,
  "title": "A Fuzzy Random Survival Forest for Predicting Lapses in Insurance Portfolios Containing Imprecise Data",
  "abstract": "We propose a fuzzy random survival forest (FRSF) to model lapse rates in a life insurance portfolio containing imprecise or incomplete data such as missing, outlier, or noisy values. Following the random forest methodology, the FRSF is proposed as a new machine learning technique for solving time-to-event data using an ensemble of multiple fuzzy survival trees. In the learning process, the combination of methods such as the c-index, fuzzy sets theory, and the ensemble of multiple trees enable the automatic handling of imprecise data. We analyse the results of several experiments and test them statistically; they show the FRSF's robustness, verifying that its generalisation capacity is not reduced when modelling imprecise data. Furthermore, the results obtained using a real portfolio of a life insurance company demonstrate that the FRSF has a better performance in comparison with other state-of-the-art algorithms such as the traditional Cox model and other tree-based machine learning techniques such as the random survival forest.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a90c6daf4118f42b8a839b2fbaec8391",
  "timestamp": "2025-05-15T00:47:50.420676"
}