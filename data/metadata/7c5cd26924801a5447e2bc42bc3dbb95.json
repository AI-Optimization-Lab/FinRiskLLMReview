{
  "id": 2487,
  "title": "What makes PhD researchers think seriously about discontinuing? an exploration of risk factors and risk profiles",
  "abstract": "Although high PhD attrition rates are a matter of international concern, the factors that lead doctoral researchers to leave their programmes are not well understood. The present study addresses that issue by exploring factors that prompted thoughts of discontinuing among 1017 PhD researchers (PhDRs) at a public, research-intensive Australian university. We analyse the prevalence, strength and clustering of the most frequently identified factors, including mental health difficulties, financial pressures, and problems with supervision. The investigated factors were all strongly associated with thoughts of discontinuing; mental health difficulties were among the strongest factors, and financial stress was the most prevalent. An exploratory cluster analysis revealed that the risk factors co-present in distinctive ways such that six discrete groups of PhDRs are identifiable with varying risk profiles and socio-demographic characteristics. We discuss the research, policy and practice implications of these findings.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7c5cd26924801a5447e2bc42bc3dbb95",
  "timestamp": "2025-05-15T02:15:59.853698"
}