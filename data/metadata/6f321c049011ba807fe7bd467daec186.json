{
  "id": 5464,
  "title": "Factors of influence on flood damage mitigation behaviour by households",
  "abstract": "Based on a literature review, this paper proposes and empirically tests an extended version of the Protection Motivation Theory (PMT) of individual disaster preparedness. A survey was completed by 885 households in three flood-prone regions in France. Regression models provide insights into the factors of influence on the implementation of three categories of flood risk mitigation measures and households' intentions to implement (additional) measures. Although the results differ per category, the overall findings show that threat appraisals have a small effect on mitigation behaviour, while coping appraisals have a more important influence. Several variables that have been added to the PMT framework appear to be influential in households' preparedness decisions, such as: flood experience; local flood risk management policies and incentives; and the social network. Based on these results, two policy recommendations are made for increasing individual flood preparedness: improving communication campaigns on flood damage mitigation measures, and providing additional financial incentives. (C) 2014 Elsevier Ltd. All rights reserved.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6f321c049011ba807fe7bd467daec186",
  "timestamp": "2025-05-15T02:48:23.544644"
}