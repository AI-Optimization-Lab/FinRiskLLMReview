{
  "id": 3606,
  "title": "IDIOSYNCRATIC VOLATILITY, EXPECTED WINDFALL, AND THE CROSS-SECTION OF STOCK RETURNS",
  "abstract": "This paper analyzes the roles of idiosyncratic risk and firm-level conditional skewness in determining cross-sectional returns. It is shown that the traditional EGARCH estimates of conditional idiosyncratic volatility may bring significant finite sample estimation bias in the presence of non-Gaussianity. We propose a new estimator that has more robust sampling performance than the EGARCH MLE in the presence of heavy-tail or skewed innovations. Our cross-sectional portfolio analysis demonstrates that the idiosyncratic volatility puzzle documented by Ang, Hodrick, Xiang, and Zhang (2006) exists intertemporally. We conduct further analysis to solve the puzzle. We show that two factors idiosyncratic variance and individual conditional skewness play important roles in determining cross-sectional returns. A new concept, the expected windfall, is introduced as an alternate measure of conditional return skewness. After controlling for these two additional factors, we solve the major piece of this puzzle: Our cross-sectional regression tests identify a positive relationship between conditional idiosyncratic volatility and expected returns for over 99% of the total market capitalization of the NYSE, NASDAQ, and AMEX stock exchanges.",
  "year": 2014,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b64f3493a496804d556aef23163ab3f7",
  "timestamp": "2025-05-15T01:19:00.662850"
}