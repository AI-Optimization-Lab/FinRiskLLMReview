{
  "id": 571,
  "title": "Fama-French three versus five, which model is better? A machine learning approach",
  "abstract": "This research proposes new estimations of the Fama-French three- and five-factor models via a machine learning approach. Specifically, it uses a Bayesian optimization-support vector regression (BSVR) approach to obtain predictions of portfolio returns. On data from five industries' portfolio returns in the United States over the period July 1926 to January 2019, the BSVR models perform well. Specifically, our new model, called the Fama-French BSVR three-factor model, outperformed the Fama-French BSVR five-factor model. More precisely, the Fama-French BSVR three-factor estimations attain out-of-sample (testing dataset) correlation coefficients of 94% for portfolio returns for the consumption and manufacturing industries. A correlation of 92% between the predicted and experimental values of portfolio returns was found for the high-tech industry; 91% was found for the mining, construction, transportation, hotels, entertainment, and finance industries. However, for the Fama-French BSVR five-factor model, the correlation coefficients lie between 48% (health industry) and 89% (high-tech industry).",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f21985ddf898b62ee1ebc7cffaa7498c",
  "timestamp": "2025-05-15T00:45:01.326773"
}