{
  "id": 2132,
  "title": "Cross-sectional stock return predictability in China",
  "abstract": "Cross-sectional stock return predictability has always been an intriguing issue for researchers, as it relates to a number of resilient finance puzzles. This paper provides a comprehensive analysis of stock return predictability in China from January 1994 to March 2011. It implements both the portfolio and cross-sectional regression methods. We find size, price, the book-to-market ratio, the cash-flow-to-price ratio and the earnings-to-price ratio to have strong predictive power. Among the risk-related predictors, total and idiosyncratic volatility exhibit predictive powers in China. The results are valid for stocks listed on both the Shanghai and Shenzhen Stock Exchanges. Momentum fails to qualify as a useful predictor according to both the portfolio and simple regression methods. However, when used with other predictors in multiple regressions, it exhibits predictive power for all of the stocks including large stocks. Although the variables related to stock 'cheapness' such as the book-to-market and cash-flow-to-price ratios demonstrate reliable forecast power, the earnings-to-price ratio is relatively less powerful.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "686ee8617004e250ff4e318482e95ffe",
  "timestamp": "2025-05-15T01:03:03.808337"
}