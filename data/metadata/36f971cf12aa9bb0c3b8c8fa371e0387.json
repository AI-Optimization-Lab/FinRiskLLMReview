{
  "id": 1964,
  "title": "Navigating the Difficulty of Achieving Global Optimality under Variance-Induced Time Inconsistency",
  "abstract": "Measuring the long-term uncertainty of following a policy is important and insightful in many risk-sensitive applications such as in finance. In the context of reinforcement learning (RL), risk-based decision-making is commonly incorporated through an RL agent's goal to optimize the trade-off between the mean and variance of cumulative rewards. However, variance-based objectives are known to induce time inconsistency (TIC). This paper aims to see how TIC permeates into the design and behavior of mean-variance (MV) agents. We approach this by zooming into two optimality classes under TIC: global optimality and equilibrium, for each of which we identify a TIC-aware MV RL method, respectively, episodic policy gradient (EPG) and subgame perfect equilibrium RL (SPERL). We position both methods as approximate tools for achieving global optimality and evaluate their performance in two discerning financial environments: portfolio management and optimal execution. Noteworthily, our results show that despite EPG's globally optimal objective, its policy does not necessarily attain global optimality and is dominated by equilibrium/SPERL's policy in numerous environment setups.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "36f971cf12aa9bb0c3b8c8fa371e0387",
  "timestamp": "2025-05-15T02:09:48.241305"
}