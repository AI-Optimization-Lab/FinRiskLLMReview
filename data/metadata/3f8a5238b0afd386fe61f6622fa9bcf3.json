{
  "id": 4637,
  "title": "Propensity Score Matching and Its Application to Risk Drivers Detection in Financial Setting",
  "abstract": "In credit risk scoring models are used as a tool to evaluate the level of risk associated with applicants or customers. The aim of these models is not only to estimate the probability that the client will not be able to fulfill his financial commitments but also to identify and estimate the risk drivers i.e., client attributes that are responsible for risk occurrence. Unfortunately, scoring models are built based on historic data stored by bank over the clients. Selection of clients is not random. This leads to systematic errors. Therefore one seeks methods that allow for a model correction that enables application of statistical inference. Quasi-experimental designs are practical solutions to this dilemma. One of such methods is propensity score matching. Propensity score matching allows also for detecting risk drivers that are independent of borrowers attributes, e.g., triggered by various bank strategies. The aim of our research is to apply propensity score matching methodology to identify these risk drivers in credit risk that could not be detected e.g., by regression models.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3f8a5238b0afd386fe61f6622fa9bcf3",
  "timestamp": "2025-05-15T02:39:26.985810"
}