{
  "id": 861,
  "title": "CATE: Contrastive augmentation and tree-enhanced embedding for credit scoring",
  "abstract": "Credit transactions are vital financial activities that yield substantial economic benefits. To further improve lending decisions, stakeholders require accurate and interpretable credit scoring methods. While the majority of previous studies have focused on the relationship between individual features and credit risk, only a few have investigated cross-features. Notably, cross -features can not only represent structured data effectively but also provide richer semantic information than individual features. Nevertheless, most previous methods for learning cross -feature effects from credit data have been implicit and unexplainable. This paper proposes a new credit scoring model based on contrastive augmentation and tree-enhanced embedding mechanisms, termed CATE. The proposed model automatically constructs explainable cross -features by using tree-based models to learn decision rules from the data. Moreover, the importance of each local cross-feature is then derived through an attention mechanism. Finally, the credit score of a user is evaluated using embedding vectors. Experimental results on 4 public datasets demonstrated the interpretability of our proposed method and outperformed 13 state-of-the-art benchmark methods in terms of performance.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "22cd4cadb31029d8d9226c90d17452bf",
  "timestamp": "2025-05-15T01:43:20.725532"
}