{
  "id": 2966,
  "title": "A new approach to exchange rate forecast: The role of global financial cycle andtime-varyingparameters",
  "abstract": "The exchange rate disconnect puzzle argues that macroeconomic fundamentals are not able to accurately predict exchange rate. Recent studies have shown that the puzzle could be upturned if: (a) the dataset is structured in a panel form; (b) the model is based on the portfolio balance theory (PBT); (c) factor models are employed and (d) time-varying parameter (TVP) regression is used. This study combines these strands of the literature. Essentially, the study conjectures that Global Financial Cycle (GFCy), drawing inspiration from PBT, has some predictive information content on exchange rate. Using dataset from 25 countries, we produced some mixed results. On the whole, the GFCy is able to produce lower forecast error, as compared to the that of benchmark model. However, its effectiveness is dependent upon the regression type (TVP vs. Panel Fixed Effect); forecast horizons (short vs. long); the sample period (early vs. late) and measures of GFCy. The results are robust to a number of checks.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2051af15409f780f55afa88c1135aa12",
  "timestamp": "2025-05-15T01:12:09.190850"
}