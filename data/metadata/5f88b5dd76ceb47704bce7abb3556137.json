{
  "id": 702,
  "title": "Optimal attention allocation: picking alpha or betting on beta?",
  "abstract": "We investigate a problem of attention allocation and portfolio selection with information capacity constraint and return predictability in a multi-asset framework. In a two-phase formulation, the optimal attention strategy maximizes the combined expected alpha payoffs and expected beta payoffs of the portfolio. Return predictors taking extreme values incentivize the investor to learn about them and this leads to competition among information sources for attention. Moreover, the investor trades with varying skills including picking alphas and betting on beta, depending on the magnitude of the related predictors. Our multi-period analysis using reinforcement learning demonstrates time-horizon effects on attention and investment strategies.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5f88b5dd76ceb47704bce7abb3556137",
  "timestamp": "2025-05-15T00:47:18.488089"
}