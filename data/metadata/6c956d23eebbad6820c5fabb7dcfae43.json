{
  "id": 4359,
  "title": "Do Individuals Have Preferences Used in Macro-Finance Models? An Experimental Investigation",
  "abstract": "Recent financial studies often assume that agents have Epstein-Zin preferences-preferences that require agents to care about when uncertainty is resolved. Under this recursive-preference framework, the preference for uncertainty resolution is entirely determined by an agent's preferences for risk and intertemporal substitution. To test the implications of this model, this paper presents an experiment designed to elicit subject preferences on risk, time, intertemporal substitution, and uncertainty resolution. Results reveal that most subjects prefer early resolution of uncertainty and have relative risk aversion greater than the reciprocal of the elasticity of intertemporal substitution, consistent with the predictions by recursive preferences. Subjects are classified in a finite mixture model by their risk, time, and intertemporal-substitution parameters. Regression results show that types predicted by the Epstein-Zin model to prefer early resolution choose early resolution with 20%-50% higher probability.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6c956d23eebbad6820c5fabb7dcfae43",
  "timestamp": "2025-05-15T02:36:05.277504"
}