{
  "id": 6505,
  "title": "Exploratory Study for Readmission in Cancer Patients Completed Research",
  "abstract": "Cancer, a major threat to public health worldwide, causes substantial financial burdens. Cancer readmission time interval, or Out-of-Hospital Days (OHD) between two consecutive hospital admissions, has been widely adopted as an important measure of healthcare service quality. However, there is a paucity of models focusing on OHD and associated risk factors due to limited access to cancer patients' data and absence of important factors (e.g., geographic factors) in the data. We utilize OHD (>30) as the result of cancer patient's conditions (e.g., personal, medical) and treatment costs. We analyze a sample of 22,231 admissions extracted from 635,261 cancer patient Electronic Health Records (EHR) from 190 hospitals in China. We apply text mining on free-form address fields to extract patients' home address and hospitals' location information. Using hierarchical linear regression, we find various types of factors significantly influence OHD: age, marital status, number of admissions and whether the treating hospital is in the same province as the patient.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a292b0f853af02e4c985779c9724c462",
  "timestamp": "2025-05-15T02:58:34.297656"
}