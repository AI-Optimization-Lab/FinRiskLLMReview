{
  "id": 3668,
  "title": "Diagnosing the distribution of GARCH innovations",
  "abstract": "The Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model, designed to model volatility clustering, exhibits heavy-tailedness regardless of the distribution of its innovation term. When applying the model to financial time series, the distribution of innovations plays an important role for risk measurement and option pricing. We investigate methods on diagnosing the distribution of GARCH innovations. For GARCH processes that are close to integrated-GARCH (IGARCH), we show that the method based on estimated innovations is not reliable, whereas an alternative approach based on analyzing the tail index of a GARCH series performs better. The alternative method leads to a formal test on the distribution of GARCH innovations. (C) 2014 Elsevier B.V. All rights reserved.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ae02eb54375aa3ef81afb58f3c4e76bc",
  "timestamp": "2025-05-15T02:29:00.025747"
}