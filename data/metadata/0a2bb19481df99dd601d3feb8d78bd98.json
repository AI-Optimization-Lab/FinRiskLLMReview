{
  "id": 2953,
  "title": "To racketeer among neighbors: spatial features of criminal collaboration in the American Mafia",
  "abstract": "The American Mafia is a network of criminals engaged in drug trafficking, violence and other illegal activities. Here, we analyze a historical spatial social network (SSN) of 680 Mafia members found in a 1960 investigatory dossier compiled by the U.S. Federal Bureau of Narcotics. The dossier includes connections between members who were 'known criminal associates' and members are geolocated to a known home address across 15 major U.S. cities. Under an overarching narrative of identifying the network's proclivities toward security (dispersion) or efficiency (ease of coordination), we pose four research questions related to criminal organizations, power and coordination strategies. We find that the Mafia network is distributed as a portfolio of nearby and distant ties with significant spatial clustering among the Mafia family units. The methods used here differ from former methods that analyze the point pattern locations of individuals and the social network of individuals separately. The research techniques used here contribute to the body of non-planar network analysis methods in GIScience and can be generalized to other types of spatially-embedded social networks.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0a2bb19481df99dd601d3feb8d78bd98",
  "timestamp": "2025-05-15T01:12:09.138045"
}