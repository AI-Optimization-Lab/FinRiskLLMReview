{
  "id": 2821,
  "title": "Anomaly Detection based on NSL-KDD using XGBoost with Optuna Tuning",
  "abstract": "The enormous internet development now day across all aspects of human life has introduced various hidden risk of malicious attacks on network security that most users didn't realize. One of the malicious attacks is intrusion of system that proliferate user's account effortlessly. Hence, in order to avoid intrusion effect that lead to financial loss and any other loss, intrusion detection system is needed to identify a dynamic pattern of cyber attacks. In this paper, we propose an Optimized XGBoost Classifier model with the help of Optuna Hypertuning method to find the best parameter for the model. In order to find the most efficient method for training, we assign three Optuna scenarios combine with feature selection to learn the data and the machine learning model. Through learning, Optuna generated the best parameter for XGBoost Classifier. Optuna avoids time consuming and low efficiency training model. The propose XGBoost Classifier model with Optuna Hypertuning method results in a greater accuracy of detection intrusion compare to any other models.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a065df226f685407ed7c89200403981c",
  "timestamp": "2025-05-15T02:19:46.597227"
}