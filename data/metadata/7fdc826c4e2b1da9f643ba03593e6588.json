{
  "id": 4363,
  "title": "Was the Classical Gold Standard Credible on the Periphery? Evidence from Currency Risk",
  "abstract": "We use a standard metric from international finance, the currency risk premium, to assess the credibility of fixed exchange rates during the classical gold standard em. Theory suggests that a completely credible and permanent commitment to join the gold standard would have zero currency risk or no expectation of devaluation. We find that, even five years after a typical emerging-market country joined the gold standard, the currency risk premium averaged at least 220 basis points. Fixed-effects, panel-regression estimates that control for a variety of borrower-specific factors also show large and positive currency risk premia. In contrast to core gold standard countries, such as France and Germany, the persistence of large premia, long after gold standard adoption, suggest that financial markets did not view the pegs in emerging markets as credible and expected that they devaluation.",
  "year": 2015,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7fdc826c4e2b1da9f643ba03593e6588",
  "timestamp": "2025-05-15T02:36:38.365453"
}