{
  "id": 2461,
  "title": "SURVIVING THE STORM: AN ANALYSIS OF FINANCIAL DISTRESS IN ASIA'S DOMINANT TELECOMMUNICATIONS COMPANIES",
  "abstract": "This research offers a thorough examination of the consequences of financial distress, operational distress, lending interest rate, firm size, and inflation on the telecommunications industry. The analysis is limited to the ten largest companies in Asia by market capitalization from 2013 to 2022. By doing so, this study provides significant contributions to the understanding of the unique dynamics of financial distress in this crucial sector and time period. Utilizing secondary data from Asian stock exchanges, the research adopts a quantitative methodology. The correlation between the independent and dependent variables is investigated through the utilization of logistic regression. A decline in lending interest rates is associated with a greater likelihood that businesses will experience financial distress, whereas an increase in inflation is linked to the same risk. During 2013-2022, the findings also indicate that financial distress in the largest telecommunications companies in Asia was not substantially impacted by operational risk, financial risk, or firm size.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7e950fb4c4a78ddc22206415d95a2de1",
  "timestamp": "2025-05-15T02:15:27.132088"
}