{
  "id": 308,
  "title": "Utilization of Quantization Method on Credit Risk Assessment",
  "abstract": "Credit risk assessment is critical factor in credit risk management, which has played a key role in financial and banking industry. Many classification methods are used in credit risk assessment aiming to establish classifiers to predict the credit state of the corporate (good or bad). However, most of classification methods can not handle continuous variables. So, continuous variables must be quantified. In this paper, we first propose an improved quantization method, namely IDM, based on the statistical independence; then we use data mining techniques, i.e., C4.5 decision tree, Naive-Bayes and SVM classifier, to classify and predict the quantified credit data. The aim is to investigate the effect of quantization method on the classification of credit approval data. The Experimental results show that our approach significantly improves the mean accuracy of classification than other known quantization methods. This denotes that the proposed method can make an effective interpretation and point out the ability of design of a new intelligent assistance credit approval data system.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f36ae61ce821bfeb69421c08536c80f9",
  "timestamp": "2025-05-15T01:49:04.693201"
}