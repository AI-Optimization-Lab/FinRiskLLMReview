{
  "id": 2326,
  "title": "Financial Risk Tolerance, Sensation Seeking, and Locus of Control Among Pre-Retiree Baby Boomers",
  "abstract": "Financial risk tolerance is an important personal characteristic that is widely used by financial professionals to guide the development and presentation of client-centered recommendations. As more baby boomers enter retirement, research on how these individuals perceive their willingness to take financial risks has gained importance, particularly as the focus of investment portfolios changes from capital accumulation to capital preservation in retirement. This study examined the role of sensation seeking and locus of control on financial risk tolerance for a pre-retiree baby boomer sample using the 2014 wave of the National Longitudinal Survey of Youth 1979. Findings from three ordinary least square (OLS) regression models showed that baby boomers who were not sensation seekers, and those who displayed an external locus of control orientation were more likely to exhibit a low tolerance for financial risk. Furthermore, those who engaged in sensation-seeking behavior were more likely to have an internal locus of control orientation and a high tolerance for risk.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "faea7d417401d8855f884561f8531523",
  "timestamp": "2025-05-15T02:14:19.569884"
}