{
  "id": 3616,
  "title": "Economic evaluation of asset pricing models under predictability",
  "abstract": "This paper performs an out-of-sample comparison of linear factor asset pricing models from an economic perspective under predictability. I assess the economic value added of several factor models when a Bayesian investor is faced with a portfolio allocation problem whereby each model imposes cross-sectional restrictions on the parameters of a predictive stock return regression. The empirical framework explicitly accounts for investor skepticism about the model, i.e., mispricing uncertainty. Despite vast statistical work on in-sample model comparison for the new-generation asset-pricing models, their out-of-sample performance cannot beat a simple benchmark on a wide range of tests from an economic perspective. This is consistent with thus extends the conclusion for the first-generation of factor models. An exceptional case of factor models yielding significant economic gains is observed when evaluating industry portfolios at the shortest horizon (1-month). In this case, I find that the q5 model of Hou et al. (2021) and the behavioral factor model of Stambaugh and Yuan (2017) outperform the historical benchmark in several cases.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6052e8ab8c6a572ccb5864cb0a5e819e",
  "timestamp": "2025-05-15T01:19:00.704516"
}