{
  "id": 7794,
  "title": "Decision-Making on the Level of Flood Protection Under High Climatic Uncertainty",
  "abstract": "J E Nash was amongst the first to recognise the importance of quantifying the uncertainty in estimates of the T-year flood. In 1966, he used Monte Carlo simulation to derive (with J Amorocho) the coefficients of an expression for the standard error of the T-year flood for moment fitting of the Gumbel distribution. Today, the problem of estimating the T-year flood and its uncertainty, and of deciding on the level of flood protection to be provided, is fraught with difficulty due to the uncertainty over a changing climate, but Monte Carlo simulation can play an important role in exploring how to deal with this uncertainty. It has been suggested that the assumption of stationarity inherent in the IID (identically and independently distributed) assumption underlying classical flood frequency analysis should be discarded in favour of nonstationarity. However, while the existence of global warming cannot be disputed, the evidence for an anthropogenic climate change signal (and nonstationarity) in precipitation extremes and flood records has proved difficult to find, based on the findings of the IPCC 2012 special report on extreme events and other extensive analyses of global flood records. Moreover, GCMs are not sufficiently reliable in reproducing the properties of precipitation extremes in control climates to provide a reliable basis for predicting how these properties will change in the future. Also, they may not reproduce the natural long-term variability in the climate. Here, the implications of a more variable climate for investment in flood protection are explored within the limits of the stationarity assumption. There is increasing evidence that the IID assumption for Annual Maximum Floods (AMF) is not valid due to natural long-term variability in the climate, which manifests itself as 'flood rich' and 'flood poor' periods. Monte Carlo simulation experiments are conducted in which an ensemble of AMF series is generated using a stationary stochastic model with increasing levels of long-term persistence. A classical Cost-Benefit (CB) approach is used to estimate the appropriate level of protection for a hypothetical site with a specified damage function. This indicates that as persistence increases there is a higher probability of having very high levels of damage in a design-life period than under the IID case, reflecting temporal clustering of highly damaging floods which is of great concern to the insurance industry. Investment approaches to account for this increased risk are explored; including the Conditional Value at Risk (CVaR) risk assessment approach which is often used to reduce the probability that a financial investment portfolio will incur large losses. The public's view of flood risk can significantly differ from that of flood risk management professionals, and hence cost benefit analysis may not adequately address the society's priorities.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8c532a46f834c7869071034f9e2972fc",
  "timestamp": "2025-05-15T03:12:11.655750"
}