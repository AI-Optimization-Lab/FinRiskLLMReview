{
  "id": 4792,
  "title": "Bank capital buffer, franchise value, and risk heterogeneity in China",
  "abstract": "The determinants of bank risk-taking have been widely investigated, yet few studies consider the possibility that effect of independent variables on bank risk-taking can vary along the conditional risk distribution. This paper addresses this issue by using panel quantile regression models. We examine the heterogeneity effect of capital buffer and franchise value on bank risk-taking among quantiles of the risk distribution in Chinese banking sector. Furthermore, we adopt a difference-in-difference method to tackle the potential endogeneity problem. Our main results indicate that the effect of capital buffer and franchise value on bank risk-taking is heterogeneous across quantiles: for banks in the upper tail of risk distribution, more capital buffer tends to reduce risk-taking; however, higher franchise value does not appear to reduce it. Additionally, our further results support for the Chinese version of Basel III implemented in 2012 that requires banks to build up more capital buffers. Yet banks in the upper tail of the conditional franchise value distribution appear to increase risk- taking after the shock of the recent financial crisis. These findings shed light on concerns about regulatory monitoring of bank risk.",
  "year": 2017,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4ecb118791b216d53d0ddf189fd6933d",
  "timestamp": "2025-05-15T02:41:00.884512"
}