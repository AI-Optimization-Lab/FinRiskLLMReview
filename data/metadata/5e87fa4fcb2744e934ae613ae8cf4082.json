{
  "id": 338,
  "title": "Mean-variance portfolio optimization with deep learning based-forecasts for cointegrated stocks",
  "abstract": "Most mean-variance (MV) models construct a portfolio based on nonstationary stocks. This study presents a new MV model constructed using stationary portfolios composed of cointegrated stocks. The expected return of this new model is predicted by using machine learning models, such as support vector machine, random forest, and attention-based long short-term memory (LSTM) network. The proposed model is evaluated using data on stocks in the CSI 300 and the S&P 500, with 42 features over 8 years from May 4, 2012 to August 4, 2020. The empirical results show that the portfolio constructed based on the stationary portfolios in both the Chinese and the US stock markets delivers significant profits. Further, the attention-based LSTM network can more accurately model the spread return using technical indicators, financial investment information, and lagged returns, and can successfully select pairs of cointegrated stocks for constructing a more profitable MV portfolio, than can conventional machine learning models. Using the attention-based LSTM to predict 20-day return movement results in model accuracy of up to 92.59% for the CSI 300 and 88.52% for the S&P 500, and a corresponding Sharpe ratio of 9.31 and 2.77, respectively.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5e87fa4fcb2744e934ae613ae8cf4082",
  "timestamp": "2025-05-15T00:42:00.511234"
}