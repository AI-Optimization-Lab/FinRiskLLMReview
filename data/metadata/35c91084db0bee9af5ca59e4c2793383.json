{
  "id": 3500,
  "title": "EXAMINING TIME-VARYING INTEGRITY AND INTERRELATIONSHIPS AMONG GLOBAL STOCK MARKETS",
  "abstract": "This paper examines co-movements and interrelationships among 6 emerging and 5 developed stock market returns between period 2001- 2017. First, principal components are extracted from returns. Results show that, for the period analyzed, there is no strong global stock market integration and there isn't any change in patterns of correlations of returns except short-term disturbances in global financial crisis time. Second, partial least squares regression models are used for predicting each stock market returns with other stock markets' current and all markets' up to three month lagged returns. Results identify divergence between developed and emerging markets and greater number of latent transmission channels among former ones. Findings indicate a strong integrity among global stock markets is not yet appeared and considered together with previous studies, today's international market structure can be attributed to last two decades of 20th century. International investors still may benefit from international portfolio diversification opportunities.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "35c91084db0bee9af5ca59e4c2793383",
  "timestamp": "2025-05-15T01:17:54.066868"
}