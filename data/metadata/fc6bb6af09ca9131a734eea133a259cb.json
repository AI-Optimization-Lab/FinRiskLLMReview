{
  "id": 4319,
  "title": "FORECASTING STOCK RETURN VOLATILITY USING THE REALIZED GARCH MODEL AND AN ARTIFICIAL NEURAL NETWORK",
  "abstract": "Volatility forecasting is required for risk management, asset allocation, option pricing, and financial market trading. It can be done by using various time series forecasting techniques and Artificial Neural Networks (ANN). T he current research focuses on the modeling and forecasting of stock market indices using high-frequency data. A recent high-frequency volatility model is called the Realized GARCH (RGARCH) model, where the key feature is an equation that relates the realized measure to the conditional variance of returns. This equation incorporates an asymmetric reaction to shocks, providing a highly flexible representation of market dynamics. This paper proposes an hybrid model where ANN and RGARCH are used to forecast stock return volatility. This model was established by entering the predicted Realized Volatility (RV), calculated using RGARCH, into the ANN. The choice of the input variables of the ANN is made using the Granger causality test in order to reduce the noise which would affect the prediction system and which could be generated by an input variable not statistically linked to stock market volatility. The results show that a hybrid model based on a recurrent neural network (RNN) outperforms the RGARCH and HAR-type models in out-of-sample evaluations according to the RMSE and the correlation coefficient.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fc6bb6af09ca9131a734eea133a259cb",
  "timestamp": "2025-05-15T02:36:05.143353"
}