{
  "id": 1814,
  "title": "Reconsideration of new product development planning based on the relationship between product complexity and product lifetime: the case of the Korean mobile phone market",
  "abstract": "This study aims to investigate the impact of product complexity on product lifetime. As product lifetime increases, firms have a longer time to develop new products and gain a higher return on investment. This study posits that product complexity has an effect on product lifetime and then investigates a product portfolio strategy for product complexity by using Korean mobile phone data released from 2004 to 2013. The impact of product complexity on product lifetime is investigated by pooled ordinary least squares regression. Then, we derive a product portfolio strategy that increases product complexity using panel data analysis. The results show that an increase in product complexity lengthens product lifetime and that a firm needs to design its product portfolio by concentrating its resources and capabilities on releasing products of the latest product type. The findings of this study can help a firm establish an optimal product portfolio that increases the return on investment.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "64742b9fc0aa1391b4a5205ccb4bef3c",
  "timestamp": "2025-05-15T00:59:53.217612"
}