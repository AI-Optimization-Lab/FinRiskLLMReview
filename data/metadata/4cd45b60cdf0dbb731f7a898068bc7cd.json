{
  "id": 2970,
  "title": "Quantiles in a multi-stage nested classification credibility model",
  "abstract": "In insurance and finance it is often important to have a satisfactory estimate for an extreme quantile, like the one underlying capital requirements in Solvency II and Basel III. If credibility techniques on means are used for the determination of such quantiles, this can lead to quite unsatisfactory results, in particular in the presence of outliers in the data. Quantile credibility models themselves, however, cannot perform effectively when the set of data has a nested (hierarchical) structure. This paper develops multi-stage nested classification hierarchical credibility models for quantiles as an alternative to Jewell's (G Ist Ital Attuari 38:1-16, 1975) approach, where more than one risk factor divides the portfolio into different sectors or classes. We establish hierarchical quantile credibility estimators and illustrate their performance in two numerical illustrations.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4cd45b60cdf0dbb731f7a898068bc7cd",
  "timestamp": "2025-05-15T01:12:09.197842"
}