{
  "id": 3259,
  "title": "Alternative estimation method of earnings growth rate for PEGR strategy",
  "abstract": "The ambiguous return pattern for the PEGR (the ratio of the stock's price/earnings to its estimated earnings growth rate) strategy has been documented in literature for the US stock markets. As stock prices and earnings per share (EPS) are objective data, earnings growth rate, however, is estimated by analyst whose method partial explains the PEGR vague return pattern. The purpose of this study is not to deny or substitute analysts' estimation, but rather, to provide a simple and popular method, log-linear regression model, to forecast the earnings growth rate (G), and examine whether the typical PEGR effect, such as PER (price/earnings ratio) or PBR (price/book ratio) effect, exists by using our alternative estimation method. Our evidence indeed shows that returns on the lowest PEGR portfolio not only dominate over all higher PEGR portfolios, but also beat the market with stochastic dominance (SD) analysis, which is consistent with our prediction. Our results, at least, imply that using the log-linear regression model to construct the PEGR-sorted portfolios can benefit investors and the model is also a good choice for analysts in their forecasting.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8df6df0f54c0b770c3e87a3164770a36",
  "timestamp": "2025-05-15T01:15:26.095553"
}