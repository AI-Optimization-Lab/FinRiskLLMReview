{
  "id": 3324,
  "title": "Identifying latent factors based on high-frequency data",
  "abstract": "This paper tests whether the continuous component of an observable candidate factor is in the space spanned by the counterparts of latent common factors with high-frequency financial data. We introduce two identification strategies corresponding to two types of regressions: the regressions of intraday asset returns on the estimated factors and the candidate, and the regression of the candidate factor on the estimated ones. We construct the test statistics by adding randomness to the statistics obtained from residuals of the regressions, and demonstrate the consistency of the novel randomized tests. Simulations are conducted to evaluate the performance of the tests in finite samples. We also perform empirical applications to identify the relationships between some candidate factors and the latent ones, and further use the factors selected by the tests for portfolio allocation.(c) 2022 Elsevier B.V. All rights reserved.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "415876aa6da0f585a7174255bfe7d603",
  "timestamp": "2025-05-15T01:15:54.796484"
}