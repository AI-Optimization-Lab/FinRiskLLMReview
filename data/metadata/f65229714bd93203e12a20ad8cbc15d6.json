{
  "id": 7589,
  "title": "Acceptability of and Preferences for Long-Acting Injectable HIV PrEP and Other PrEP Modalities among Sexual Minority Men in Nigeria, Africa",
  "abstract": "Sexual minority men (SMM) in Nigeria have been disproportionately affected by HIV. Pre-exposure prophylaxis (PrEP) reduces risk for HIV acquisition among SMM by over 90%. The current study investigated the association between demographics, socioeconomic marginalization, sexual health and willingness to use long-acting injectable (LAI-) PrEP and preferences for other PrEP modalities in a sample of HIV-negative SMM in Nigeria. Between March and June 2019, SMM residing in Abuja, Delta, Lagos, and Plateau completed a quantitative survey. To examine willingness to use LAI-PrEP and PrEP modality preferences, multivariable binomial and multinomial logistic regression models were fit. We found that 88% were willing to use LAI-PrEP and 44% preferred LAI-PrEP to other PrEP modalities. Participants who reported interest in LAI-PrEP were more likely to be single, engage in inconsistent condom use, and report having a primary care provider. Compared to participants who preferred daily oral PrEP, participants who preferred other PrEP modalities had higher odds of having some university education/university degree or higher and reporting low financial hardship. It is imperative that SMM in Nigeria are prioritized for access to new HIV prevention interventions, as they bear a disproportionate burden of HIV and are especially vulnerable to HIV infection.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f65229714bd93203e12a20ad8cbc15d6",
  "timestamp": "2025-05-15T03:10:05.593703"
}