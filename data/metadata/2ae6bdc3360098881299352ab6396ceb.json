{
  "id": 5325,
  "title": "Understanding the Prediction Process of Deep Networks by Forests",
  "abstract": "Deep networks have continually outperformed conventional methods on a variety of tasks. However, the unexplainable aspect of deep networks limits its application in the high impact yet high risk tasks, e.g., financial credit scenario, medical diagnosis, terrorism detection and so on. Most existing interpretable approaches focus on heuristic visualization without a clear meaning. To tackle this issue, we propose a general framework to track the prediction making process (or the rules) from the input to the latent representations of deep networks. Given a deep model, the proposed method chooses a group of hidden units by considering both importance and diversity. Each selected unit is then fit by a random forest model to imitate the local structure of the overall model. For a specific input data, all paths from input to the selected units are merged in an intersection and union way to generate the final rules. We experimentally demonstrate that the proposed method can extract rules in various scenarios, ranging from age prediction, scene classification to activity classification, to interpret the prediction process. We further analyze the validity of the rules and visualize the layer-wise rules.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2ae6bdc3360098881299352ab6396ceb",
  "timestamp": "2025-05-15T02:46:44.871355"
}