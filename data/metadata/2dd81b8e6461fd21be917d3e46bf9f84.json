{
  "id": 3206,
  "title": "The political economy of renewable portfolio standards in the United States",
  "abstract": "We investigate the political economy of Renewable Portfolio Standard (RPS) adoption, the most commonly used policy tool by U.S. states to curb greenhouse gas emissions. A growing body of research examines the drivers of RPS adoption, but this work has given scant attention to the ways in which the fossil fuel industry potentially moderates the effects of the other drivers of adoption. Therefore, we use discrete-time logistic regression analysis to estimate models of RPS adoption that include an interaction between fossil fuel production levels, and a commonly observed driver of adoption - state government ideology. We find that conservative, moderate, and liberal state governments are likely to adopt an RPS in states with no fossil fuel production. However, only moderate and liberal governments are likely to adopt in major fossil fuel producing states.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2dd81b8e6461fd21be917d3e46bf9f84",
  "timestamp": "2025-05-15T01:14:54.888652"
}