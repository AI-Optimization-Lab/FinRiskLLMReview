{
  "id": 2569,
  "title": "Nonprofit Revenue Strategy and Downside Risk: Applying Portfolio Theory and Extreme Value Theory",
  "abstract": "The risk of revenue instability is a concern for any nonprofit. Existing research leads to the well-known strategy of equalizing revenues across sources to reduce revenue volatility. This study offers several expansions to this strategy. First, rather than focusing solely on deviations from mean revenue, I incorporate extreme revenue loss, or downside risk, as it threatens organizational survival and occurs more frequently than expected. Second, while revenue equalization is indifferent to the type of revenue source, I incorporate portfolio diversification that seeks a negative correlation between sources to avoid simultaneous losses. Using 2008 to 2012 financial data, results of fixed-effects regression suggest that portfolio diversification can reduce revenue volatility and downside risk. Moreover, the relationships between financial flexibility, growth potential, and revenue risk are more nuanced compared to existing research focused solely on deviation risk. The results can help nonprofits consider incorporating downside risk in revenue portfolio management to enhance financial security.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f3fc8158d9f5ddb83ad9f9efbfee2ca2",
  "timestamp": "2025-05-15T02:17:01.598811"
}