{
  "id": 2735,
  "title": "Default models based on scale mixtures of Marshall-Olkin copulas: properties and applications",
  "abstract": "We present a unification of the Archimedean and the L,vy-frailty copula model for portfolio default models. The new default model exhibits a copula known as scale mixture of Marshall-Olkin copulas and an investigation of the dependence structure reveals that desirable properties of both original models are combined. This allows for a wider range of dependence patterns, while the analytical tractability is retained. Furthermore, simultaneous defaults and default clustering are incorporated. In addition, a hierarchical extension is presented which allows for a heterogeneous dependence structure. Finally, the model is applied to the pricing of CDO contracts. For this purpose, an efficient Laplace transform inversion approach is developed. Supporting a separation of marginal default probabilities and dependence structure, the model can be calibrated to CDS contracts in a first step. In a second step, the calibration of several parametric families to CDO contracts demonstrates a good fitting quality, which further emphasizes the suitability of the approach.",
  "year": 2013,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "302cc26055fa7e1288f907db9d6944ad",
  "timestamp": "2025-05-15T01:09:43.410670"
}