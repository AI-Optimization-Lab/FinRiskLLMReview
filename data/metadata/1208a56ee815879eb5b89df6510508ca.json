{
  "id": 2261,
  "title": "The Application of Graph-Structured Cox Model in Financial Risk Early Warning of Companies",
  "abstract": "An effective financial risk forecast depends on the selection of important indicators from a broad set of financial indicators that are often correlated with one another. In this paper, we address this challenge by proposing a Cox model with a graph structure that allows us to identify and filter out the crucial indicators for financial risk forecasting. The Cox model can be converted to a weighted least squares form for the purpose of solution, where the regularization l(0) compresses the signs of the variable coefficients and reduces the error caused by the compression of the coefficients. The graph structure reflects the correlations among different financial indicators and is incorporated into the model by introducing a Laplace penalty term to construct the Graph Regularization-Cox (GR-Cox) model. Monte Carlo simulation results show that the GR-Cox model outperforms the model without a graph structure with respect to the choice of parameters. Here, we apply the GR-Cox model to the forecast of the financial risk of listed companies and find that it shows good classification accuracy in practical applications. The GR-Cox model provides a new approach for improving the accuracy of financial risk early warning.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1208a56ee815879eb5b89df6510508ca",
  "timestamp": "2025-05-15T02:12:51.106871"
}