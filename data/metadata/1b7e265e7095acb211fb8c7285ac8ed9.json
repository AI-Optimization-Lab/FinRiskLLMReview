{
  "id": 333,
  "title": "Principled pasting: attaching tails to risk-neutral probability density functions recovered from option prices",
  "abstract": "The popular 'curve-fitting' method of using option prices to construct an underlying asset's risk neutral probability density function (RND) first recovers the interior of the density and then attaches left and right tails. Typically, the tails are constructed so that values of the RND and risk neutral cumulative distribution function (RNCDF) from the interior and the tails match at the attachment points. We propose and demonstrate the feasibility of also requiring that the left and right tails accurately price the options with strikes at the attachment points. Our methodology produces a RND that provides superior pricing performance than earlier curve-fitting methods for both those options used in the construction of the RND and those that were not. We also demonstrate that Put-Call Parity complicates the classification of in and out of sample options.",
  "year": 2023,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1b7e265e7095acb211fb8c7285ac8ed9",
  "timestamp": "2025-05-15T01:30:46.401021"
}