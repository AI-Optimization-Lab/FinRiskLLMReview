{
  "id": 1320,
  "title": "How the Subprime Crisis went global: Evidence from bank credit default swap spreads",
  "abstract": "How did the Subprime Crisis, a problem in a small corner of U.S. financial markets, affect the entire global banking system? To shed light on this question we use principal components analysis to identify common factors in the movement of banks' credit default swap spreads. We find that fortunes of international banks rise and fall together even in normal times along with short-term global economic prospects. But the importance of common factors rose steadily to exceptional levels from the outbreak of the Subprime Crisis to past the rescue of Bear Stearns, reflecting a diffuse sense that funding and credit risk was increasing. Following the failure of Lehman Brothers, the interdependencies briefly increased to a new high, before they fell back to the pre-Lehman elevated levels - but now they more clearly reflected heightened funding and counterparty risk After Lehman's failure, the prospect of global recession became imminent, auguring the further deterioration of banks' loan portfolios. At this point the entire global financial system had become infected. (C) 2012 Elsevier Ltd. All rights reserved.",
  "year": 2012,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2a98893b1941e45d2c176231b789e3c0",
  "timestamp": "2025-05-15T02:01:44.688232"
}