{
  "id": 3194,
  "title": "Higher Co-Moment CAPM and Hedge Fund Returns",
  "abstract": "This paper uses a higher moment capital asset pricing model to characterize the returns of several types of hedge fund indices. The quantile regression approach is used to test for any possible changes in the coefficients of the model. The hypothesis that the parameters are stable across the distribution of returns is tested and rejected. The most stable coefficient is the second moment (beta) coefficient. The higher moment coefficients vary considerably. Alpha returns tend to be positive and significant at the center of the distribution. The importance of higher co-moments (i.e., co-skewness and co-kurtosis) is more prevalent at the tails of the distribution of returns suggesting that there are significant tail risks. These findings could potentially have important implications for portfolio strategies and performance evaluation.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1847125496267f7317f0ad214ddd747f",
  "timestamp": "2025-05-15T01:14:54.820635"
}