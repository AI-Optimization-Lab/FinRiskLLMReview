{
  "id": 4365,
  "title": "A New Viterbi-Based Decoding Strategy for Market Risk Tracking: an Application to the Tunisian Foreign Debt Portfolio During 2010-2012",
  "abstract": "In this paper, a novel market risk tracking and prediction strategy is introduced. Our approach takes volatility clustering into account and allows for the possibility of regime shifts in the intra-portfolio's latent correlation structure. The proposed specification combines hidden Markov models (HMM) with latent factor models that takes into account the presence of both the conditional skewness and leverage effects in stock returns.A computationally efficient expectation-maximization (EM) algorithm based on the Viterbi decoder is developed to estimate the model parameters. Using daily exchange rate data of the Tunisian dinar versus the currencies of the main Tunisian government's creditors, during the 2011 revolution period, the model parameters are estimated. Then, the suitable model is used in conjunction with a Monte Carlo simulation strategy to predict the Value-at-Risk (VaR) of the Tunisian government's foreign debt portfolio. The backtesting results indicate that the new approach appears to give a good fit to the data and can improve the VaR predictions, particularly during financial instability periods.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "aede693c276cb387fd78f13d9eaf617f",
  "timestamp": "2025-05-15T02:36:38.372453"
}