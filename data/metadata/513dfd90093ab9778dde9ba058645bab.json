{
  "id": 3199,
  "title": "Equity premium prediction and optimal portfolio decision with Bagging",
  "abstract": "We propose using the statistical method of Bagging to forecast the equity premium out-of-sample for multivariate regression models. Bagging allows for the flexible and efficient extraction of valuable informational content from a large set of predictors, leading to statistically and economically significant gains relative to not only the historical mean, but also other soft-threshold methods such as forecast combinations and shrinkage estimators in our empirical results. Furthermore, we find that the source of economic gains for Bagging primarily comes from the fact that it encourages the investor to actively manage portfolio by flexibly utilizing short selling or leveraging to better time the market following correctly prognosticated trends. However, other strategies such as forecast combinations keep the equity shares nearly fixed regardless of the predicted market prospect.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "513dfd90093ab9778dde9ba058645bab",
  "timestamp": "2025-05-15T01:14:54.846768"
}