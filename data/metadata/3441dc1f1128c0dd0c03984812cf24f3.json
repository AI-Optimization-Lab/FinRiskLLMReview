{
  "id": 3869,
  "title": "An integrated efficiency evaluation of China stock market",
  "abstract": "Several studies employ data envelopment analysis (DEA) in the area of portfolio optimization or stock market efficiency evaluation, however, they have not extended two-stage network DEA to further decompose the efficiency of firms on the stock market and understand the origin of performance. Because of China's rising role in global stock market, the current research utilizes the sample of 140 firms extracted from China CSI 300 Index from 2012 to 2016 and proposes an additive two-stage network DEA model to evaluate the performance of China listed firms derived from production and financial production process with fundamental and technical analysis approaches. Analytic hierarchy process and second order cone programming are developed to determine the weights and to solve the nonlinear problem in the additive efficiency decomposition model. In addition to, future data estimated from regression analysis is applied in the DEA model to predict the performance. The main findings are as follows: The production process is more important in defining the overall efficiency of a firm on the stock market; There are significant differences in the distribution of efficiencies among various group industries; The health care, information technology or energy industry has a predicted outstanding performance in general.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3441dc1f1128c0dd0c03984812cf24f3",
  "timestamp": "2025-05-15T01:21:41.373817"
}