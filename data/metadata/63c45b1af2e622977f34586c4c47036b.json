{
  "id": 996,
  "title": "Predicting Bank Failures: A Synthesis of Literature and Directions for Future Research",
  "abstract": "Risk management has been a topic of great interest to Michael McAleer. Even as recent as 2020, his paper on risk management for COVID-19 was published. In his memory, this article is focused on bankruptcy risk in financial firms. For financial institutions in particular, banks are considered special, given that they perform risk management functions that are unique. Risks in banking arise from both internal and external factors. The GFC underlined the need for comprehensive risk management, and researchers since then have been working towards fulfilling that need. Similarly, the central banks across the world have begun periodic stress-testing of banks' ability to withstand shocks. This paper investigates the machine-learning and statistical techniques used in the literature on bank failure prediction. The study finds that though considerable progress has been made using advanced statistical and computational techniques, given the complex nature of banking risk, the ability of statistical techniques to predict bank failures is limited. Machine-learning-based models are increasingly becoming popular due to their significant predictive ability. The paper also suggests the directions for future research.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "63c45b1af2e622977f34586c4c47036b",
  "timestamp": "2025-05-15T01:57:29.934420"
}