{
  "id": 2179,
  "title": "From Local Search to Global Conclusions: Migrating Spin Glass-Based Distributed Portfolio Selection",
  "abstract": "Spin glass optimization is a distributed technique inspired by the interactions in spin glasses in nature. Spin glasses are the lattices of spins where each spin is only a part of the entire solution, in contrast to genetic algorithms (GAs), where each chromosome represents a complete solution. The interaction between spins creates special optimal patterns given appropriate temperature. This optimization paradigm is promising in complex multiobjective optimization tasks because it allows high-computational parallelism among its member spins. Furthermore, since the overall network of spins represents only one solution, there is a great promise in computational efficiency when compared with other population-based/stochastic approaches such as GAs and simulated annealing. The nature of this method is also entirely different from other distributed frameworks such as Hopfield neural network since spins' paradigm of interaction does not have to be fully connected; i.e., the neighborhoods of interactions can expand or collapse, hence less computation and better convergence speed. In this paper, we apply a heuristic method based on the spin glass model that uses migration and elitism operators in addition to temperature control in order to trace out an efficient frontier in the optimization landscape. The proposed methodology is then applied to the problem of portfolio selection. Portfolio selection is one of the nondeterministic polynomial complete problems where each asset's behavior is similar to spin's behavior and it is therefore suitable as a case study. We show that, in proper circumstances, decrementing local energy of each spin can decrement global energy of the glass, and correspondingly, if the optimization problem can be suitably mapped to the glass, the expected cost function decreases.",
  "year": 2010,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "52c9b0639fd3f391b77a5e1603f717f8",
  "timestamp": "2025-05-15T01:03:36.736684"
}