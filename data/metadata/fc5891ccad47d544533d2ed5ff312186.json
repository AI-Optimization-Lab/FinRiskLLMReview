{
  "id": 78,
  "title": "Portfolio management using online reinforcement learning with adaptive exploration and Multi-task self-supervised representation",
  "abstract": "Reinforcement learning (RL) has been widely used to make continuous trading decisions in portfolio management. However, traditional quantitative trading methods often generalize poorly under certain market conditions, whereas the output of prediction-based approaches cannot be easily translated into actionable insights for trading. Market volatility, noisy signals, and unrealistic simulation environments also exacerbate these challenges. To address the aforementioned limitations, we developed a novel framework that combines Multi-task self-supervised learning (MTSSL) and adaptive exploration (AdapExp) modules. The MTSSL module leverages auxiliary tasks to learn meaningful financial market representations from alternative data, whereas the AdapExp module enhances RL training efficiency by improving the fidelity of the simulation environment. Experimental results obtained in backtesting conducted in real financial markets indicate that the proposed framework achieved approximately 13% higher returns relative to state-of-the-art models. Furthermore, this framework can be used with various RL methods to considerably improve their performance.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fc5891ccad47d544533d2ed5ff312186",
  "timestamp": "2025-05-16T17:50:25.538838"
}