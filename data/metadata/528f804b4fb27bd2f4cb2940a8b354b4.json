{
  "id": 4209,
  "title": "Cryptocurrency returns and consumption-based asset pricing",
  "abstract": "This paper examines whether the cross-section of cryptocurrency returns is captured by risk factors based on consumption-based asset pricing. It is an imperative task for financial economists to find the fundamental risk behind characteristic-based cryptocurrency factors in order to economically understand cryptocurrency. To address the data availability issue in the analysis of cryptocurrency, we employ mixed data sampling (MIDAS) regression to check the relation between the principal component analysis (PCA) factors in cryptocurrency returns and the factors in the consumption capital asset pricing models (CCAPMs) and intertemporal capital asset pricing models (ICAPMs). We establish significant links between them, which in turn implies that cryptocurrency returns are investors' compensation for bearing consumption risk, conditional consumption risk, and intertemporal consumption risk. This finding underscores that cryptocurrency returns are the manifestation of macroeconomic equilibrium derived from investors' utility maximization.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "528f804b4fb27bd2f4cb2940a8b354b4",
  "timestamp": "2025-05-15T02:34:13.509265"
}