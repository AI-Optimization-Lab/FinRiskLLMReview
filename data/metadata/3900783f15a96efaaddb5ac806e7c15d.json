{
  "id": 2134,
  "title": "SCALABLE FINANCIAL INDEX TRACKING WITH GRAPH NEURAL NETWORKS",
  "abstract": "As a prevailing passive investment strategy in the financial world, index tracking aims at replicating or surpassing the performance of a financial index. The core part of an index tracking strategy is to design a sparse index tracking portfolio (ITP) from a basket of candidate financial assets. In this paper, a scalable two-stage approach is developed for ITP design under the minimax criterion, which consists of an asset selection stage (i.e., to select a subset of the assets from the index constituent stocks) and a capital allocation stage (i.e., to allocate the capital among the selected assets). The asset selection problem is tackled via a well-calibrated graph neural network (GNN), followed by a light-weight linear programming problem for capital allocation resolved via a standard solver. The idea proposed in this paper is novel for the area of ITP design in that it is especially scalable for tracking large-scale and dynamic-updating financial indices. Numerical simulations validate the scalability and high-efficiency of the proposed GNN-based approach with comparisons to the standard solver-based approach.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3900783f15a96efaaddb5ac806e7c15d",
  "timestamp": "2025-05-15T01:03:03.813250"
}