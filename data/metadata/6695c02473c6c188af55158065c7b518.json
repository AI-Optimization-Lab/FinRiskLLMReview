{
  "id": 4104,
  "title": "A heuristic self-organizing map trained using the Tanimoto coefficient",
  "abstract": "A variation of the Self-Organizing Map introduced by Kohonen (1983, 1995) which uses the Tanimoto similarity measure is applied to clustering a financial risk management data set comparing companies with Internet registration data to U.S. businesses in general. Instead of the traditional measure of Euclidean distance, the node with the highest value for the Tanimoto coefficient is the winner, and the nodes forming a square neighborhood around the winner node receive weight updates along with the winning node. In this model, the Tamimoto coefficient is applied two ways: first, to select the winner, and second, as the learning coefficient for the weight updating algorithm. The result is a set of weight vectors that lends itself to heuristic interpretation for data representing set membership through the use of categorical dummy variables.",
  "year": 1998,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6695c02473c6c188af55158065c7b518",
  "timestamp": "2025-05-15T02:33:03.545919"
}