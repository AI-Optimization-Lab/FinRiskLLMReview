{
  "id": 5564,
  "title": "A sparse regression and neural network approach for financial factor modeling",
  "abstract": "Factor models are central to understanding risk-return trade-offs in finance. Since Fama and French (1993), hundreds of factors have been found to have explanatory power for asset pricing. To construct a factor model, two tasks have to be performed: Feature Selection, selecting a small subset given a large number of factors to overcome overfitting in regression, and Feature Engineering, determining the interactions between the factors. In this work, the process of constructing factor models (not the factors themselves) is examined. A unified, two-step process of dimensionality reduction and nonlinear transformation that produces parsimonious, general factor models is proposed. Comparisons between frameworks implementing linear feature selection models as well as non-linear feature reduction techniques are conducted. A second stage generalizes the models by learning nonlinear interactions. The framework attempts to strike a balance between accuracy and interpretability. Results of computational experiments on historical financial data, on three models of varying degrees of nonlinearity and interpretability suggest that mixed-integer-programming-based formulations are suitable for the task of linear financial factor selection and that the second-stage nonlinearity due to neural networks improves accuracy. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "42f9fbaff0801a2a00b2a2b4b4cefe6a",
  "timestamp": "2025-05-15T02:49:21.592510"
}