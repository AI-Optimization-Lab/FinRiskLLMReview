{
  "id": 832,
  "title": "Shallow or deep? Training an autoencoder to detect anomalous flows in a retail payment system â˜†",
  "abstract": "Our paper applies a deep neural network autoencoder (AE) to detect anomalous payment flows in Canada's retail batch clearing payments system, the Automated Clearing Settlement System (ACSS). We aim to investigate an AE's potential for detecting complex changes in the liquidity outflows between participants, which could provide an early warning indication for exceptionally large outflows for a participant. As the Canadian financial system has neither faced bank runs nor severe liquidity shocks in recent history, we trained our models on normal  data and evaluated them out-of-sample using test data drawn from two constructed scenarios: a sample derived from the largest 1% of observed historical multilateral net outflows and a sample drawn from a simulated bank run. In both cases, the trained AE performed well by producing larger than usual reconstruction errors. Our approach highlights the efficacy of a class of unsupervised machine learning methods as a useful component of a system operator's risk management toolkit.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "69462d56ced51f79ca3976ec15eac278",
  "timestamp": "2025-05-15T01:55:41.559252"
}