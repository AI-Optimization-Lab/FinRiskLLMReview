{
  "id": 3612,
  "title": "Research on Electricity Demand Combination Forecasting Model in Beijing-Tianjin-Hebei Region Based on Shapley Value",
  "abstract": "Due to the electricity power is the foundation of economic development, reasonable and accurate electricity demand forecasting not only helps to develop electricity power planning scientifically, but also has important reference value to the economic development planning. In order to integrate different forecast models' strengths as well as improve the forecast accuracy, this paper puts forward the electricity demand forecasting model based on Shapley value. First, it chooses one-dimensional linear regression model, Holt two-parameter linear exponential smoothing model and ARIMA model for electricity demand forecasting respectively, and through the game theory Shapley value theory determines the weight of single model in the portfolio model, and then the combined forecasting result is obtained; Second, it analyzes the electricity demand data in Beijing-Tianjin-Hebei Region and the results show that the proposed method has high forecast accuracy; Finally, it forecasts electricity demand in Beijing-Tianjin-Hebei Region during 13th five-year period and provides decision-making basis for the economic development planning of Beijing, Tianjin Hebei province during 13th five-year period.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "569653abbe4232209ac71619af853f67",
  "timestamp": "2025-05-15T01:19:00.696619"
}