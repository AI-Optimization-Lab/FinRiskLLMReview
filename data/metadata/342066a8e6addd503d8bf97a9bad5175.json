{
  "id": 687,
  "title": "Integrating deep transformer and temporal convolutional networks for SMEs revenue and employment growth prediction",
  "abstract": "Although the accurate potential for growth prediction is very important for Government grants and contributions programs to better support Small and Medium-sized Enterprises (SMEs), it is a challenging task due to the data heterogeneity (both structured data and free text data bilingual in English and French), the class imbalance issue, and the difficulties in efficient feature learning. To address these challenges, this paper presents a novel BERT-TCN model for portfolio predictions in government funding programs, with the following key contributions. First, we describe the application of a novel architecture to a prediction task involving sequential, structured, partially quantitative input data and free text input data. Specifically, our novel model predicts the growth of firms receiving government funding for innovation. Our model also deals with class imbalance in the data and the difficulties in efficient feature learning. Our model integrates a Transformer model, i.e., BERT, for text modeling with a Temporal Convolutional Network (TCN) for sequential prediction. Second, we also developed various performance evaluation criteria in Section 4.3, allowing comprehensive assessments of the proposed approach from both the machine learning perspective and funding program -specific perspective. Third, the importance of features (both text and numerical features) is quantified and evaluated, allowing insights into how different features contribute to the prediction and explainability of the proposed model. The proposed approach is trained and tested on a large dataset from a rich database, demonstrating that the proposed approach can greatly help individual human experts improve their results.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "342066a8e6addd503d8bf97a9bad5175",
  "timestamp": "2025-05-15T00:46:24.555815"
}