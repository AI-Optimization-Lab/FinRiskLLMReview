{
  "id": 283,
  "title": "Bankrobotics: Artificial Intelligence and Machine Learning Powered Banking Risk Management Prevention of Money Laundering and Terrorist Financing",
  "abstract": "Based on a country study related to money laundering and terrorist financing, the Financial Action Group downgraded Hungary's compliance with Recommendation R15 (use of new technologies). At the same time, between 2020 and 2021, the Magyar Nemzeti Bank imposed fines on several commercial banks operating in Hungary for shortcomings on complying with money laundering and terrorist financing regulations. As a gap-filling analysis, the study examines supervised (classification, regression), unsupervised (clustering, anomaly detection), and hybrid machine learning models and algorithms operating based on highly unbalanced dataset of anti-money laundering and terrorist financing prevention of banking risk management. The author emphasizes that there is no one ideal algorithm. The choice between machine learning algorithm is highly determined based on the underlying theoretical logic and additional comparative. Model building requires a hybrid perspective of the give business unit, IT and visionary management.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "414b2014decaa6d40f947ffd3c4685c4",
  "timestamp": "2025-05-15T01:49:04.574840"
}