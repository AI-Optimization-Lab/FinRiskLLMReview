{
  "id": 4964,
  "title": "Physicians' Trust in Relevant Institutions during the COVID-19 Pandemic: A Binary Logistic Model",
  "abstract": "Little research has been done on professionals' perceptions of institutions and governments during epidemics. We aim to create a profile of physicians who feel they can raise public health issues with relevant institutions during a pandemic. A total of 1285 Romanian physicians completed an online survey as part of a larger study. We used binary logistic regression to profile physicians who felt they were able to raise public health issues with relevant institutions. Five predictors could differentiate between respondents who tended to agree with the trust statement and those who tended to disagree: feeling safe at work during the pandemic, considering the financial incentive worth the risk, receiving training on the use of protective equipment, having the same values as colleagues, and enjoying work as much as before the pandemic. Physicians who trusted the system to raise public health issues with the appropriate institutions were more likely to feel that they shared the same values as their colleagues, to say they were trained to use protective equipment during the pandemic, to feel that they were safe at work during the pandemic, to enjoy their work as much as before the pandemic, and to feel that the financial bonus justified the risk.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "bb74fb02395c591ec803ee32c020cc76",
  "timestamp": "2025-05-15T02:43:06.672917"
}