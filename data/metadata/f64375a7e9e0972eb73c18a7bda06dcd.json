{
  "id": 1480,
  "title": "Efficient planning through automatic configuration and machine learning",
  "abstract": "Although in the last decade the performance of domain-independent planners has significantly improved, there is no planner that outperforms all the others in every benchmark domains. In many domains, the planning performance can be improved by automatically deriving and exploiting knowledge about the domain and problem structure that is not explicitly given in the input formalization, and that can be used for optimizing the planner behavior. This thesis proposes three innovative techniques for deriving and using additional domain and problem knowledge through automatic algorithm configuration and machine learning techniques: a prediction model of the planning horizons, a method for synthesizing domain-optimized planners from a highly parameterized generic planner, and a system for the automatic configuration of a portfolio planner.",
  "year": 2013,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f64375a7e9e0972eb73c18a7bda06dcd",
  "timestamp": "2025-05-15T00:56:08.398119"
}