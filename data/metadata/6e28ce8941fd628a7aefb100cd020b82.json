{
  "id": 1115,
  "title": "Interpretability Meets Generalizability: A Hybrid Machine Learning System to Identify Nonlinear Granger Causality in Global Stock Indices",
  "abstract": "Globalization has posed challenges to financial risk management, connecting markets with each other, and making it more difficult to diversify the portfolio to uncorrelated markets than ever in history. In light of this growing complexity of causal relationships between global stock markets, nonlinear Granger causality has superseded its linear counterpart in providing quantitative evidence for these relationships. In this paper, we propose a hybrid system that extends existing nonlinear Granger causality frameworks using machine learning-based time series prediction models. We improve the accuracy of identifying nonlinear Granger causality by combining p-values of causality statistics from individual machine learning models. By adjusting a model independence coefficient, our model is generalized to datasets where the strength of causality varies. Meanwhile, the causality statistic is still interpretable, because the distribution and critical value are known. Our findings challenge the current understanding that the United States market has the dominating influence and show that Asian markets play a significant role in spreading financial risk worldwide.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6e28ce8941fd628a7aefb100cd020b82",
  "timestamp": "2025-05-15T01:59:21.566471"
}