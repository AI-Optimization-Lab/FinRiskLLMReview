{
  "id": 572,
  "title": "TraderNet-CR: Cryptocurrency Trading with Deep Reinforcement Learning",
  "abstract": "The predominant method of developing trading strategies is technical analysis on historical market data. Other financial analysts monitor the public activity towards cryptocurrencies, in order to forecast upcoming trends in the market. Until now, the best cryptocurrency trading models rely solely on one of the two methodologies and attempt to maximize their profits, while disregarding the trading risk. In this paper, we present a new machine learning approach, named TraderNetCR, which is based on deep reinforcement learning. TraderNet-CR combines both methodologies in order to detect profitable round trips in the cryptocurrency market and maximize a trader's profits. Additionally, we have added an extension method, named N-Consecutive Actions, which examines the model's previous actions, before suggesting a new action. This method is complementary to the model's training and can be fruitfully combined, in order to further decrease the trading risk. Our experiments show that our model can properly forecast profitable round trips, despite high market commission fees.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8d5ba5116a39a65b3967fc2637f8fecf",
  "timestamp": "2025-05-15T01:52:49.401210"
}