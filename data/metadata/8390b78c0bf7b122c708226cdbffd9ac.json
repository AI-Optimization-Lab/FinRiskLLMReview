{
  "id": 6213,
  "title": "Forecasting Chinese cruise tourism demand with big data: An optimized machine learning approach",
  "abstract": "After more than ten years of exponential development, the growth rate of cruise tourist in China is slowing down. There is increasingly financial risk of investing in homeports, cruise ships and promotional activities. Therefore, forecasting Chinese cruise tourism demand is a prerequisite for investment decision-making and planning. In order to enhance forecasting performance, a least squares support vector regression model with gravitational search algorithm (LSSVR-GSA) is proposed for forecasting cruise tourism demand with big data, which are search query data (SQD) from Baidu and economic indexes. In the proposed model, hyper-parameters of the LSSVR model are optimized with GSA. By comparing these models with various settings, we find that LSSVR-GSA with selected mobile keywords and economic indexes can achieve the highest forecasting performance. The results indicate the proposed framework of the methodology is effective and big data can be helpful predictors for forecasting Chinese cruise tourism demand.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8390b78c0bf7b122c708226cdbffd9ac",
  "timestamp": "2025-05-15T02:56:10.553995"
}