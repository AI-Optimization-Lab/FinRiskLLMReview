{
  "id": 2778,
  "title": "AN IMPROVED TEST OF THE SQUARED SHARPE RATIO",
  "abstract": "The sample squared Sharpe ratio (SSR) is a critical statistic of the risk-return tradeoff. We show that sensitive upper-tail probabilities arise when the sample SSR is employed to test the mean-variance efficiency under different test statistics. Assuming the error's normality with a nonzero mean, we integrate the sample SSR and the arbitrage regression into a noncentral chi-square (chi(2)) test. We find that the distribution of the sample SSR based on the regression error is to the left of the F-distribution when assuming the returns' normality. Compared to two benchmarks that use the noncentral F-distribution and the central F-statistic, the chi(2)-statistic is more effective, competitive, significant, and locally robust when used to reject the upper-tailed mean-variance efficiency test using the usual parameters (sample size, portfolio size, and SSR).",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ddccd85e0cabdddcdda461b3aab850ed",
  "timestamp": "2025-05-15T01:10:16.549034"
}