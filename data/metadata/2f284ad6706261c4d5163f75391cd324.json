{
  "id": 228,
  "title": "Mixed data sampling expectile regression with applications to measuring financial risk",
  "abstract": "To avoid information loss or measurement error in traditional methods dealing with mixed frequency data, we develop a novel mixed data sampling expectile regression (MIDAS-ER) model to measure financial risk. We construct the MIDAS-ER model by introducing a MIDAS structure into expectile regressions. This enables us to perform an expectile regression on raw mixed frequency data directly. We apply the proposed MIDAS-ER model to estimate two popular financial risk measures, namely, Value at Risk and Expected Shortfall, with both simulated data and four stock indices, and compare the model's performance with those of several popular models. The outstanding performance of our model demonstrates that high-frequency information helps to improve the accuracy of risk measurement. In addition, the numerical results also imply that our model can be a significant tool for risk-averse investors to control risk losses and for financial institutions to implement robust risk management.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2f284ad6706261c4d5163f75391cd324",
  "timestamp": "2025-05-15T01:48:30.680833"
}