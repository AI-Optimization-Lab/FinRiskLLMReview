{
  "id": 3504,
  "title": "Modeling extreme negative returns using marked renewal Hawkes processes",
  "abstract": "Extreme return financial time series are often challenging to model due to the presence of heavy temporal clustering of extremes and strong bursts of return volatility. One approach to model both these phenomena in extreme financial returns is the marked Hawkes self-exciting process. However, the Hawkes process restricts the arrival times of exogenously driven returns to follow a Poisson process and may fail to provide an adequate fit to data. In this work, we introduce a model for extreme financial returns, which provides added flexibility in the specification of the background arrival rate. Our model is a marked version of the recently proposed renewal Hawkes process, in which exogenously driven extreme returns arrive according to a renewal process rather than a Poisson process. We develop a procedure to evaluate the likelihood of the model, which can be optimized to obtain estimates of model parameters and their standard errors. We provide a method to assess the goodness-of-fit of the model based on the Rosenblatt residuals, as well as a procedure to simulate the model. We apply the proposed model to extreme negative returns for five stocks traded on the Australian Stock Exchange. The models identified for the stocks using in-sample data were found to be able to successfully forecast the out-of-sample risk measures such as the value at risk and provide a better quality of fit than the competing Hawkes model.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "eb3836fb0611af410feb39eac70b1d1e",
  "timestamp": "2025-05-15T02:26:48.590436"
}