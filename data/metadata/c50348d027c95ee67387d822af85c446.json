{
  "id": 533,
  "title": "Use of Prediction Bias in Active Learning and Its Application to Large Variable Annuity Portfolios",
  "abstract": "Given the computational challenges associated with valuing large variable annuity (VA) portfolios, a variety of data mining frameworks, including metamodeling and active learning, have been proposed in recent years. Active learning, a promising alternative to metamodeling, enhances the efficiency of VA portfolio assessments by adaptively improving a predictive regression model. This is achieved by augmenting data for model training with strategically selected informative samples. Successful application of active learning requires an effective metric in order to gauge the informativeness of data. Current sampling methods, which focus on prediction error-based informativeness, typically rely solely on prediction variance and assume an unbiased predictive model. In this paper, we address the fact that prediction bias can be nonnegligible in large VA portfolio valuation and investigate the impact of prediction bias in both the modeling and sampling stages of active learning. Our experimental results suggest that bias-based sampling can rival the efficacy of traditional ambiguity-based sampling, with its success contingent upon the extent of bias present in the predictive model.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c50348d027c95ee67387d822af85c446",
  "timestamp": "2025-05-15T00:44:21.128177"
}