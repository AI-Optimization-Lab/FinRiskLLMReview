{
  "id": 5714,
  "title": "Volatility risk premium in the interest rate market: Evidence from delta-hedged gains on USD interest rate swaps",
  "abstract": "This study examines whether the interest rate market compensates for volatility risk. It demonstrates that the delta-hedged gain (DHG) method introduced by Balcshi and Kapadia (2003) shows the existence and sign of DHG in the interest rate swap markets where they use measures different from what Bakshi and Kapadia assumed. This finding is applied to the USD interest rate swap and swaption market. The result shows that, over the short term, there is negative compensation for volatility risk premiums, aldn to the equity or currency markets. Over the long term, the signs of compensation change and regression tests show the possibility that the volatility risk premium in the interest rate market can be different from those in other asset markets. However, this interpretation entails an overlapping data problem that is not easy to overcome especially for the long term DHG data. The difference in interest rate market may be due to the fact that the interest rate swap market is different from the equity or currency markets in that it is more driven by financial institutions and option traders than by individuals or directional traders. (C) 2015 Elsevier Inc. All rights reserved.",
  "year": 2015,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9f760982e356e5b8a4f1bace0eb4368b",
  "timestamp": "2025-05-15T02:50:53.842711"
}