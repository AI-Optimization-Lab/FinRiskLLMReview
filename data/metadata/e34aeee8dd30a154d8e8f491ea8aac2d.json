{
  "id": 2582,
  "title": "Deciphering the influence of the macroeconomic environment on economic preferences: A comprehensive analysis of the Global Preferences Survey",
  "abstract": "This study investigates the nuanced impact of macroeconomic conditions on various economic preferences, broadening the scope beyond conventional risk preference analysis. Utilizing the Global Preference Survey data with a sophisticated nonlinear regression model, we reveal that favorable macroeconomic conditions boost patience, yet simultaneously reduce risk aversion, altruism, willingness to penalize unfair actions, and trust levels. These significant shifts in economic preferences, driven by macroeconomic scenarios, carry substantial implications for financial markets, investment behaviors, and the broader macroeconomy, necessitating in-depth exploration in financial research and policy design.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e34aeee8dd30a154d8e8f491ea8aac2d",
  "timestamp": "2025-05-15T02:17:01.671245"
}