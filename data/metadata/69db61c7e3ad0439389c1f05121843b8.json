{
  "id": 1720,
  "title": "Exploiting the low-risk anomaly using machine learning to enhance the Black-Litterman framework: Evidence from South Korea",
  "abstract": "Many studies have revealed that global financial markets are experiencing low-risk anomalies. In the Korean market, for example, even the portfolios of high-risk stocks recorded a loss of about 70% between 2000 and 2016. In this study, we construct a low-risk portfolio that responds to low-risk anomalies in the Korean market using the Black-Litterman framework. We use three machine-learning predictive and traditional time-series models to predict the volatility of assets listed in the Korean Stock Price Index 200 (KOSPI 200) and select the best-performing one. Then, we use the model to classify assets into high- and low-risk groups and create a Black-Litterman portfolio that reflects the investor's view where low-risk stocks outperform high-risk stocks. The experiment shows that reflecting the low-risk view in the market equilibrium portfolio improves profitability and that this view dominates the market portfolio.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "69db61c7e3ad0439389c1f05121843b8",
  "timestamp": "2025-05-15T02:06:52.661175"
}