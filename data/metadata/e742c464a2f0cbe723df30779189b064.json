{
  "id": 2373,
  "title": "Learning financial asset-specific trading rules via deep reinforcement learning",
  "abstract": "Generating asset-specific trading signals based on the financial conditions of the assets is one of the challenging problems in automated trading. Various asset trading rules are proposed experimentally based on different technical analysis techniques. However, these kind of trading strategies are profitable, extracting new asset specific trading rules from vast historical data to increase total return and decrease the risk of portfolios is difficult for human experts. Recently, various deep reinforcement learning (DRL) methods are employed to learn the new trading rules for each asset. In this paper, a novel DRL model with various feature extraction modules is proposed. The effect of different input representations on the performance of the models is investigated and the performance of DRL-based models in different markets and asset situations is studied. The proposed model in this work outperformed the other state-of-the-art models in learning single asset-specific trading rules and obtained almost 12.4% more profit over the best state-of-the-art model on the Dow Jones Index in the same time period.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e742c464a2f0cbe723df30779189b064",
  "timestamp": "2025-05-15T02:14:50.279584"
}