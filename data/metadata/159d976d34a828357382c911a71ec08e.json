{
  "id": 2520,
  "title": "Building Energy Management for Demand Response Using Kernel Lifelong Learning",
  "abstract": "Demand response (DR) aims at improving the reliability and efficiency of the power grids by shaping the power demand over time. Given that building energy consumption constitutes a significant portion of the overall grid load, building energy management is a critical component for the DR portfolio. In this study, DR control policies for lighting and air-conditioner systems for the individual spaces in buildings are proposed. The policies are designed to achieve the energy reduction amount specified in the DR request while minimizing the user discomfort. A significant challenge is to cope with the uncertainty of various environmental factors such as the solar illuminance and ambient temperature, as well as the psycho-economic factors such as the energy usage preferences of the occupants. We employ a data-driven machine learning approach to tackle this challenge. Our novel idea is to take advantage of the structural similarity of the control policies across the spaces in a lifelong multi-task learning framework. To accommodate significant nonlinearity in efficient policies, a kernel-based learning approach is pursued. The dual decomposition method is employed to relax the constraint coupled across the spaces, which allows solving the overall learning problem via a series of unconstrained subproblems. The efficacy of the proposed method is verified by numerical experiments based on semi-real data sets.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "159d976d34a828357382c911a71ec08e",
  "timestamp": "2025-05-15T01:07:35.740929"
}