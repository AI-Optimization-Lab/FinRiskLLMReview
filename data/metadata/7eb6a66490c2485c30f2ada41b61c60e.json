{
  "id": 3222,
  "title": "What variables make a forest stand vulnerable to browsing damage occurrence?",
  "abstract": "Ungulate browsing results in important damages on the forests, affecting their structure, composition and development. In the present paper, we examine the occurrence of browsing damage in Norwegian forests, using data provided by the National Forest Inventory along several consecutive measurements (entailing the period 1995-2014). A portfolio of variables describing the stand, site and silvicultural treatments are analyzed using classification trees to retrieve combinations related to browsing damage. Our results indicate that the most vulnerable forest stands are young with densities below 1400 trees ha(-1) and dominated by birch, pine or mixed species. In addition, stand diversity and previous treatments (e.g. thinnings) increase the damage occurrence and other variables, like stand size, could play a role on forest susceptibility to browsing occurrence although the latter is based on weaker evidence. The methods and results of our study can be applied to implement management measures aiming at reducing the browsing damages of forests.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7eb6a66490c2485c30f2ada41b61c60e",
  "timestamp": "2025-05-15T01:14:54.969494"
}