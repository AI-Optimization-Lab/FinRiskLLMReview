{
  "id": 1670,
  "title": "Currency Risk Modelling by GARCH-Copula Model",
  "abstract": "Time series modelling and subsequent risk estimation is difficult and important activity of any financial institution. Financial time series are characterized by volatility clustering and heavy-tailed distribution of returns. Both these characteristics have a great influence for risk estimation. Especially when modelling more-dimensional probability distribution, shocks in terms of extreme losses (or returns) in particular risk drivers are usually more correlated than the losses (returns) closer to the mean. In this paper we focus on GARCH-copula models. The copula functions are the tool which allows us to model the dependence among individual risk drivers. On the other hand, GARCH model allows depicting the volatility clustering. Concretely, GARCH model with Student distribution of innovations and various copula functions are assumed in the paper. These joined models are backtested on chosen dataset and VaR exceedances (i.e. their quantity and distribution in time) are statistically tested by Kupiec and Christoffersen tests.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "78d533d52b64f9ce0f6189f0b1255dfd",
  "timestamp": "2025-05-15T02:06:22.394058"
}