{
  "id": 379,
  "title": "Attention-based CNN-LSTM for high-frequency multiple cryptocurrency trend prediction",
  "abstract": "With the price of Bitcoin, Ethereum, and many other cryptocurrencies climbing, the cryptocurrency market has become the most popular investment area in recent years. Unlike other relatively more stable financial derivatives, the cryptocurrency market has high volatility which requires a high-frequency prediction model for quantitative trading. However, the excessive number of trading becomes a critical issue due to the instability of the prediction results and high error rate. To relieve such a problem, based on the observation of high frequency data, we use local minimum series to replace the original series and propose a more stable triple trend labeling method that reduces the number of trades by potentially influencing the training of the model. In addition, a new attention-based CNN-LSTM model for multiple cryptocurrencies (ACLMC) is proposed to optimize model effects by exploiting correlations across frequencies and currencies, and to smooth out the investment risk associated with prediction errors by supporting simultaneous multi-currency predictions. Experiments show that our labeling method with ACLMC can achieve much better financial metrics and fewer number of transactions than traditional baselines.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5c8f4af1834ddf86c9860d3cb91e5cb6",
  "timestamp": "2025-05-15T01:37:32.456924"
}