{
  "id": 1998,
  "title": "COMPARISON OF STOCK SELECTION METHODS: AN EMPIRICAL RESEARCH ON THE BORSA ISTANBUL",
  "abstract": "This paper compares the performances of stock selection methods developed by artificial neural network (ANN), second order stochastic dominance (SSD), and Markowitz portfolio optimization by generating annual portfolios whose stocks are selected from several types of indexes traded in the Borsa Istanbul. Daily returns in SSD and Markowitz, and annual ratios in ANN models, are taken as inputs, with the following annual returns as outputs. By the perspective of stock selection literature, this study carries unique value for including comparisons of these methods with the purpose of generating portfolios with higher returns. Thus, two questions emerge: Are these methods able to overcome losses during financial crises and bear or bull periods, and can they provide positive alpha? Results indicate that average returns of portfolios generated by ANN are relatively higher than SSD and Markowitz, but all three models provide positive alpha over indexes. However, none of the models could overcome negative returns during economic crises.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "26d95fe326ea4ef58a2948a55b48b0c8",
  "timestamp": "2025-05-15T01:02:10.121756"
}