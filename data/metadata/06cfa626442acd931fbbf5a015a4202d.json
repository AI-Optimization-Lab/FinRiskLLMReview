{
  "id": 5869,
  "title": "Multinomial Ordinal Response Models",
  "abstract": "For managing of credit risk in banks, it is fundamental to decide to which customer a loan or service will be offered and to which one it will not. The banks use many methods modeling and forecasting risk - application scorecards, probability of default etc. Inside these methods mainly binary models and logistic regression as technique are applied. The article intends to shortly illustrate and describe extension of these models on situations where response (dependent) variable has more than two categories. The binary models (or dichotomous, i.e. models in which dependent variable has just two categories) are not the only ones, it is possible to use models in which dependent variable has more than two categories - multinomial models. There are two main groups of such models: with ordered (ordinal) categories and non-ordered (nominal) categories of the dependent variable. The article describes representatives from both groups, but focus on ordinal models, which are likely to be used in the financial institutions than nominal ones. For both groups practical examples are calculated and their results are discussed.",
  "year": 2013,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "06cfa626442acd931fbbf5a015a4202d",
  "timestamp": "2025-05-15T02:52:31.366245"
}