{
  "id": 938,
  "title": "Stock Ranking Prediction Based on an Adversarial Game Neural Network",
  "abstract": "With the globalization of the economy and financial markets today, predicting stock rankings and constructing appropriate portfolio strategies have become hot research topics for many scholars. However, because the stock market has different styles in different periods, market-style switching will seriously affect the prediction performance of the model. To eliminate the influence of style exposure and make the stock selection performance of the deep learning model more balanced, we propose an adversarial game neural network model based on LSTM and an attention mechanism for stock ranking prediction. We also combined trading tasks to construct an MS-WRSE loss function that considers stock rankings to optimize the network. Compared with classic time series prediction models, the adversarial game neural network can eliminate the influence of market-style factors on stock ranking predictions through the mutual game between the main neural network and the auxiliary neural network. This can strengthen the stock selection performance of the model.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c3d4ecefe10f057129015541721fdf4e",
  "timestamp": "2025-05-15T00:49:36.423522"
}