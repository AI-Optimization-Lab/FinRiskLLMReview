{
  "id": 5112,
  "title": "Measuring CoVaR: An Empirical Comparison",
  "abstract": "Recent literature has proposed a market-based measure to assess the contribution of a single bank to the systemic risk, i.e. the delta conditional value-at-risk (Delta CoVaR). This measure could be useful to control the dynamics of systemic risk as perceived by the market. We estimate the Delta CoVaR of Italian and main European banks over the time span from January 2007 to December 2018 by considering three possible methodologies: (1) the quantile regression; (2) a closed form formula; (3) a non-parametric method. The estimates based on closed form formula do not differ substantially from those of the other two methodologies, moreover they provide more robust results. Furthermore, we compare the ranking derived by the Delta CoVaR with the global systemically important banks (GSIBs) buckets determining additional loss absorbency requirements. We show that there are differences in the ranking defined by the Delta CoVaR and the GSIBs bucket allocation provided by the Financial Stability Board even if the Delta CoVaR seems to be able to divide the good from the bad, from a systemic risk perspective.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "493a3589e7c3cb43fb3bbfbe2196a170",
  "timestamp": "2025-05-15T02:44:11.129404"
}