{
  "id": 675,
  "title": "Anomaly Detection Aided Budget Online Classification for Imbalanced Data Streams",
  "abstract": "Learning from imbalanced data streams differs fromthe traditional learning paradigm due to the issues of imbalanced classes. It has significant implications in a myriad of real-world applications, ranging from financial risk, network security, to medical diagnosis. Moreover, outliers usually appear in data streams. The issue of class imbalance or anomaly itself could negatively affect the performance of the underlying learning algorithms, and their combination makes the learning problemharder. In this work, we propose an anomaly detection aided budget online weighted learning method (BOW-LM) to identify positive and negative instances from imbalanced data streams. BOW-LMis based on the widely used Feedforward Networks with Random Weights. An agile lightweight anomaly detector is designed based on the nonlinear mapping of the network. To reduce computational complexity and to response promptly, BOW-LM employs amatrix correction technique to update the learning model by only O(L-2) operations for each data chunk with L hidden layer nodes. Empirical studies on both synthetic and real-world datasets demonstrate the effectiveness of BOW-LM.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c3113298ef5d30501655861c551cacab",
  "timestamp": "2025-05-15T01:41:29.787770"
}