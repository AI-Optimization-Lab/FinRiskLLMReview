{
  "id": 2295,
  "title": "Nonparametric machine learning models for predicting the credit default swaps: An empirical study",
  "abstract": "Credit default swap which reflects the credit risk of a firm is one of the most frequently traded credit derivatives. In this paper, we conduct a comprehensive study to verify the predictive performance of non parametric machine learning models and two conventional parametric models on the daily credit default swap spreads of different maturities and different rating groups, from AA to C. The whole period of data set used in this study runs from January 2001 to February 2014, which includes the global financial crisis period when the credit risk of firms were very high. Through experiments, it is shown that most nonparametric models used in this study outperformed the parametric benchmark models in terms of prediction accuracy as well as the practical hedging measures irrespective of the different credit ratings of the firms and the different maturities of their spreads. Especially, artificial neural networks showed better performance than the other parametric and nonparametric models. (C) 2016 Elsevier Ltd. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3f908ec157cb9b5dfff4d9caeefa3aa9",
  "timestamp": "2025-05-15T02:13:31.867273"
}