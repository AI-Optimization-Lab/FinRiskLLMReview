{
  "id": 4180,
  "title": "Optimal time-varying tail risk network with a rolling window approach",
  "abstract": "We contribute to the literature on financial network contagion and systemic risk by developing a time-varying framework based on the rolling window technic and high dimensional quantile regression. The new selection criterion enables us to determine the optimal rolling width, which trades off the estimation accuracy and time variation of the tail risk network. Monte Carlo simulations show that our procedure significantly improves upon the estimation performance of the time-varying model. Using Chinese banking's market data over 2009-2019, we measure the time-varying tail risk connectedness among banks using idiosyncratic returns. We find strong evidence of prominent risk dependence across banks during the stock crash. The network model using the optimal rolling window outperforms the traditional approaches in capturing structural changes. (C) 2021 Elsevier B.V. All rights reserved.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "15aa11a49190f4fdd33b98409aaa2093",
  "timestamp": "2025-05-15T02:34:13.398688"
}