{
  "id": 1838,
  "title": "Automatic suppression of false positive alerts in anti-money laundering systems using machine learning",
  "abstract": "Criminal activities generate an estimated $2 trillion in laundered money per year, highlighting the need for financial institutions to detect and report suspicious activity to protect their reputation. However, rule-based models commonly used for this purpose generate a high number of false positives, draining compliance team time, and increasing investigation costs. However, the application of machine learning in conjunction with rule-based models presents noteworthy implications, encompassing the potential reduction in false positives and the concomitant risk of machine learning inadvertently suppressing true positive alerts. This paper proposes a framework called automatic suppression based on XGBoost for anti-money laundering (ASXAML) to enhance detection by reducing false positives. ASXAML leverages recursive feature elimination with cross-validation for optimal feature selection. Subsequently, Optuna is employed to fine-tune hyperparameters for the XGBoost model. Results indicate that ASXAML achieves an optimal balance between reducing false positives and avoiding missed money laundering events, with an 86% F-beta score and only 11% money laundering customers were incorrectly closed out of 1926 in the test data.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "190c0400a37650ebb90af166366fcaca",
  "timestamp": "2025-05-15T02:08:03.958042"
}