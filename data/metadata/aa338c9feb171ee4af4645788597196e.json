{
  "id": 3082,
  "title": "Asset allocation under predictability and parameter uncertainty using LASSO",
  "abstract": "We consider a short-term investor who exploits return predictability in stocks and bonds to maximize mean-variance utility. Since the true parameters are unknown, we resort to portfolio optimization in form of linear regression with LASSO in order to mitigate problems related to estimation errors. As standard cross-validation relies on the assumption of i.i.d. returns, we propose a new type of cross-validation that selects lambda from simulated returns sampled from a multivariate normal distribution. We find an inverse U-shaped relationship between the selected lambda and the expected utility, and we show that the optimal value of lambda declines as the number of observations used to estimate the parameters increases. We finally show how our strategy outperforms some commonly employed benchmarks.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "aa338c9feb171ee4af4645788597196e",
  "timestamp": "2025-05-15T01:13:09.909979"
}