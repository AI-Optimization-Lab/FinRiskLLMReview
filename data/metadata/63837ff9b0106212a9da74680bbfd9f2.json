{
  "id": 3518,
  "title": "Bidirectional Gated Recurrent Unit Neural Networks for Relation Extraction of Chinese Enterprises",
  "abstract": "With the improvement of people's entrepreneurial awareness and the encouragement of policies, the number of enterprises is gradually increasing. However, the profit-seeking nature of enterprises has increased the number of criminal activities. In order to help financial practitioners effectively identify high-risk enterprises, legal persons or shareholders, domestic and foreign scholars construct enterprise knowledge graphs for risk warning. Extracting enterprise relations from unstructured data such as financial news plays an important role in constructing enterprise knowledge graphs, but its irregular data structure and scarce processing tools bring challenges to relation extraction. To solve this problem, this paper proposes SDP-BGRU model to extract enterprise relations from unstructured data and treats enterprise relations extraction as a classification problem. The model uses the shortest dependency path (SDP) between two entities, obtains feature vectors through Bidirectional Gated Recurrent Unit (BGRU) networks and support vector machines as classifier. Experimental results show that the model can achieve good results on test data.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "63837ff9b0106212a9da74680bbfd9f2",
  "timestamp": "2025-05-15T02:27:19.063392"
}