{
  "id": 2728,
  "title": "New room to maneuver? National tax policy under increasing financial transparency",
  "abstract": "Why have Organisation for Economic Co-operation and Development (OECD) governments raised taxes on dividends at the shareholder level since 2008? Previous research points to the importance of budget deficits and voter demand for compensatory fairness in the aftermath of the financial crisis. We complement this literature by showing that the effect of domestic drivers of tax increases on capital income crucially depends on the level of financial transparency in a country's investment network. Low financial transparency increases the risk of capital flight in response to a tax hike, whereas high financial transparency reduces this risk. Hence, governments facing fiscal pressure become more likely to raise taxes on capital income when transparency is high. To substantiate our argument, we construct an original indicator of financial transparency in countries' investment networks, which we utilize in a regression analysis of tax reforms by 204 cabinets in 35 OECD countries between 2001 and 2018.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "23ef17174b7688865382c44eb0147f02",
  "timestamp": "2025-05-15T02:18:37.095823"
}