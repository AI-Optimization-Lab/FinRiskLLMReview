{
  "id": 3186,
  "title": "A penalty decomposition approach for multi-objective cardinality-constrained optimization problems",
  "abstract": "In this manuscript, we consider multi-objective optimization problems with a cardinality constraint on the vector of decision variables and additional linear constraints. For this class of problems, we analyse necessary and sufficient conditions of Pareto optimality. We afterwards propose a Penalty Decomposition type algorithm, exploiting multi-objective descent methods, to tackle the aforementioned family of problems. We conduct a rigorous convergence analysis for the proposed method, where we prove that the produced sequence of points has limit points, each one being feasible and satisfying first-order optimality conditions. Numerical computational experiments, carried out on instances of relevant real-world problems such as sparse mean/variance portfolio selection and sparse regularized logistic regression, in their multi-objective formulation, show that the proposed procedure is effective at finding solutions forming good Pareto sets approximations.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "834203b652acbae967fdef4feefd2fc8",
  "timestamp": "2025-05-15T01:14:16.306942"
}