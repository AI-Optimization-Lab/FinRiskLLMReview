{
  "id": 2208,
  "title": "Explainable deep learning model for predicting money laundering transactions",
  "abstract": "Money laundering has been a global issue for decades. The ever-changing technology landscape, digital channels, and regulations make it increasingly difficult. Financial institutions use rule-based systems to detect suspicious money laundering transactions. However, it suffers from large false positives (FPs) that lead to operational efforts or misses on true positives (TPs) that increase the compliance risk. This paper presents a study of convolutional neural network (CNN) to predict money laundering and employs SHapley Additive exPlanations (SHAP) explainable artificial intelligence (AI) method to explain the CNN predictions. The results highlight the role of CNN in detecting suspicious transactions with high accuracy and SHAP's role in bringing out the rationale of deep learning predictions.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "55adf8fe94e930d8e68bae8cf49798d7",
  "timestamp": "2025-05-15T02:12:17.468268"
}