{
  "id": 5989,
  "title": "The Anxiety and Depression of International Medical Students during COVID-19 Pandemic: A Cross-Sectional Study",
  "abstract": "The COVID-19 pandemic has inflicted physical harm and exacerbated a significant mental health crisis, warranting greater attention. This study investigated the prevalence of anxiety and depression among international medical students (IMSs) during the pandemic and explored its correlation with demographic factors. Participants completed a comprehensive questionnaire encompassing demographic details, the Zung self-rating anxiety scale, and the Zung self-rating depression scale. The findings revealed that 23.27% of IMSs reported anxiety, while 48.52% experienced symptoms of depression. Multivariate logistic regression analysis identified poor health conditions and limited access to the family as independent risk factors for anxiety. At the same time, depression was associated with both compromised health and notable financial burdens. This study provides crucial insights for policymakers, college administrators, and government authorities, urging proactive measures to support and manage the wellbeing of IMSs during pandemic situations.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "32b163539969b47e16dddbc6591d31cd",
  "timestamp": "2025-05-15T02:53:33.440470"
}