{
  "id": 356,
  "title": "宏观审慎监管口头沟通与系统性风险",
  "abstract": "良好的宏观审慎监管口头沟通有助于引导市场主体预期,防范系统性金融风险。在中国不断加强宏观审慎监管口头沟通的背景下,文章对2009年4月至2019年12月的宏观审慎监管口头沟通内容进行语料处理,生成计算词典,计算每次口头沟通事件态度的得分,进而构建宏观审慎监管口头沟通指数,并分析口头沟通指数与金融机构关联度指数的关系。研究表明:监管当局会针对系统性金融风险的变化进行口头沟通,银行、证券、保险和信托四部门关联度指数的变化均会影响宏观审慎监管口头沟通;宏观审慎监管口头沟通可以降低系统性金融风险,但发挥作用的时滞为半年,其主要影响银行业被传染指数和保险业传染指数,并且偶尔会出现沟通失灵。进一步地,本文基于监督学习方法,通过训练子样本词典得到具有倾向的短语及其概率分布,利用文本分类器对新的沟通文本进行自动分类,并计算新样本的指数,结果表明不同样本的指数结果较为一致,说明本文的宏观审慎监管口头沟通量化方法具有可复制性和可延展性。",
  "year": 2021,
  "source": "CNKI",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e20a2a5128ba75f403bdb5c9a7507462",
  "timestamp": "2025-05-14T22:29:34.792383"
}