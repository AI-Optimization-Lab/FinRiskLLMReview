{
  "id": 978,
  "title": "Banks' stock price crash risk prediction with textual analysis: a machine learning approach",
  "abstract": "This study develops models that predict banks' stock price crash risk using novel machine learning techniques. A key element of our approach is that we retrieve textual information from ECB presidents' speeches. To this end, we employ quarter-bank level data and various measures for stock price crash risk, ensuring the robustness of our findings. First, we find that the machine learning models can generally perform better than the simple regressions. Next, our results also suggest that textual information from the ECB president's speeches has significant predictive power. Finally, when we jointly use textual information and macro-financial variables as inputs, the performance of our models is substantially increased compared to models using a single type of input. Our empirical findings provide significant policy implications for investors and policymakers as they can help regulators assess the financial system's stability and identify any potential systemic risks, allowing them to take proactive measures to prevent or mitigate a financial crisis.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e5a8cfb2e4a8df27bd26ea900d51e7df",
  "timestamp": "2025-05-15T01:57:29.842021"
}