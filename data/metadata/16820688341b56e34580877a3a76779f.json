{
  "id": 3421,
  "title": "Optimizing Stock Portfolio Performance with a Combined RG1-TOPSIS Model: Insights from the Chinese Market",
  "abstract": "Quantitative investment has gained popularity in the global financial market, and China is no exception. However, existing quantitative stock selection methods have several limitations that restrict their effectiveness. This study proposes a novel approach called the regression corrected G1-TOPSIS (RG1-TOPSIS) method to address these shortcomings. This method combines the G1 and regression methods to objectively assign weights to each factor in multi-factor stock selection. Subsequently, a scoring method is employed to comprehensively rank stocks based on the derived weighting results. Lastly, utilizing the closeness metric, the G1 algorithm is applied to optimize fund allocation within the investment portfolio. We empirically apply this proposed method to stocks in the A-share market of the Shanghai Stock Exchange, covering the period from July 1, 2012, to June 30, 2022. We then compare the obtained results with the market index. Our back-testing results demonstrate that the rate of return (ROR) achieved by our stock selection model significantly surpasses that of the market index. Furthermore, our fund allocation method proves to be more suitable in bear markets. The comprehensive set of investment models we present showcases a high investment value, thereby establishing the effectiveness of this method in the Chinese stock market.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "16820688341b56e34580877a3a76779f",
  "timestamp": "2025-05-15T01:16:55.538805"
}