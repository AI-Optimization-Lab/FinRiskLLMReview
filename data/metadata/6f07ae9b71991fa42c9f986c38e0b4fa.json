{
  "id": 1540,
  "title": "Mining Financial Risk Events from News and Assessing their Impact on Stocks",
  "abstract": "The impact of financial risk events on stock market is a fairly established area of research in the financial domain. However, the analysts require these events to be represented in a structured form in order to carry out statistical analysis. In this work, we aim is to identify and extract various financial risk events from news articles along with associated organizations to facilitate integrated analysis with structured business data. We propose a two-phase risk extraction algorithm involving a CNN based semi-supervised risk event identification and gradient boosting based entity association algorithm to extract risk events from news and associate them to their target organizations. We have analyzed large volumes of past available data using Granger causality to assess the impact of these events on various stock indices. Further, the utility of extracted risk events in predicting stock movement has been shown using a Bi-LSTM network based prediction model. The proposed system outperforms state of the art linear SVM on data for different stock indices.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6f07ae9b71991fa42c9f986c38e0b4fa",
  "timestamp": "2025-05-15T02:04:20.098257"
}