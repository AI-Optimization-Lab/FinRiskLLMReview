{
  "id": 400,
  "title": "Linearly Autonomous Symmetries of a Fractional Gueant-Pu Model",
  "abstract": "We study the group structure of the Gueant-Pu equation of the fractional order with respect to the price of the underlying asset variable. It is one of the models of the dynamics of options pricing, taking into account transaction costs. The search for continuous groups of linearly autonomous equivalence transformations is carried out. The equivalence transformations found are used in constructing a group classification (within the framework of linearly autonomous transformations) of the equation under consideration with a nonlinear function in the right side of the equation as a free element. In the case of a nonzero risk-free rate, it is shown that two cases of Lie algebras of the equation under study are possible: two-dimensional in the case of a special type of free element and one-dimensional in the remaining cases. If the risk-free rate is zero, there are four variants of the Lie algebra, which can be two-, three-, or four-dimensional. In the future, we assume to use the obtained group classification in calculating invariant solutions and conservation laws of the model under study.",
  "year": 2023,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "234af65e19639bafd91d022fe37f274a",
  "timestamp": "2025-05-15T01:31:50.626717"
}