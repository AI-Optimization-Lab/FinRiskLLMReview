{
  "id": 6144,
  "title": "Does Green Credit Policy Work in China? The Correlation between Green Credit and Corporate Environmental Information Disclosure Quality",
  "abstract": "Roughly a decade ago, the Chinese government implemented a green credit policy aimed at lowering emissions from highly polluting corporations through improving information disclosure quality during the loan process. According to policy guidelines, banks may provide financial support only for new projects that passed an environmental assessment or were explicitly designed to decrease pollution. This paper used panel data from 320 companies in heavy polluting industries listed on the Shanghai Stock Exchange from 2008 to 2016 and adopted a fixed effects regression model to examine whether collusion between local governments and Chinese listed companies has prevented the green credit policy from achieving its target. The results show that there is no significant positive correlation between CEID and corporate green financing, which means that the environmental information disclosure system does not send valuable signals to the market and has failed to become a decision-making tool for bank-risk management.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7e6f3872c9354003fa0ec0c774f7bf41",
  "timestamp": "2025-05-15T02:55:12.928757"
}