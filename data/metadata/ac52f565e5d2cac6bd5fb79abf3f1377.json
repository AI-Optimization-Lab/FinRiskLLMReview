{
  "id": 34,
  "title": "MA-FDRNN: Multi-Asset Fuzzy Deep Recurrent Neural Network Reinforcement Learning for Portfolio Management",
  "abstract": "Reinforcement learning (RL) in the context of portfolio optimisation (PO) aims to generate profits beyond the abilities of human traders. However, there is no definitive framework sufficiently able to generate consistent profits in line with expectations on real-world stock exchanges. Previous research has demonstrated the ability of Deep Deterministic Policy Gradient (DDPG), Proximal Policy Optimisation (PPO), Ensemble of Identical Independent Evaluators (EIIE) and Fuzzy Deep Recurrent Neural Network (FDRNN) methodologies to generate positive results within controlled testing environments. This paper consolidates recent progress by evaluating these RL methodologies applied to five different stock exchanges. The selected methodologies are tested on real-world data within a simulated trading framework to assess their performance under different market conditions. To this end, we extend the FDRNN method to allow it to interact with a multi-asset market. As a result, in contrast to previous methods, our new Multi-Asset FDRNN (MA-FDRNN) can take short positions, which we hypothesised would give it an advantage over DDPG, PPO and EIIE. To assess the performance of the RL methods, we compare their results to a selection of benchmark PO algorithms. The results show the superiority of the MA-FDRNN method within the sideways and bear market, where it can exploit short positions and earn higher returns at lower risk than the other methods. In the bull markets, the benchmark algorithms outperform the RL agents, and in the crashed market, the RL methods and benchmark algorithms demonstrate similar performance. The results indicate that while RL methods do show some promise in achieving the goals of portfolio management, further research is needed to create an RL framework capable of succeeding in real-world market conditions.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ac52f565e5d2cac6bd5fb79abf3f1377",
  "timestamp": "2025-05-15T00:38:16.307197"
}