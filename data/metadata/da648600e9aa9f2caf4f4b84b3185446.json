{
  "id": 919,
  "title": "An in-depth investigation of genetic programming and nine other machine learning algorithms in a financial forecasting problem",
  "abstract": "Machine learning (ML) techniques have shown to be useful in the field of financial forecasting. In particular, genetic programming has been a popular ML algorithm with proven success in improving financial forecasting. Meanwhile, the performance of such ML algorithms depends on a number of factors including data analysis from different markets, data periods, forecasting days ahead, and the transaction cost which have been neglected in most previous studies. Therefore, the focus of this paper is on investigating the effect of such factors. We perform an extensive evaluation of a financial genetic programming-based approach and compare its performance against 9 popular machine learning algorithms and the buy and hold trading strategy. Experiments take place over daily data from 220 datasets from 10 international markets. Results show that genetic programming not only provides profitable results but also outperforms the 9 machine learning algorithms in terms of risk and Sharpe ratio.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "da648600e9aa9f2caf4f4b84b3185446",
  "timestamp": "2025-05-15T01:56:49.544990"
}