{
  "id": 3325,
  "title": "Interaction of size, book-to-market and momentum effects in Australia",
  "abstract": "This study seeks to disentangle the effects of size, book-to-market and momentum on returns. Initial results show that each characteristic has a role in explaining returns, but that there is interaction between size and momentum, as well as between size and book-to-market. Three key findings emerge. First, the size premium is the strongest, particularly in the loser portfolios. Second, the value premium is generally limited to the smallest portfolios. Third, the momentum premium is evident for the large- and middle-sized portfolios, but loser stocks significantly outperform winner stocks in the smallest size portfolio. When these interactions are controlled with multivariate regression, we find a significant negative average relation between size and returns, a significant positive average relation between book-to-market and returns, and a significant positive average relation between momentum and returns.",
  "year": 2010,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "37661bf8d866d533ab91abf2126279d2",
  "timestamp": "2025-05-15T01:15:54.809007"
}