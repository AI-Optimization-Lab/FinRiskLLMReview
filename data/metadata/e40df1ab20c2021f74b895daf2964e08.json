{
  "id": 3132,
  "title": "Efficiency and risk in Japanese banking",
  "abstract": "This paper investigates the impact of risk and quality factors on banks' cost by using the stochastic cost frontier methodology to evaluate scale and X-inefficiencies, as well as technical change for a sample of Japanese commercial banks between 1993 and 1996, Loan-loss provisions are included in the cost frontier model to control for output quality, with a financial capital and a liquidity ratio included to control risk. Following the approach suggested in Mester (1996) we show that if risk and quality factors are not taken into account optimal bank size tends to be overstated. That is, optimal bank size is considerably smaller when risk and quality factors are taken into account when modelling the cost characteristics of Japanese banks. We also find that the level of financial capital has the biggest influence on the scale efficiency estimates. X-inefficiency estimates, in contrast, appear less sensitive to risk and quality factors. Our results also suggest that scale inefficiencies dominate X-inefficiencies. These are important findings because they contrast with the results of previous studies on Japanese banking. In particular, the results indicate an alternative policy prescription, namely, that the largest banks should shrink to benefit from scale advantages. It also seems that financial capital has the largest influence on optimal bank size. (C) 2000 Elsevier Science B.V. All rights reserved. JEL classification: G21; D21; G23.",
  "year": 2000,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e40df1ab20c2021f74b895daf2964e08",
  "timestamp": "2025-05-15T02:22:56.707517"
}