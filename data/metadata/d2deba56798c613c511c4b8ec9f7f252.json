{
  "id": 886,
  "title": "Hashing for Financial Credit Risk Analysis",
  "abstract": "Hashing techniques have recently become the trend for accessing complex content over large data sets. With the overwhelming financial data produced today, binary embeddings are efficient tools of indexing big datasets for financial credit risk analysis. The rationale is to find a good hash function such that similar data points in Euclidean space preserve their similarities in the Hamming space for fast data retrieval. In this paper, first we use a semi-supervised hashing method to take into account the pairwise supervised information for constructing the weight adjacency graph matrix needed to learn the binarised Laplacian EigenMap. Second, we train a generalised regression neural network (GRNN) to learn the k-bits hash code. Third, the k-bits code for the test data is efficiently found in the recall phase. The results of hashing financial data show the applicability and advantages of the approach to credit risk assessment.",
  "year": 2014,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d2deba56798c613c511c4b8ec9f7f252",
  "timestamp": "2025-05-15T01:56:17.538923"
}