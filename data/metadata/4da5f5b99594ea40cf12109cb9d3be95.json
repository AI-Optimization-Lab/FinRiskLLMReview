{
  "id": 3706,
  "title": "LONG MEMORY IN THE VOLATILITY OF AN EMERGING FIXED-INCOME MARKET: EVIDENCE FROM SOUTH AFRICA",
  "abstract": "This paper tests for long memory in volatility of fixed-income returns; specifically, South Africa's local currency 10-year government bond, given that the characterisation of stochastic long-memory volatility is of interest and importance in portfolio and risk management. The long-memory parameter is estimated using methods based on wavelets, which have gained prominence in recent years. Evidence of long memory in fixed-income return volatility is conclusively demonstrated across a variety of volatility measures and wavelet forms. This finding suggests a pattern of time dependence, which may potentially be exploited to generate improved volatility forecasting performance especially over long horizons. This paper further extends the extant literature by comparing the predictive power of long-memory forecasts with those obtained from a standard (short-memory) generalised autoregressive conditional heteroskedasticity (GARCH) process. The results of this exercise suggest that the information content of long-memory models does not lead to improved forecast accuracy. The GARCH(1,1) model is shown to provide the best forecasts across most horizons (i.e. daily, weekly and monthly). Forecast performance is further revealed to be sensitive to the choice of volatility proxy used. Finally, the derived volatility forecasts are generally very close, and in some cases, almost indistinguishable. JEL Classification: C22, G12",
  "year": 2011,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4da5f5b99594ea40cf12109cb9d3be95",
  "timestamp": "2025-05-15T01:20:03.418787"
}