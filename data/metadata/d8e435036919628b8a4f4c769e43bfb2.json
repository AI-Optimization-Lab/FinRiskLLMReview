{
  "id": 2892,
  "title": "The Slow Convergence of Ordinary Least Squares Estimators of α, β and Portfolio Weights under Long-Memory Stochastic Volatility",
  "abstract": "We consider inference for the market model coefficients based on simple linear regression under a long memory stochastic volatility generating mechanism for the returns. We obtain limit theorems for the ordinary least squares (OLS) estimators of alpha and beta in this framework. These theorems imply that the convergence rate of the OLS estimators is typically slower than T if both the regressor and the predictor have long memory in volatility, where T is the sample size. The traditional standard errors of the OLS-estimated intercept (alpha(boolean AND)) and slope (beta(boolean AND)), which disregard long memory in volatility, are typically too optimistic, and therefore the traditional t-statistic for testing, say, alpha = 0 or beta = 1, will diverge under the null hypothesis. We also obtain limit theorems (which imply slow convergence) for the estimated weights of the minimum variance portfolio and the optimal portfolio in the same framework. In addition, we propose and study the performance of a subsampling-based approach to hypothesis testing for alpha and beta. We conclude by noting that analogous results hold under more general conditions on long-memory volatility models and state these general conditions which cover certain fractionally integrated exponential generalized autoregressive conditional heteroskedasticity (EGARCH) models.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d8e435036919628b8a4f4c769e43bfb2",
  "timestamp": "2025-05-15T01:11:48.299131"
}