{
  "id": 1706,
  "title": "Attention-based fuzzy neural networks designed for early warning of financial crises of listed companies",
  "abstract": "Developing an early warning model for company financial crises holds critical significance in robust risk management and ensuring the enduring stability of the capital market. Although the existing research has achieved rich results, the disadvantages of insufficient text information mining and poor model performance still exist. To alleviate the problem of insufficient text information mining, we collect related financial and annual report data from 820 listed companies in mainland China from 2018 to 2023 by using sophisticated web crawlers and advanced text sentiment analysis technologies and using missing value interpolation, standardization, and data balancing to build multi-source datasets of companies. Ranking the feature importance of multi-source data promotes understanding the formation of financial crises for companies. In the meantime, a novel Attention-based Fuzzy Neural Network (AFNN) was proposed to parse multi-source data to forecast financial crises among listed companies. Experimental results indicate that AFNN exhibits significantly improved performance compared to other advanced methods.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "21d1619da4e676f7293f4b02a0babb90",
  "timestamp": "2025-05-15T02:06:22.516391"
}