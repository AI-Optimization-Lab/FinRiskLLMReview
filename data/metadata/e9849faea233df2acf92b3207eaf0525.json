{
  "id": 2153,
  "title": "A Dataset for the Vietnamese Banking System (2002-2021)",
  "abstract": "This data article describes a dataset that consists of key statistics on the activities of 45 Vietnamese banks (e.g., deposits, loans, assets, and labor productivity), operated during the 2002-2021 period, yielding a total of 644 bank-year observations. This is the first systematic compilation of data on the splits of state vs. private ownership, foreign vs. domestic banks, commercial vs. policy banks, and listed vs. nonlisted banks. Consequently, this arrives at a unique set of variables and indicators that allow us to capture the development and performance of the Vietnamese banking sector over time along many different dimensions. This can play an important role for financial analysts, researchers, and educators in banking efficiency and performance, risk and profit/revenue management, machine learning, and other fields.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e9849faea233df2acf92b3207eaf0525",
  "timestamp": "2025-05-15T02:11:40.631159"
}