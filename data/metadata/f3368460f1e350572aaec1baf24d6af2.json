{
  "id": 3583,
  "title": "Neural Networks for Portfolio Analysis With Cardinality Constraints",
  "abstract": "Portfolio analysis is a crucial subject within modern finance. However, the classical Markowitz model, which was awarded the Nobel Prize in Economics in 1991, faces new challenges in contemporary financial environments. Specifically, it fails to consider transaction costs and cardinality constraints, which have become increasingly critical factors, particularly in the era of high-frequency trading. To address these limitations, this research is motivated by the successful application of machine learning tools in various engineering disciplines. In this work, three novel dynamic neural networks are proposed to tackle nonconvex portfolio optimization under the presence of transaction costs and cardinality constraints. The neural dynamics are intentionally designed to exploit the structural characteristics of the problem, and the proposed models are rigorously proven to achieve global convergence. To validate their effectiveness, experimental analysis is conducted using real stock market data of companies listed in the Dow Jones Index (DJI), covering the period from November 8, 2021 to November 8, 2022, encompassing an entire year. The results demonstrate the efficacy of the proposed methods. Notably, the proposed model achieves a substantial reduction in costs (which combines investment risk and reward) by as much as 56.71% compared with portfolios that are averagely selected.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f3368460f1e350572aaec1baf24d6af2",
  "timestamp": "2025-05-15T02:27:54.041023"
}