{
  "id": 60,
  "title": "Reinforcement learning about asset variability and correlation in repeated portfolio decisions",
  "abstract": "Lay investors construct portfolios that are often not efficient and fail to take the correlation of assets into account. The present work examines whether providing people with a learning opportunity makes them sensitive to the correlation between assets. In two studies, where participants repeatedly allocated their endowment to three assets with feedback, participants changed their portfolio over time dependent on the asset correlation. To model learning about relevant characteristics of a portfolio, we developed reinforcement learning models that take learning about asset variability and correlation into account. We demonstrated via out-of-sample predictions that these models explain portfolio allocations better than basic reinforcement learning models, a static 1/N diversification strategy, and the mean-variance model using sample means, variances, and correlation. Hence, experiencing returns can help investors take asset correlations into account. The principles of reinforcement learning are a cognitively plausible and descriptively valid framework for understanding repeated portfolio allocations. (C) 2021 The Author(s). Published by Elsevier B.V.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ced3b522ba0239ba7abde1acc5a573dc",
  "timestamp": "2025-05-15T00:38:56.968407"
}