{
  "id": 3587,
  "title": "EBA's Capital Exercise and Technical Efficiency of the Banks",
  "abstract": "This study uses a sample of 194 banks from 15 EU countries and two-stage data envelopment analysis (DEA) to provide evidence on the impact of the European Banking Authority (EBA)'s capital exercise on banks' efficiency. In the first stage of the analysis, we measure the efficiency by employing DEA. We then use Tobit regression to investigate the impact of the capital exercise on banks' technical efficiency. We estimate several specifications while controlling for bank-specific attributes and country-level characteristics accounting for macroeconomic conditions, financial development and market structure. The results indicate that EBA's capital exercise came, as a shock for the banks would be contributing towards making the banks more stable. It would be preventing banks from excessive risk-taking activities. Furthermore, it would be allowing the banks to withstand the financial distress and contributing in banks becoming less prone to the systemic risk. The study finds that the capital requirements would be creating favourable economic conditions, which would be, affect the extent, depth and quality of financial intermediation and banking services.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "027950d0fc0e8048808d97fb53ec8491",
  "timestamp": "2025-05-15T02:27:54.063303"
}