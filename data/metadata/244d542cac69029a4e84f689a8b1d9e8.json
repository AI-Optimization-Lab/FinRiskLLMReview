{
  "id": 1914,
  "title": "Network Analysis of Multivariate Transfer Entropy of Cryptocurrencies in Times of Turbulence",
  "abstract": "We investigate the effects of the recent financial turbulence of 2020 on the market of cryptocurrencies taking into account the hourly price and volume of transactions from December 2019 to April 2020. The data were subdivided into time frames and analyzed the directed network generated by the estimation of the multivariate transfer entropy. The approach followed here is based on a greedy algorithm and multiple hypothesis testing. Then, we explored the clustering coefficient and the degree distributions of nodes for each subperiod. It is found the clustering coefficient increases dramatically in March and coincides with the most severe fall of the recent worldwide stock markets crash. Further, the log-likelihood in all cases bent over a power law distribution, with a higher estimated power during the period of major financial contraction. Our results suggest the financial turbulence induce a higher flow of information on the cryptocurrency market in the sense of a higher clustering coefficient and complexity of the network. Hence, the complex properties of the multivariate transfer entropy network may provide early warning signals of increasing systematic risk in turbulence times of the cryptocurrency markets.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "244d542cac69029a4e84f689a8b1d9e8",
  "timestamp": "2025-05-15T02:09:14.276751"
}