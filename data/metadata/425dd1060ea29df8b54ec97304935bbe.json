{
  "id": 3082,
  "title": "Along short-term memory enhanced realized conditional heteroskedasticity model",
  "abstract": "This paper examines the potential of using realized volatility measures for capturing financial markets' uncertainty. Earlier studies show the usefulness of the high-frequency data based Generalized AutoRegressive Conditional Heteroskedasticity (RealGARCH) model for enhancing volatility forecasting accuracy; however, this model focuses only on linear and short-term dependencies of realized volatility measures on the underlying volatility. Recognizing the critical economic implications of this limitation, the long short-term memory neural network is integrated into RealGARCH, aiming to explore the full impact of realized volatility on volatility modeling and forecasting via capturing the nonlinear and long-term effects. A comprehensive empirical study using 31 indices from 2004 to 2021 is conducted. The results demonstrate that our proposed framework achieves superior in-sample and out-of-sample performance compared to several benchmark models. Importantly, it retains interpretability and effectively adapts to the stylized facts observed in volatility, emphasizing its significant potential for enhancing economic decision-making and risk management.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "425dd1060ea29df8b54ec97304935bbe",
  "timestamp": "2025-05-15T02:22:25.098649"
}