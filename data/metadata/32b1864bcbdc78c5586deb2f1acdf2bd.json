{
  "id": 344,
  "title": "A Machine Learning Portfolio Allocation System for IPOs in Korean Markets Using GA-Rough Set Theory",
  "abstract": "An initial public offering (IPO) is a type of public offering in which a company's shares are sold to institutional and individual investors. While the majority of studies on IPOs have focused on the efficiency of raising capital and price adequacy in IPOs, studies on portfolio allocation strategies for IPO stocks are relatively scarce. This paper develops a machine learning investment strategy for IPO stocks based on rough set theory and a genetic algorithm (GA-rough set theory). To reduce issues of information asymmetry, we use nonfinancial data that are publicly available to individual and institutional investors in the IPO process. Based on the rule sets generated from the training sets, we conduct 120 tests with various conditions involving the target days and the partition of the training and testing sets, and we find excess returns of the constructed portfolios compared to the benchmark portfolios. Investors in IPO stocks can formulate more efficient investment strategies using our system. In this sense, the system developed in this paper contributes to the efficiency of financial markets and helps achieve sustained economic growth.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "32b1864bcbdc78c5586deb2f1acdf2bd",
  "timestamp": "2025-05-15T00:42:32.448286"
}