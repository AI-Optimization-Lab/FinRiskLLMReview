{
  "id": 512,
  "title": "Portfolio optimization for American options",
  "abstract": "American options allow early exercise, which yields an additional challenge - besides the weights of each option - when optimizing a portfolio of American options. In this work, we construct strategies for an American option portfolio by exercising options at optimal timings with optimal weights determined concurrently. To model such portfolios, a reinforcement learning (Q-learning) algorithm is proposed, combining an iterative progressive hedging method and a quadratic approximation to Q-values by regression. By means of Monte Carlo simulation and empirical experiments, using data from the SPY options market, we evaluate the quality of our algorithms and examine their performance under various investment assumptions, such as different portfolio settings and distributions of the underlying asset returns. With discretized timings, our strategies work better in a relatively long time horizon and when the portfolio is hedged using the underlying instrument. Due to the highly leveraged and risky nature of our strategies, overly risk-averse investors are proved unsuitable for such investment opportunities.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f209aa6c71d7ee050caa8701f1f37f4e",
  "timestamp": "2025-05-15T00:44:20.992840"
}