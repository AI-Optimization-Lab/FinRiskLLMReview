{
  "id": 1428,
  "title": "End-to-end risk budgeting portfolio optimization with neural networks",
  "abstract": "Traditional stochastic optimization in financial operations research applications consist of a two-step process: (1) calibrate parameters of the assumed stochastic processes by minimizing a loss function, and (2) optimize a decision vector by reference to the investor's reward/risk measures. Yet this approach can encounter the error maximization problem. We combine the steps in a single unified feedforward network, called end-to-end. Two variants are examined: a model-free neural network, and a model-based network in which a risk budgeting model is embedded as an implicit layer in a deep neural network. We performed computational experiments with major ETF indices and found that the model-based approach leads to robust performance out-of-sample (2017-2021) when maximizing the Sharpe ratio as the training objective, achieving Sharpe ratio of 1.16 versus 0.83 for a pure risk budgeting model. Simulation studies show statistically significant difference between model-based and model-free approaches as well. We extend the end-to-end algorithm by filtering assets with low volatility and low returns, boosting the out-of-sample Sharpe ratio to 1.24. The end-to-end approach can be readily applied to a wide variety of financial and other optimization problems.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "729ee5ed83f2f07a65433acdbee89e18",
  "timestamp": "2025-05-15T02:03:00.651892"
}