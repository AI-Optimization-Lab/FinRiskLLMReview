{
  "id": 4010,
  "title": "Mixture Cure Models in Prediction of Time to Default: Comparison with Logit and Cox Models",
  "abstract": "Mixture cure models are an extension of the standard survival models used for predicting survivors in the case of two distinct subpopulations [Sy and Taylor (Biometrics 56: 227-236, 2000)]. The models assume that the studied population is a mixture of susceptible individuals, who may experience the event of interest, and non-susceptible individuals, who will never experience it [Corbiere and Joly (Comput Methods Prog Biomed 85(2): 173-180, 2007)]. Mixture cure models were used for the first time in medical statistics to model long-term survival of cancer patients. Tong et al. (Eur J Oper Res 218(1): 132-139, 2012) introduced mixture cure models to the area of credit scoring, where a large proportion of the accounts do not experience default during the loan term. In this paper, we investigate the use of a mixture cure model for a sample of 5000 consumer credit accounts from a 60-month personal loans portfolio of a Polish financial institution. All loans have been observed for 24 months. Default is the event of interest, whereas earlier repayment is considered to be censoring. We develop and compare default prediction models using the logistic regression, Cox model and mixture cure approaches. Similarities with and differences to the study results obtained by Tong et al. (Eur J Oper Res 218(1): 132-139, 2012) are scrutinised. The final discussion focuses on the usefulness of mixture cure models in predicting the probability of default, and the limitations of these models.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "58593a68efc06d43ea8ef8eded5823d2",
  "timestamp": "2025-05-15T01:23:07.322245"
}