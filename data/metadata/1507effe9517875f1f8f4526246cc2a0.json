{
  "id": 341,
  "title": "Fuzzy neural network with backpropagation for fuzzy quadratic programming problems and portfolio optimization problems",
  "abstract": "The study aspires to adopt back propagation fuzzy neural networks to solve fuzzy quadratic programming problems. The main motivation behind proposing a back propagation neural network is that it can easily adjust and fine-tune the weights of the network from the error rate obtained at the previous layer. The error rate customarily called the loss function is the dissimilarity between the desired and predicted outputs. The chain and power rules of the derivative allow back propagation and successively update the weights of the network to perform efficiently. Thus, the gradient of the loss function is calculated by iterating backward layer by layer but one at a time to reduce the difference between the desired and the predicted outputs. The research flow is such that first of all the quadratic programming problem is formulated in a fuzzy environment. The problem with fuzzy quadratic programming is formulated as a lower, central, and upper model. The formulated models are then solved with backpropagation fuzzy neural networks. The proposed method is then implemented in the capital market to identify the optimal portfolio for potential investors in the Pakistan Stock Exchange. Six leading stocks traded on the stock exchange from Jan 2016 to Oct 2020 were taken into consideration. At all three levels (lower, central, and upper), the results of identifying the best investment portfolio for investors are consistent. The proposed three models identify the investors to invest in ATHL, MCB and ARPL, whereas, the remaining three IGIHL, INIL and POL are not desirable for investment. In all three cases, the convergence is obtained at 475 iterations which is faster than the previously conducted studies. Moreover, another advantage of the proposed technique is that it brings an improvement of 28.77% in the objective function of mean variance optimization MVO model.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1507effe9517875f1f8f4526246cc2a0",
  "timestamp": "2025-05-15T00:42:32.428447"
}