{
  "id": 8322,
  "title": "Streamlining Classical RCM Using a Digitized Model-based Approach",
  "abstract": "The classical RCM methodology is heralded for its rigor, though it can have significant barriers to entry (cost and time) before the organization can reap the benefits of an optimized maintenance approach. As a means of achieving similar operational outcomes to classical RCM (CRCM) while avoiding the initial overhead, the idea of a Streamlined RCM (SRCM) process has gained traction across several industries. Although there is no standardized definition of the SRCM procedure, variations follow the general steps of classical RCM but trade off rigor (in the analysis or classification steps) for schedule and cost savings. While the necessity for employing an SRCM approach may be understandable from a financial standpoint, the potential efficiency dividend of a model-based RCM requires investigation. The concept of the Digital Thread to integrate all engineering data and analyses is now widely accepted as the most effective method to address challenges in the design process - particularly increased system and logistical complexity. Integrated Digital Risk Twins (DRT) that can simulate performance of a system during operations are increasingly relied upon to support analysis of operational performance and maintenance. This paper describes how a DRT can generate the requisite RAMS data to digitize the classical RCM analysis process so that it can be performed with greater efficiency and consistency. The potential efficiency gains of a digitized, model- based classical RCM workflow provides the benefits of a SRCM without compromising analytical diligence. Adoption of a digitized, model-based RCM tool allows for the flexibility to apply aspects of both SRCM and CRCM to meet the project objectives pursuant to modern operational requirements, failure management policies and compliance, with a much smaller barrier of entry.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cde6230c1609b8cdff001eb2462f6771",
  "timestamp": "2025-05-15T03:17:49.080978"
}