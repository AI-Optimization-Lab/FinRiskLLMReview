{
  "id": 3842,
  "title": "Network Analysis of SIFIs Based on Tail Systemic Linkage",
  "abstract": "This article applies several distinct methods including the systemic linkage method and network analysis to address intranational systemic risk interdependencies. Specifically, we initially quantify dynamic systemic linkages among US and Chinese systemically important financial institutions through time-varying adjacency matrices related to an extreme value theory (EVT) approach and then visualize them using network analysis. Numerical and graphical results show that intranational systemic linkages are obviously enhancive under extreme scenarios such as large negative shocks in the financial system. In addition, we apply a tail event-driven network quantile regression (TENQR) model to address the interdependence and dynamics of the entire network. The estimation results show that the network factors respond more strongly when the market suffers extreme stress.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "41113c75cfa5e061cda1955292d62c7d",
  "timestamp": "2025-05-15T02:30:28.738913"
}