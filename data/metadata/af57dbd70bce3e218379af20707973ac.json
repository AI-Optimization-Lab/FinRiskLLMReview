{
  "id": 6752,
  "title": "Victoria symptom validity test performance in a heterogenous clinical sample",
  "abstract": "We retrospectively reviewed Victoria Symptom Validity Test (VSVT) in 374 patients who underwent neuropsychological assessment in an academic hospital-based practice. Patients were classified as either non-TBI clinically referred (generally patients referred from neurology, neurosurgery, or medicine), clinically referred TBI (no known external financial incentive), and non-clinical referrals (e.g., attorney-referred, Worker's Compensation). Three patients were not classified into any group and considered separately. Intentional response distortion, defined as statistically less than chance performance on hard VST items, was present in only 1/306 (0.3%) clinically referred non-TBI patients, and no clinically referred TBI patient obtained scores significantly less than chance on this measure. One additional clinically referred patient with a non-neurologic diagnosis who was subsequently found to be pursuing a disability claim also performed worse than chance. In contrast, 5=25 patients (20%) referred by attorneys or otherwise deemed a priori to be at-risk for deficit exaggeration performed less than chance. These data suggest that intentional response distortion in patients referred for non-forensic neuropsychological evaluation is rare. Performances by specific diagnosis using different classification criteria are also presented.",
  "year": 2007,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "af57dbd70bce3e218379af20707973ac",
  "timestamp": "2025-05-15T03:01:03.638359"
}