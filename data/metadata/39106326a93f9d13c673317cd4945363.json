{
  "id": 2931,
  "title": "Accrual mispricing, value-at-risk, and expected stock returns",
  "abstract": "We investigate the extent to which a parsimonious measure of maximum likely loss that captures the tail risk of returns-known as value-at-risk (VaR)-explains the relationship between accruals and the cross-sectional dispersion of expected stock returns. We construct portfolios based on Sloan's (Account Rev 71(3):289-315, 1996) total accruals (TA) measure and individual asset-level VaR, which reflects the dynamic behavior of the asset distribution. We document that VaR is in congruence with portfolio-level accruals and that there is a significant positive relationship between VaR and the cross-section of portfolio returns. Allowing a double-sort involving VaR and TA further suggests that the spread between low- and high-TA portfolios is significantly attenuated after controlling for VaR. We also conduct a firm-level cross-sectional regression analysis and demonstrate that the TA- and VaR-based characteristics-but not the factor-mimicking portfolios-are compensated with higher expected returns, and that VaR neither subsumes nor is subsumed by TA. Finally, our cross-sectional decomposition analysis suggests that the firm-level VaR captures at least 7% of the accrual premium even in the presence of size and book-to-market. These findings lend support for the mispricing explanation of the accrual anomaly.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "39106326a93f9d13c673317cd4945363",
  "timestamp": "2025-05-15T01:11:48.480237"
}