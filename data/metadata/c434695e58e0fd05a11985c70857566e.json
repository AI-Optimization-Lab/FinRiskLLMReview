{
  "id": 57,
  "title": "基于高斯混合模型的分布因子聚类方法",
  "abstract": "随着信息技术的发展，人类社会产生的数据规模越来越庞大、形式越来越复杂，对聚类分析形成了巨大挑战。在越来越多的应用场景中，观测数据具有相互关联、层次嵌套的结构，使传统聚类方法难以直接适用。通常的解决方案是采用特征工程方法将观测信息压缩为低维特征向量进行聚类，但这将带来不可避免的信息损失。为充分利用观测数据，本文以分布函数表示聚类对象，大幅降低信息损失，进而提出基于高斯混合模型的分布因子模型。该模型将聚类对象的观测数据分解为两部分，一是以高斯成分表示的公共因子，反映数据中具有共性的典型模式；二是载荷矩阵，矩阵中每个载荷向量反映个体的异质性特征。估计得到载荷向量后即可对不同个体实现聚类划分。本文提出的方法具有优良的统计学效率，能够证明在一定假设条件下聚类误差率能够随着观测个体数目的发散而趋近于0。基于模拟数据和股票收益、大气污染实际数据的实验表明，该方法能够区分具有不同特征模式的个体，解决多维数据的分布函数聚类问题，并为金融风险管理、空气质量的差异化治理等现实问题提供决策支持。",
  "year": 2024,
  "source": "CNKI",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c434695e58e0fd05a11985c70857566e",
  "timestamp": "2025-05-14T22:26:31.840884"
}