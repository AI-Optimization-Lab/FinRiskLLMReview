{
  "id": 2055,
  "title": "Relationship between leverage and the bargaining power of labor unions: evidence from theoretical and empirical perspectives",
  "abstract": "This study examines whether a firm's leverage can be used strategically to improve its bargaining position with an organized labor union using samples of non-financial firms listed on the Korean Stock Exchange (KSE) from 1999 to 2013. Through empirical testing, we find that the portfolio with the lowest union labor coverage has the lowest leverage, while the portfolio with the highest union labor coverage has the highest leverage. We also find that collective bargaining power positively affects leverage through the regression of leverage on the bargaining power of the labor union, regardless of the analysis methods, such as static and dynamic models. With a robustness test model that used the industry adjusted labor union concentration index (IUCI), we obtain evidence that collective bargaining power positively influences leverage, which corresponds with the regression results. In conclusion, we suggest the existence of evidence demonstrating that variables related to labor unions affect leverage levels, as suggested in previous studies.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6e01d364e197aa1b90a0f0815db1f9dd",
  "timestamp": "2025-05-15T01:02:32.832391"
}