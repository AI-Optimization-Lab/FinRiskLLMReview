{
  "id": 1995,
  "title": "RETRACTED: LSTM-Based Deep Model for Investment Portfolio Assessment and Analysis (Retracted Article)",
  "abstract": "In recent years, within the scope of financial quantification, quantitative investment models that support human-oriented algorithms have been proposed. These models attempt to characterize fiat-delayed series through intelligent acquaintance methods to predict data and arrange investment strategies. The standard long short-term memory (LSTM) neural network has the shortcoming of low effectiveness of the fiscal cycle sequence. This work utters throughout the amended LSTM design. The augury result of the neural reticulation was upgraded by coalesce attentional propose to the LSTM class, and a genetic algorithmic program product was formulated. Genetic algorithm (GA) updates the inalienable parameters to a higher generalization aptitude. Using man stock insignitor future data from January 2019 to May 2020, we accomplish a station-of-the-contrivance algorithmic rule. Inferences have shown that the improved LSTM example proposed in this paper outperforms other designs in multiple respect, and it performs effectively in investment portfolio design, which is suitable for future investment.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "390a9c27c61633a36a56f7d3e196040c",
  "timestamp": "2025-05-15T01:02:10.093708"
}