{
  "id": 1139,
  "title": "Anomaly Detection: Case of Mobile Money Transactions with Isolation Forrest",
  "abstract": "Generally, the risk of fraud involves any intentional deception made for financial gain. Managing the risks associated with mobile money is a difficult task, particularly with regard to the risk of fraud. Fraud not only results in financial loss for the customer, it also damages the reputation of the service and endangers the reputation of the industry as a whole. As such, mitigating the risk of fraud is a primary objective of a robust risk management strategy. The detection of fraudulent money transactions is an important application in the detection of anomalies. Different approaches exist to detect anomalies such as k-mean, SVM, regression, decision trees, etc. However, each algorithm has its own limits depending on the model and the case. The objective of this document is to have a system for predicting fraudulent monetary transactions capable of overcoming the various problems associated with the detection of anomalies and having high accuracy. We compare three different techniques in order to distinguish the best approach. We choose the isolation forest with the best test evaluation and best time response.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3b94e6cda659265309ec3130f824e399",
  "timestamp": "2025-05-15T01:59:21.635985"
}