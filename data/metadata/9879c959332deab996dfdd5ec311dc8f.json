{
  "id": 5278,
  "title": "Director liability reduction and stock price crash risk: Evidence from Korea",
  "abstract": "This article investigates the effect of a firm's adoption of director liability reduction coverage laws on their directors' bad news hoarding behavior. Using unique Korean institutional settings, we find that, compared to directors of noncovered firms, those of covered firms are more likely to withhold negative information, proxied by stock price crash risk measures. Our regression analysis implies that legal protections of a company through DLR coverage makes directors relatively relaxed about litigation risks, which induces them to take advantage of the laws. Furthermore, we find that the relation manifests when the firm is owned by a high proportion of foreign investors, covered by many financial analysts, and is less regulated by listed exchange.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9879c959332deab996dfdd5ec311dc8f",
  "timestamp": "2025-05-15T02:46:16.185709"
}