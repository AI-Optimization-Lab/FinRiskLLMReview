{
  "id": 495,
  "title": "Industry return prediction via interpretable deep learning",
  "abstract": "We apply an interpretable machine learning model, the LassoNet, to forecast and trade U.S. industry portfolio returns. The model combines a regularization mechanism with a neural network architecture. A cooperative game-theoretic algorithm is also applied to interpret our findings. The latter hierarchizes the covariates based on their contribution to the overall model performance. Our findings reveal that the LassoNet outperforms various linear and nonlinear benchmarks concerning out-of-sample forecasting accuracy and provides economically meaningful and profitable predictions. Valuation ratios are the most crucial covariates, followed by individual and cross-industry lagged returns. The constructed industry ETF portfolios attain positive Sharpe ratios and positive and statistically significant alphas, surviving even transaction costs.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f0f40646a66d1c2d108828107943aae6",
  "timestamp": "2025-05-15T00:44:20.921802"
}