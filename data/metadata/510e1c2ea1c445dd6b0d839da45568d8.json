{
  "id": 5362,
  "title": "Choice architecture improves pension selection",
  "abstract": "Consumers in Chile of private and semi-private pension systems are not selecting the pension providers that give them the largest Net Present Value. Regulators present retirees with a comparison report of pension plan offerings ranked by the amount offered and the risk classification of the provider. A field experiment implemented in Chile explores alternative presentations of provider performance. We treat respondents by modifying the status quo information 'package' (that exactly replicate what is currently provided by the regulator). Two design choices clearly enhanced consumer welfare in this experiment: Reducing the amount of information improved choice - specifically eliminating the risk profiles of providers leads to better decisions by our respondents. Second, adopting a loss frame also resulted in respondents selecting providers that generated higher returns. These gains from modifying information presentation were considerably higher for those with lower levels of financial literacy. This study is conducted in association with the Superintendencia de Pensiones (SP) and the Comision para el Mercado Financiero (CMF), two of the public offices that oversee the pensions market in Chile.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "510e1c2ea1c445dd6b0d839da45568d8",
  "timestamp": "2025-05-15T02:46:45.053392"
}