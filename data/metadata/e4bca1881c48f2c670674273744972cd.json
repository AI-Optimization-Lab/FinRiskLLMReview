{
  "id": 3485,
  "title": "Energy Investment Risk Assessment for Nations Via Seq2seq Model",
  "abstract": "China's Belt & Road Initiative has been proposed for several years, which has stimulated the economic and financial development of the countries alongside the Belt & Road. For a world's leading energy consuming country, China tries to secure the energy supply from the resource-rich countries via oversea energy investment. In this paper, we propose a sequence to sequence (seq2seq) model to evaluate the energy investment risk of 50 countries alongside the Belt & Road Initiative. Specifically, we first build an indicator system mainly containing six factors. Then we adopt Bi-long-short term memory (Bi-LSTM) as encoder to process the historical statistics. Afterward, we use self-attention mechanism to assign the weights on the six factors of the indicator system. Finally we use a hierarchical convolution neural network decoder to generate the assessment results. Our findings indicate that resource potential and Chinese factor are the most important indicators. And through our thorough investigation, we find that Russia, Kazakhstan, Pakistan, United Arab Emirates, Saudi Arabia, Malaysia and Indonesia are the most recommended target countries for China's oversea energy investment.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e4bca1881c48f2c670674273744972cd",
  "timestamp": "2025-05-15T02:26:48.520916"
}