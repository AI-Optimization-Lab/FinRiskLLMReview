{
  "id": 1162,
  "title": "Explainable artificial intelligence for crypto asset allocation",
  "abstract": "Many investors have been attracted by Crypto assets in the last few years. However, despite the possibility of gaining high returns, investors bear high risks in crypto markets. To help investors and make the markets more reliable, Robot advisory services are rapidly expanding in the field of crypto asset allocation. Robot advisors not only reduce costs but also improve the quality of the service by involving investors and make the market more transparent. However, the reason behind the given solutions is not clear and users face a black-box model that is complex. The aim of this paper is to improve trustworthiness of robot advisors, to facilitate their adoption. For this purpose, we apply Shapley values to the predictions generated by a machine learning model based on the results of a dynamic Markowitz portfolio optimization model and provide explanations for what is behind the selected portfolio weights.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f630f43bba23d5a714bcfc857107e80b",
  "timestamp": "2025-05-15T00:52:33.858649"
}