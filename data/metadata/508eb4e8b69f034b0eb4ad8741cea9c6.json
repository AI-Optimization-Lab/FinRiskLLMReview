{
  "id": 2883,
  "title": "A hybrid approach for generating investor views in Black-Litterman model",
  "abstract": "One of the main contributions of Black-Litterman asset allocation modeling is bringing out the idea of updating portfolios relying on the investor views. The incorporation of such views may inject subjectivity to the modeling phase. There is a recent literature on generating the views with financial forecasting methods, which employ either econometric or learning algorithms. In this research, we aim to propose a hybrid approach to the stream, in which GARCH modeling is used for predicting indicators for the stocks, followed by translating the indicator forecasts to return forecasts through Support Vector Regression. The obtained return forecasts are then fed into Black-Litterman modeling as investor views vector in order to generate portfolios with rolling data. The proposed model is tested in two markets: an emerging market (BIST-30 index of Istanbul Stock Exchange) and a developed market (Dow Jones Index of US Stock Market). In general, the results reveal better portfolio returns and Sharpe ratios than the index return for different holding periods, as well as better portfolio returns than randomly generated portfolios. We further show that the indicators can be accounted relying on the state or the market (non-trending or trending). (C) 2019 Elsevier Ltd. All rights reserved.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "508eb4e8b69f034b0eb4ad8741cea9c6",
  "timestamp": "2025-05-15T01:11:17.479433"
}