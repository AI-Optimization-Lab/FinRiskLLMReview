{
  "id": 977,
  "title": "Patent Portfolio Analysis of the Synergy between Machine Learning and Photonics",
  "abstract": "Machine learning in photonics has potential in many industries. However, research on patent portfolios is still lacking. The purpose of this study was to assess the status of machine learning in photonics technology and patent portfolios and investigate major assignees to generate a better understanding of the developmental trends of machine learning in photonics. This can provide governments and industry with a resource for planning strategic development. I used data-mining methods (correspondence analysis and K-means clustering) to explore competing technological and strategic-group relationships within the field of machine learning in photonics. The data were granted patents in the USPTO database from 2019 to 2020. The results reveal that patents were primarily in image data processing, electronic digital data processing, wireless communication networks, and healthcare informatics and diagnosis. I assessed the relative technological advantages of various assignees and propose policy recommendations for technology development.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "575d5f740f49dafd78711bea671d86bc",
  "timestamp": "2025-05-15T00:50:08.786764"
}