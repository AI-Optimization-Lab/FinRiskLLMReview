{
  "id": 796,
  "title": "Conservative or Aggressive? Confidence-Aware Dynamic Portfolio Construction",
  "abstract": "Indicator-based investing is a popular investment strategy driven by technical analysis for the stock market, the key issue of which is to construct portfolios from technical indicators. Due to the high volatility and non-stationary of the stock market, the effectiveness of an indicator, however, varies largely across different periods, which has made it necessary to dynamically adjust indicator-based investing. In this paper, we propose a confidence-based calibration approach for dynamic portfolio construction. The major intuition behind is to tune a more concentrated portfolio when the indicator yields higher confidence otherwise a relatively equal-weighted one. To seek a maximized long-term profit, we further propose to integrate learning the confidence (i.e., future effectiveness) of an indicator into a unified portfolio construction approach powered by a recurrent reinforcement learning framework. Compared with the traditional indicator investing strategies, our confidence-based calibrated indicator of investing can obtain significantly higher returns with lower risks.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b9c8cfedf61ca70933f317b5e44d4e63",
  "timestamp": "2025-05-15T00:48:22.269927"
}