{
  "id": 3333,
  "title": "Forecasting conditional volatility based on hybrid GARCH-type models with long memory, regime switching, leverage effect and heavy-tail: Further evidence from equity market",
  "abstract": "The properties of clustering, long memory, switching regime, leverage effect and heavy tail in volatility dynamic behavior are induced by important stylized facts in financial markets. There is still a controversy whether incorporating these properties could improve the modelling and forecasting performance of volatility. We construct hybrid volatility models via three perspectives including short memory, long memory and Markov switching GARCH with leverage effect and heavy tail, and empirically compare their performance of in-sample estimation, out-of-sample forecast and risk measurement based on trading data of Chinese equity market index. The outof-sample forecast results indicate that the FIEGARCH model with innovation distribution of GED outperforms the competing models, and the backtesting results of VaR and ES confirm that this model performs well in the application of risk measurement.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "92524110978fd9f3717b9c1d41bd988d",
  "timestamp": "2025-05-15T02:25:13.689158"
}