{
  "id": 3389,
  "title": "Nonlinearity in the cross-section of stock returns: Evidence from China",
  "abstract": "We study which characteristics provide independent information for the cross-section of expected returns in the Chinese stock market based on nonlinear predictive functions. Using 100 commonly explored stock characteristics from January 2000 to December 2019, we identify 15 to 19 characteristics that provide incremental predictive information. We find significant alphas based on the most up-to-date four-factor model of Liu et al. (2019) when exploring these characteristics jointly using nonlinear predictive models. A long-short spread portfolio based on out-of-sample predicted returns by a nonlinear model delivers a higher Sharpe ratio than that by a linear model. We document more supportive evidence for the nonlinear model after exploring interaction effects with firm size, earnings-to-price ratio, turnover, state dependency of predictors, and various methods of predictive information aggregation, such as forecast combination, principle component regression, and partial least squares.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6988223d4ce1d8b0bad2437234caa459",
  "timestamp": "2025-05-15T01:16:22.228821"
}