{
  "id": 3614,
  "title": "A Reinterpretation of the Optimal Demand for Risky Assets in Fund Separation Theorems",
  "abstract": "In a continuous-time portfolio selection model with N risky assets and K state variables driving their risk and return parameters, we derive simple expressions for the allocation to each asset in the K + 1 risky funds of the (K + 2)-fund separation theorem. We show that the allocation to any given risky asset in each fund can be written in terms of the parameters of a regression of the excess returns of this asset on those of the N - 1 remaining assets. We also use these parameters to provide quantitative measures of the increase in Sharpe ratio of the speculative demand, or in the maximum correlation of each hedging demand with respect to the corresponding risk factor, associated with the introduction of a new asset in the investment universe. Finally, we show that in a multiperiod setting, an asset is spanned by others if and only if it improves neither the maximum Sharpe ratio of the speculative demand nor the maximum correlations of the hedging demands with the risk factors.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2bb4a2edc8a9b1bb0deac82ebb0de930",
  "timestamp": "2025-05-15T01:19:00.699133"
}