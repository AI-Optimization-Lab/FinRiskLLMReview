{
  "id": 104,
  "title": "A portfolio construction framework usingLSTM-based stock markets forecasting",
  "abstract": "A novel framework that injects future return predictions into portfolio constructionstrategies is proposed in this study. First, a long-short-term-memory (LSTM) model is trained to learn the monthly closing prices of the stocks. Then these predictions are used in the calculation of portfolio weights. Five different portfolio construction strategies are introduced including modifications to smart-beta strategies. The suggested methods are compared to a number of baseline methods, using the stocks of BIST30 Turkey index. Our strategies yield a very high mean annualized return (25%) which is almost 50% higher than the baseline approaches. The mean Sharpe ratio of our strategies is 0.57, whereas the compared methods' are 0.29 and -0.32. Comprehensive analysis of the results demonstrates that utilizing predicted returns in portfolio construction enables a significant improvement on the performance of the portfolios.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9fe81c499c2d31f53a02aaab13fa4f71",
  "timestamp": "2025-05-15T00:32:31.386544"
}