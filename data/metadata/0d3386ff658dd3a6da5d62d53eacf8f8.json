{
  "id": 5221,
  "title": "Realized quantity extended conditional autoregressive value-at-risk models",
  "abstract": "This paper introduces quantile models that incorporate realized variance, realized semivariance, jump variation and jump semivariation based on a conditional autoregressive quantile regression model framework for improved value-at-risk (VaR) and improved joint forecasts of VaR and expected shortfall (ES), which we denote by (VaR, ES). Our empirical results show that high-frequency-data-based realized quantities lead to better VaR and (VaR, ES) forecasts. We evaluate these using conditional coverage and dynamic quantile backtests for VaR, regression-based backtests for (VaR, ES) and comparison tests based on scoring functions and model confidence sets. The study includes data sets covering the global financial crisis of 2007-9 and the Covid-19 pandemic to ensure stability over different market conditions. The results indicate that realized quantity extensions improve forecasts in terms of classic and comparison tests for all quantile levels and time periods, with stand-alone VaR forecasts benefiting the most. It is shown that the symmetric absolute value",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0d3386ff658dd3a6da5d62d53eacf8f8",
  "timestamp": "2025-05-15T02:45:47.321809"
}