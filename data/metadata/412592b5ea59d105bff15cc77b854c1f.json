{
  "id": 3336,
  "title": "Moral-Hazard Conduct in the European Banks During the First Wave of the Global Financial Crisis",
  "abstract": "This paper investigates the extent to which public interventions, largely implemented to countervail the effects of the 2007-2009 financial crisis, have generated moral hazard behaviour in large European banks, twisting their risk-taking and profitability profiles according to the so-called too-big-to-fail hypothesis. To this end, we devise an empirical framework linking credit risk and profitability changes to bank dimension. By applying a Quantile Regression Approach to a sample of 1476 European financial institutions, and considering the combination of risk and profit size sensitivities at quantile level, we observe that the theoretical hypothesis behind the too big to fail is confirmed when the change in ROA proxies for the risk undertaking. We obtain a different picture-much less consistent with our moral hazard hypothesis-when the change in ROE is employed instead. We also provide a possible explanation for this contradictory pattern by discussing the role of managers versus shareholders in the bank strategic design.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "412592b5ea59d105bff15cc77b854c1f",
  "timestamp": "2025-05-15T02:25:13.706144"
}