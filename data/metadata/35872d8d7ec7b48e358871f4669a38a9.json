{
  "id": 2496,
  "title": "Capital flows at risk: Taming the ebbs and flows",
  "abstract": "We propose a new quantile regression framework to predict the entire future probability distribution of capital flows to emerging markets, based on changes in global financial conditions, domestic structural characteristics, and policies. The approach allows us to differentiate between the impact on the median versus the tails of the future predicted density of flows, and between short-and medium-term effects. We find that FX-and macroprudential interventions are effective in mitigating downside risks to portfolio flows stemming from adverse global shocks, while tightening of capital controls in response appears to be counterproductive. Good institutional frameworks are not able to shield countries from the increased volatility of portfolio flows in the immediate aftermath of global shocks, but can contribute to a more rapid bounce-back of foreign flows over the medium term. Our results highlight the limitations of the standard approach, which focuses only on the short-term behavior of average flows.(c) 2021 International Monetary Fund. Published by Elsevier B.V. All rights reserved.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "35872d8d7ec7b48e358871f4669a38a9",
  "timestamp": "2025-05-15T01:07:35.670254"
}