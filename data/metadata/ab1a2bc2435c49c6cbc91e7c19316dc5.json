{
  "id": 1372,
  "title": "Two-dimensional Technology Profiling of Patent Portfolio",
  "abstract": "Noticing that some technology content is not reflected by patents' individual classification symbols but by the co-assignment of these symbols, this study proposes to represent the technology content of a patent portfolio using a two-dimensional matrix, referred to as the profile matrix of the portfolio. The element Mij of a profile matrix M counts the co-assignment frequency of symbols Ci and Cj, and the element Mii is the individual assignment frequency of symbol Ci. The profile matrix not only covers the traditional one-dimensional patent classification analysis, but also provides a more comprehensive picture to the portfolio's technologies. The profile matrix may be applied to detect the similarity or relatedness between patent portfolios, monitor the shift of an entity's R&D direction, and discover the emergence of new, cross-disciplinary technology.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ab1a2bc2435c49c6cbc91e7c19316dc5",
  "timestamp": "2025-05-15T00:54:56.506550"
}