{
  "id": 1085,
  "title": "Quantifying systemic risk in US industries using neural network quantile regression",
  "abstract": "The study quantified the systemic risk spillovers between top 10 US industries using conditional value-at-risk in a network context by calibrating the marginal effects of a quantile regression process and coving period from January 3, 2007-May 28, 2021 and found significant variations in the risk measured using a nonlinear process. Neural networks identified the manufacturing industry as the center of risk spillovers with the disconnected telecommunication industry in a system-wide neural network portraying its diversification potential. The systematic fragility index, which ranks industries with a high exposure to tail risk in a system, revealed the utilities industry as being the most vulnerable to economically fragile periods. By contrast, the systematic hazard index, which measures the risk contribution of an industry, showed the manufacturing industry as the principal risk contributor. With this tail risk assessment, particularly during distress periods, we stipulate several implications for policymakers, regulators, investors, and financial market participants.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3b7c4f4c38ab787c5e366779626b291a",
  "timestamp": "2025-05-15T01:58:43.396116"
}