{
  "id": 2405,
  "title": "A paradox of openness: Democracies, financial integration & crisis",
  "abstract": "Why do democracies experience financial crises more often than non-democracies? Revisiting the 2008 Great Financial Crisis (GFC) as a significant and informative test case, I argue that considering the way domestic institutions inhere in system-level structures is important to explaining crisis susceptibility among democracies since the turn of the twenty-first century. I introduce the mechanism of co-regime financial connections in showing that regime type is an important systematic feature of global financial flows. Employing a latent space network regression model using IMF Coordinated Portfolio Investment Survey (CPIS), I find that the network of cross-border portfolio asset investments is systematically patterned by co-democracy pairs. I then show that this regime-patterned interdependence affects increased financial crisis susceptibility among democracies. My findings build on literature highlighting the interdependence between domestic- and system-level factors and inform an empirical puzzle regarding the prevalence of financial crises among democracies.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b477714e742794e1546fe9767df61402",
  "timestamp": "2025-05-15T01:06:38.054169"
}