{
  "id": 2653,
  "title": "Using simd genetic programming for faulttolerant trading strategies",
  "abstract": "In this chapter we study the effects of representing a traditional portfolio optimization problem as a classification task in order to reduce the computational cost, and finding more reliable solutions. We use N-Version Genetic Programming to represent the market as a binary classification problem, and evolve two trading strategies that independently look for either buy, or sell, opportunities in parallel. The system is made more fault-tolerant using majority voting for the investment decisions. As inputs to our system we use a large number of instruments from technical analysis, which allows us to increase the execution speed over 100 times using a Sub-Machine-Code Genetic Programming system that evaluates 128 fitness cases in parallel. We see that the strategies generalize well and outperform the buy-and-hold strategy on simulated out-of-sample trading, so there is a clear connection between good classification results and returns on trading. We also see that the n-version voting system can successfully be used to reduce risk. Finally we see that some of the technical analysis instruments appear more frequently than others in the most successful strategies, which could be an indication on actual correlations to the future share price.",
  "year": 2004,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "215f7e6f5ae6a8fd1d278b3b5836eb93",
  "timestamp": "2025-05-15T01:09:14.883818"
}