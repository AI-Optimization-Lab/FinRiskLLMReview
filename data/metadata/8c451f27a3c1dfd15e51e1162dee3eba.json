{
  "id": 438,
  "title": "Testing of warrants market efficiency on the Warsaw stock exchange - Classical approach",
  "abstract": "The efficiency of different markets was a subject of research by plenty of analysts. Most market research on the derivatives market was concentrated on valuation, but only a little part was concentrated on market efficiency. The goal of this article is to provide an empirical test of efficiency of the warrants quoted on the Warsaw Stock Exchange. One of the approaches of the derivatives' market efficiency testing is researching a relationship between implied and historical volatility. The efficient market hypothesis assumes that volatility prediction, which is build on the sign from market, its named implied volatility, could be estimator of empirical volatility in the future, named historical volatility. Using standard procedures for estimating regression line by the OLS and for verification of econometric models, researchers could conclude about rejection or the lack of bases' disallowable the hypotheses' about market efficiency. The research includes testing weak and strong efficiency. There are two ways of testing the warrants market efficiency in the paper. The first approach includes empirical tests based on the following stages: 1. The calculation of underlying assets' historical volatility. 2. The estimation of implied volatility from warrants prices. 3. Verification of hypothesis. The second approach consists in comparing the actual warrants prices and the estimated prices generated from the Black-Scholes pricing model.",
  "year": 2005,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8c451f27a3c1dfd15e51e1162dee3eba",
  "timestamp": "2025-05-15T01:31:50.785859"
}