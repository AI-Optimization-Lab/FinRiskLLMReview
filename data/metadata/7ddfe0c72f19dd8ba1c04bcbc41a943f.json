{
  "id": 477,
  "title": "THE WELFARE CONSEQUENCES OF TAXING CARBON",
  "abstract": "For EMF 32, we applied a new version of our Intertemporal General Equilibrium Model (IGEM) based on the North American Industry Classification System (NAICS). We simulated the impacts arising from the Energy Modeling Forum's broad range of carbon taxes under three revenue recycling options - lump sum redistributions, capital tax reductions, and labor tax cuts. We examined their consequences for industry prices and quantities, for the overall economy, and for the welfare of households, individuals, and society, the latter in terms of efficiency and equity. We rank recycling mechanisms from most to least favorable in terms of the magnitudes of their impacts on net social welfare - efficiency net of equity - recognizing that other objectives may be more important to policy makers and the public. Finally, we and the EMF 32 effort focus only on the economic effects of carbon taxation and revenue recycling; the environmental benefits arising from emissions reductions are not within our scope of study. We find CO2 emissions abatement to be invariant to the chosen recycling scheme. This means that policy makers need not compromise their environmental objectives when designing carbon tax swap options. We also find additional emissions reductions beyond the scope of coverage and points of taxation. Reducing capital taxes promotes new saving, investment and capital formation and is the most favorable recycling mechanism. In 2010 dollars, the welfare loss per ton abated ranges from $0.19 to $3.90 depending on the path of carbon prices. Reducing labor taxes promotes consumption and work through real-wage incentives and is the next most favorable recycling scheme. Here, the welfare loss per ton abated ranges from $11.09 to $16.49 depending on the carbon tax trajectory. Lump sum redistribution of carbon tax revenues is the least favorable recycling option. It incentivizes neither capital nor labor. Consequently, the damages to the economy and welfare are the greatest among the three schemes. With lump sum recycling, the welfare loss per ton abated ranges from $37.15 to $43.61 as carbon taxation becomes more aggressive. While this ranking is common among the participating EMF 32 models, the spread in our results is the greatest in comparison which we attribute to the substitution possibilities inherent in IGEM's econometrics, the absence of barriers to factor mobility, and likely differences in the manner in which tax incentives are structured. We find welfare gains are possible under capital and labor tax recycling when emissions accounting is viewed from a top-down rather than a bottom-up perspective and carbon pricing is at an economy-wide average. However, these gains occur at the expense of abatement. We find capital tax recycling to be regressive while labor tax recycling is progressive as is redistribution through lump sums. Moreover, we find that the lump sum mechanism provides the best means for sheltering the poorest from the welfare consequences of carbon taxation. Thus, promoting capital formation is the best use of carbon tax revenues in terms of reducing the magnitudes of welfare losses while the lump sum and labor tax options are the best uses for reducing inequality.",
  "year": 2018,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7ddfe0c72f19dd8ba1c04bcbc41a943f",
  "timestamp": "2025-05-15T01:32:30.179572"
}