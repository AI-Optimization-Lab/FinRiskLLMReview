{
  "id": 434,
  "title": "How Institutional Actions Before Vaccine Affect Time Vaccination Intention Later: Prediction via Machine Learning",
  "abstract": "Effective vaccination is often the only way to eliminate a major pandemic, to the extent that people welcome the cure. In general, vaccination preferences are shaped before actual vaccines are found. Factors that accelerate/inhibit expected uptake must then be understood upfront if one hopes to nudge hesitants towards vaccination. We predict the portfolio of COVID-19 vaccination drivers through a large set of Machine Learning (ML) techniques for five European countries during the first wave of the COVID-19 and before vaccines were found and rolled out. We find better accuracy emerging from more sophisticated supervised ML techniques than regressions. While some factors are common to all ML tools, some only arise from the most accurate techniques: Gradient Boosting Machine and Support Vector Machine. In general, institutional trust (e.g. towards government actions) is a critical influencer of vaccine intent. How governments have reacted to the pandemic rise is a crucial filter as to how people will accept being vaccinated.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "037573866f46d356c87440871a6f4970",
  "timestamp": "2025-05-15T00:43:10.606853"
}