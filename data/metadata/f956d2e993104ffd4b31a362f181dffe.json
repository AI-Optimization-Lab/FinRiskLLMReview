{
  "id": 1631,
  "title": "Sustainable consumption behaviours in P2P accommodation platforms: an exploratory study",
  "abstract": "This paper examines how sustainable consumption behaviours are assembled in peer-to-peer (P2P) platforms, based on four factors-services portfolio complexity, network membership, reputation and innovative practices-and its impact on P2P platform performance. Using data from one P2P accommodation platform in Romania and based on 2556 observations, we tested the research hypothesis using ordinary least squares regression. Specifically, services portfolio complexity positively influences sustainable consumption behaviours, while network membership has a negative influence. Services portfolio complexity has a positive influence on sustainable consumption behaviours when innovative practices are high. Finally, sustainable consumption behaviours positively influence P2P platform performance.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f956d2e993104ffd4b31a362f181dffe",
  "timestamp": "2025-05-15T00:57:46.914581"
}