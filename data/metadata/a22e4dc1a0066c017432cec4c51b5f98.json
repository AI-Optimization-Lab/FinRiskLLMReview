{
  "id": 57,
  "title": "Consistent time-homogeneous modeling of SPX and VIX derivatives",
  "abstract": "This paper shows how to recover a stochastic volatility model (SVM) from a market model of the VIX futures term structure. Market models have more flexibility for fitting of curves than do SVMs, and therefore are better suited for pricing VIX futures and VIX derivatives. But the VIX itself is a derivative of the S&P500 (SPX) and it is common practice to price SPX derivatives using an SVM. Therefore, consistent modeling for both SPX and VIX should involve an SVM that can be obtained by inverting the market model. This paper's main result is a method for the recovery of a stochastic volatility function by solving an inverse problem where the input is the VIX function given by a market model. Analysis will show conditions necessary for there to be a unique solution to this inverse problem. The models are consistent if the recovered volatility function is non-negative. Examples are presented to illustrate the theory, to highlight the issue of negativity in solutions, and to show the potential for inconsistency in non-Markov settings.",
  "year": 2022,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a22e4dc1a0066c017432cec4c51b5f98",
  "timestamp": "2025-05-15T01:28:03.821440"
}