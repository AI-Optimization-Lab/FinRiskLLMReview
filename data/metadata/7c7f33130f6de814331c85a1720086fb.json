{
  "id": 203,
  "title": "Index tracking using shapley additive explanations and one-dimensional pointwise convolutional autoencoders",
  "abstract": "The aim of index tracking is to mimic the performance of a benchmark index via minimizing the tracking error between the returns of the market index and the tracking portfolio. Lately, various deep learning solutions have been proposed to perform stock prediction or active investment. However, there remains a gap in literature to explore the application of deep learning to index tracking. In this paper, the one-dimensional Pointwise Convolutional Autoencoder is proposed to capture the main market characteristics and the Shapley Additive Explanations feature importance ranking is applied to select stocks to implement the partial replication index tracking with and without Covid-19 data. Moreover, portfolios with different holding periods and with different rebalancing frequency are created on different financial markets to check the effectiveness of the proposed strategy. Compared with different benchmark stock selection strategies, including Pearson correlation, mutual information, and Euclidean distance, the proposed strategy achieves state-of-the-art performance on different financial markets.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7c7f33130f6de814331c85a1720086fb",
  "timestamp": "2025-05-15T00:33:39.411738"
}