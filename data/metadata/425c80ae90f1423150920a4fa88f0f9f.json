{
  "id": 2073,
  "title": "Do US Active Mutual Funds Make Good of Their ESG Promises? Evidence from Portfolio Holdings",
  "abstract": "We investigate the occurrence of greenwashing in the US mutual fund industry. Using panel regression methods, we test whether there exist differences in the portfolio investment behaviors of active equity funds that are self-declared to be driven by ESG motives when compared to all other funds. In particular, we focus on two aspects of funds' portfolio allocation decisions, i.e., the actual implied average ESG ratings of the stocks a mutual fund invests in and the portfolio share invested in sin stocks. We do not find strong evidence that ESG and non-ESG funds make identical investment choices and hence reject the hypothesis of widespread greenwashing. ESG funds, on average, invest more in companies with higher ESG ratings and avoid sin stocks more than non-ESG funds. Nonetheless, we obtain evidence that some degree of greenwashing may still be occurring. However, over time, the differences between ESG and non-ESG funds in these behaviors seem have declined, suggesting a potential reduction in greenwashing practices.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "425c80ae90f1423150920a4fa88f0f9f",
  "timestamp": "2025-05-15T01:02:32.913734"
}