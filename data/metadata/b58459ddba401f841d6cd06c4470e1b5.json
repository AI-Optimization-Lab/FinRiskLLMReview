{
  "id": 1485,
  "title": "Performance attribution for multifactorial equity portfolios",
  "abstract": "This paper revisits the cross-sectional approach to the performance analysis of multifactor investment strategies. Its main contributions are threefold: first, the use of a cross-sectional projection of asset returns onto the factor portfolio weights to form approximate portfolio returns; second, the introduction of nonlinear interaction terms between factors that faithfully reproduce the investment portfolio construction; and third, a natural and intuitive decomposition of the portfolio performance as the sum of factor contributions. The method we propose has several advantages over other time-series-based or general cross-sectional regression models: it faithfully reflects the current state of the investment portfolio, it is parsimonious in the number of explanatory variables, it leads to an approximation of the portfolio returns that has a small residual error and it provides a straightforward interpretation of the portfolio performances in terms of the factors it is designed from. The method we advocate is first presented and explained in detail, and then concrete applications to multifactor equity strategies are presented.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b58459ddba401f841d6cd06c4470e1b5",
  "timestamp": "2025-05-15T00:56:08.447827"
}