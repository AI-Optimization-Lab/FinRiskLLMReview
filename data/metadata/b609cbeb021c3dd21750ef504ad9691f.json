{
  "id": 1725,
  "title": "Access scarcity, legislative generalization, and the business-oriented shift of the congressional agenda",
  "abstract": "Policy specialization in the U.S. Congress benefits the institution collectively and members individually. Yet members of Congress (MCs) are insufficiently specialized to optimize lawmaking success (Volden and Wiseman 2020). In this paper, we demonstrate the increasing propensity of MCs to generalize legislatively is driven largely by an expansion of MC legislative agendas in business domains. We then offer and test an explanation for this trend whereby business's increasing demand for congressional attention (Drutman 2015) has outpaced the supply of congressional capacity to serve business needs (Crossen, Furnas, LaPira, and Burgat 2020; McKay 2022). This unmet demand incentivizes MCs to expand their business portfolio, which results in increased campaign contributions from business political action committees (PACs). We provide evidence consistent with this theory, showing that under conditions of access scarcity, MCs benefit financially (in terms of increased business PCA contributions) by broadening the number of business domains they are active in legislatively.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b609cbeb021c3dd21750ef504ad9691f",
  "timestamp": "2025-05-15T00:58:50.690282"
}