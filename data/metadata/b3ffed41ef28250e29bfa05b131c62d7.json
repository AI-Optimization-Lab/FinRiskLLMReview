{
  "id": 5225,
  "title": "Block Trading Based Volatility Forecasting: An Application of VACD-FIGARCH Model",
  "abstract": "The purpose of this study is to construct the ACD model for the block trading volume duration. The ACD model based on the block trading volume duration is referred to as Volume ACD (VACD) in this study. By integrating with GARCH-type models, the VACD based GARCH type models, which include VACD-GARCH, VACD-IGARCH and VACD-FIGARCH models, are set up. This study selects Chunghwa Telecom (CHT) Inc., offering the America Depository Receipt (ADR) in NYSE, to investigate the block trading volume duration in Taiwanese equity market. The empirical results indicate that the long memory in volume duration series increases dependence at level of volatility clustering by VACD (2,1)-FIGARCH (3,d,1) model. Moreover, the VACD (2,1)-IGARCH (1,1) exhibits relatively better performance of prediction on capturing block trading volume duration. This volatility model is more appropriate in this study to portray the change of the CHT Inc. prices and provides more information about the volatility process for investment strategy, which can be a reference indicator of financial asset pricing, hedging strategy and risk management.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b3ffed41ef28250e29bfa05b131c62d7",
  "timestamp": "2025-05-15T02:45:47.349524"
}