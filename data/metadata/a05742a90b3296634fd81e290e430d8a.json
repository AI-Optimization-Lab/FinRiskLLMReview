{
  "id": 1868,
  "title": "Size and value effects in high-tech industries: The role of R&D investment",
  "abstract": "We use monthly US stock data over 55 years from 1962 to 2017 to show that the R&D Intensity at firms adds another important dimension to the size and value effects in describing stock returns, especially for small high-tech firms. A trading strategy that double sorts on R&D intensity and size or book-to-market ratio outperforms a simple small-minus-big (SMB) or high-minus-low (HML) strategy in producing higher and more significant portfolio returns. The most profitable schemes involve triple sorts by size, BM, and R&D intensity: the payoffs of buying high-BM/R&DActive portfolio and selling low-BM/R&D-Inactive portfolio in the small-size/high-tech group and that of buying high-tech/high-BM and selling low-tech/low-BM in the small-size/R&D-active group generate a return of more than 2% on a monthly basis. Our results are robust to alternative classification method of assigning stocks in portfolios.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a05742a90b3296634fd81e290e430d8a",
  "timestamp": "2025-05-15T01:00:27.391833"
}