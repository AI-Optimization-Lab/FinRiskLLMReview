{
  "id": 3328,
  "title": "Artificial intelligence for load forecasting: A stacking learning approach based on ensemble diversity regularization",
  "abstract": "State-of-art artificial intelligence (AI) has made great breakthroughs in various industries. Ensemble learning mixed with various predictors provides a considerable solution for electric load forecasting in power system. In our paper, the generalization error of ensemble learning is statistically decomposed to exhibit the significance of base-learner diversity. A diversity regularized Stacking learning approach is proposed to solve the electric load forecasting issue. In our model, the input features are comprehensively selected by various tree-based embedded methods to understand the feature contribution. The robust candidate base-learners are extracted from sub -model pool depending on diversity regularization besides the individual learning capability. Mutual informa-tion theory and hierarchical clustering quantitatively assess the dissimilarity degree among base-leaners by exploiting error distribution. The Stacking ensemble framework is utilized to avoid the over-fitting occurrence by employing leave-one-out data splitting procedure for raw dataset block. At last, various cases from different time horizons or geographical scopes are deployed to verify the validity of the model. The case shows that the di-versity regularized Stacking learning has better prediction performance compared with the traditional ensemble model or single model. Load forecasting results become more accurate and stable when elaborately selecting base-learners portfolio.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a3c4f152bdc15b69e3fa15ac80d72865",
  "timestamp": "2025-05-15T01:15:54.843094"
}