{
  "id": 3991,
  "title": "Tax Avoidance and Financial Statement Readability",
  "abstract": "This paper examines whether managers of firms that engage in high levels of tax avoidance (TA) strategically reduce their financial statement readability (FSR) to mitigate the risk of exposing their TA strategies. On average, results are inconclusive, but mainly hold in the sample of firms with low effective tax rates (i.e., high TA levels). Specifically, focused on firms with above-industry-median TA, the panel regression results show a negative relation between TA and FSR, and the difference-in-differences analysis, based on the 'Check-the-Box' regulation in 1997 that exogenously increases tax planning opportunities, suggests the negative impact of TA on FSR is likely causal. The relationship is stronger for firms faced with a greater likelihood of tax audit ex ante, firms in industries with a higher tax risk, and firms with a larger institutional ownership. The evidence adds to our understanding of the influence of corporate TA on financial disclosures.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "06897ab7867b268cf39d6d6fc0e513b3",
  "timestamp": "2025-05-15T02:31:58.651377"
}