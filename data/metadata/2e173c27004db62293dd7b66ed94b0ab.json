{
  "id": 2761,
  "title": "Volatility-sensitive Bayesian estimation ofportfolio value-at-risk and conditionalvalue-at-risk",
  "abstract": "We suggest a new method for integrating volatility information for estimating the value-at-risk and conditional value-at-risk of a portfolio. This new method is devel- oped from the perspective of Bayesian statistics and is based on the idea of volatility clustering. By specifying the hyperparameters in a conjugate prior based on two dif- ferent rolling window sizes, it is possible to quickly adapt to changes in volatility and automatically specify the degree of certainty in the prior. This gives our method an advantage over existing Bayesian methods, which are less sensitive to such changes in volatilities and usually lack standardized ways of expressing the degree of belief. We illustrate our new approach using both simulated and empirical data. The new method provides a good alternative to other well-known homoscedastic and heteroscedastic models for risk estimation, especially during turbulent periods, when it can quickly adapt to changing market conditions.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2e173c27004db62293dd7b66ed94b0ab",
  "timestamp": "2025-05-15T01:10:16.471747"
}