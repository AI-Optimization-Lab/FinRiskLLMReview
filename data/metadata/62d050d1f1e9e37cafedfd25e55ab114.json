{
  "id": 576,
  "title": "Integrate where it matters",
  "abstract": "Many studies have shown that the most treacherous time in the failure-strewn business of mergers comes when companies attempt to combine operations. Surprisingly, however, they often destroy value not as a result of inattention to cle ail but through excessive zeal in their integration efforts.That's because acquirers, recognizing the many potential dangers inherent in the merger process, often attempt to immunize themselves by painstakingly mapping out comprehensive, detailed plans for blending every aspect of operations. What they don't realize is that too much integration can block companies from realizing the benefits of a merger just as easily as too little can. And, in some cases, overintegrating can do far more damage. The authors posit that M&A activity is typically based on one of three types of investment theses . active investing, growing scope and growing scale and that each requires different degrees of merger integration. If an acquired company is the first plank of a new platform in a venture-capitalist firm's portfolio, for example, it will probably require the bare minimum of integration. But deals that enhance scope or scale require executives to pay much more attention to integration. The authors explain how Illinois Too] Works, Sears, Roebuck and Co., BP, Philips Medical Systems and Keppel Offshore & Marine have all benefited from integrating selectively, comprehensively or with a mix of the two, according to whether they were seeking economics of scale or scope.",
  "year": 2004,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "62d050d1f1e9e37cafedfd25e55ab114",
  "timestamp": "2025-05-15T00:37:38.175812"
}