{
  "id": 191,
  "title": "Volatility forecasting using deep recurrent neural networks as GARCH models",
  "abstract": "Estimating and predicting volatility in time series is of great importance in different areas where it is required to quantify risk based on variability and uncertainty. This work proposes a new methodology to predict Time Series volatility by combining Generalized AutoRegressive Conditional Heteroscedasticity (GARCH) methods with Deep Neural Networks. Additionally, the proposal incorporates a mechanism to determine the optimal size of the sliding window used to estimate volatility. In this work, the recurrent neural networks Gated Recurrent Units, Long/Short-Term Memory (LSTM), and Bidirectional Long/Short-Term Memory (BiLSTM) are evaluated with the methods of the family Garch (fGARCH). We conducted Monte Carlo simulation studies with heteroscedastic time series to validate our proposed methodology. Moreover, we have applied the proposed method to real financial data from the stock market, such as the Selective Stock Price Index Chile index, Standard & Poor's 500 Index (S &P500), and the prices of the Stock Exchange from Australia (ASX200). The proposed methodology performs well in predicting the stock options returns volatility one week ahead.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d416f7a53280c339da80e9afedc6f880",
  "timestamp": "2025-05-15T01:35:04.454743"
}