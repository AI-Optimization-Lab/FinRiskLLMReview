{
  "id": 365,
  "title": "Generative Bayesian neural network model for risk-neutral pricing of American index options",
  "abstract": "Financial models with stochastic volatility or jumps play a critical role as alternative option pricing models for the classical Black-Scholes model, which have the ability to fit different market volatility structures. Recently, machine learning models have elicited considerable attention from researchers because of their improved prediction accuracy in pricing financial derivatives. We propose a generative Bayesian learning model that incorporates a prior reflecting a risk-neutral pricing structure to provide fair prices for the deep ITM and the deep OTM options that are rarely traded. We conduct a comprehensive empirical study to compare classical financial option models with machine learning models in terms of model estimation and prediction using S&P 100 American put options from 2003 to 2012. Results indicate that machine learning models demonstrate better prediction performance than the classical financial option models. Especially, we observe that the generative Bayesian neural network model demonstrates the best overall prediction performance.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "dfc75ba419a8c8fda88c5435a10e3190",
  "timestamp": "2025-05-15T01:50:13.992183"
}