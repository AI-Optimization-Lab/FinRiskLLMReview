{
  "id": 1483,
  "title": "BEYOND WORD-LEVEL TO SENTENCE-LEVEL SENTIMENT ANALYSIS FOR FINANCIAL REPORTS",
  "abstract": "This paper attempts to conduct a sentence-level sentiment analysis with respect to financial risk on a collection of financial reports. Specifically, we first propose a simple yet efficient algorithm to generate financial sentiment phrases (senti-phrases), and then with the obtained senti-phrases, we utilize multiple sentence embedding models for better learning the representations of financial risk sentences. In order to verify the performance of the proposed approach, we conduct a risk classification task of financial sentences on a sentence-level labeled dataset of finance reports. Experimental results show that incorporating the obtained senti-phrases into the embedding-based models improves the classification performance.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c3656fa942834e2b024eea1bd2799aba",
  "timestamp": "2025-05-15T02:03:39.769947"
}