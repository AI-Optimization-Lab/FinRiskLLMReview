{
  "id": 2337,
  "title": "Exploring The Efficient Market Hypothesis for Accurate Stock Movement Prediction via Feature-Axis Transformer",
  "abstract": "Stock movement forecasting is a significant challenge in financial machine-learning application fields due to its profound impact on financial markets. While the Efficient Market Hypothesis (EMH) [10] postulates that predicting stock movement is nearly impossible, recent studies using deep learning methodologies exhibit promising results. The EMH is based on the assumption that stock prices immediately incorporate all available information, such as financial statements, earnings announcements, and macroeconomic news. Input features used by the prior studies commonly include Open, High, Low, Close, and Adjusted Close (OHLCA), which potentially contain salient information for forecasting movements. However, most have yet to extract the information implicit in each price separately and utilize it to make predictions. This paper proposes FATE: Feature-Axis Transformer based on EMHdesigned to leverage each OHLCA component to facilitate stock movement prediction. FATE consists of three modules: 1) capturing each feature's temporal correlation between various stocks; 2) generating global market context data; and 3) constructing a contextual vector through correlating to other prices around the closing price. Experimental results show that FATE demonstrates better predictive results than state-of-the-art baselines on six real-world datasets. FATE also yields profitable portfolio trading gains compared to the baselines. Furthermore, it offers interpretable visualized results during its stock movement forecasting operations.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "28d3fc2a6b7b1e93ba9ec86d0d5a7487",
  "timestamp": "2025-05-15T01:05:39.727538"
}