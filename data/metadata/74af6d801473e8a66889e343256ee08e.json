{
  "id": 253,
  "title": "A Risk-sensitive Automatic Stock Trading Strategy Based on Deep Reinforcement Learning and Transformer",
  "abstract": "Navigating the world of stock trading can be a daunting task, with its high risk, high reward potential, and intricate market dynamics. For investors and financial researchers, making informed investment decisions is essential. To our knowledge, limited by insufficient feature representation in highly dynamic trading scenarios, existing rule-based and machine learning-based methods struggle to extract complex intrinsic information from the stock market. Thus, with the huge success of reinforcement learning (RL) in sequential decision-making tasks and the powerful feature extraction capabilities of Transformer, this paper proposes an automatic stock trading method based on RL and Transformer. First, data from multiple stocks are collected into Transformer to extract the correlations among stocks. Then, the output of Transformer is used as the state input for deep RL, and the trading action is obtained. Furthermore, we propose a risk-sensitive reward function by adding the turbulence index to avoid risk. Experimental results on Dow Jones 30 constituent stocks show that the proposed method outperforms baselines regarding profit, risk, and comprehensive indicators.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "74af6d801473e8a66889e343256ee08e",
  "timestamp": "2025-05-15T01:48:30.804204"
}