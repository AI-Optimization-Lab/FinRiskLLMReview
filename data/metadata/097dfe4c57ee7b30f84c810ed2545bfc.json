{
  "id": 4326,
  "title": "Does the implementation of responsibility centers, total quality management, and physician fee programs improve hospital efficiency? Evidence from Taiwan hospitals",
  "abstract": "This study evaluates whether the implementation of various types of hospital-physician integration strategies, such as the responsibility centers system, total quality management, and physician fee programs, enhance efficiency for Taiwan hospitals. Because hospitals in Taiwan are structurally similar to staff-model HMOs, the study has implications beyond Taiwan. RESEARCH DESIGN. The Data Envelopment Analysis model is applied to measure hospital efficiency. Hospital efficiency refers to the ability to produce more outputs (eg, ambulatory and emergency visits, inpatient days, and inpatient visits) with the same inputs (eg, physicians, nurses, ancillary labor, and hospital beds). The sample consists of 90 general hospitals in Taiwan from 1994 to 1996. In addition, multitobit regression analysis is used to simultaneously estimate the effects of the hospital-physician integration strategies and provide better control for the effect of other factors (eg, size, degree of competition, ownership structure, teaching status, and the change in regulatory regime) that may also affect hospital efficiency. RESULTS. When evaluating the hospital-physician integration strategies individually, hospitals that implemented the responsibility centers system, total quality management, and physician fee programs were more efficient than hospitals that did not. Controlling for other factors using the multitobit model, hospitals that implemented physician fee programs remained significantly more efficient than others. In addition, hospitals that implemented total quality management were more efficient when they had implemented the strategy for at least 2 years. Hospitals that implemented the responsibility centers system were more efficient but only when integrating the system with formal incentive schemes. CONCLUSIONS. Physician fee programs seem to provide the most direct and robust incentives to enhance hospital efficiency under a fee-for-service regime like that in Taiwan. Because of time-lagged effects, hospitals that implemented the total quality management programs were more efficient but only when the programs had been implemented for at least 2 years. The responsibility centers system can also be effective when the system was associated with formal incentive schemes. The results indicate the importance of having both the individual-based and team-based incentives in place. Finally, the hospital-physician integration strategies appear to be effective individually, but the results change significantly when they are evaluated simultaneously, together with other control variables, in the tobit model. This indicates the importance of investigating hospital-physician integration strategies as a portfolio instead of a single tool.",
  "year": 2002,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "097dfe4c57ee7b30f84c810ed2545bfc",
  "timestamp": "2025-05-15T01:25:50.598605"
}