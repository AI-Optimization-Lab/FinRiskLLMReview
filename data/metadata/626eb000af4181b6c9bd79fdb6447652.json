{
  "id": 1867,
  "title": "The Use and Misuse of Tracking Error",
  "abstract": "Equity portfolio tracking error to a benchmark is a most ubiquitous restriction for active portfolios as prescribed by fiduciaries. The restriction is typically a tight range with minimum and maximum ex-ante extremes. The typical capitalization-weighted benchmark (i.e., the S&P 500 Index) has significant changes in diversification character overtime. This brings into question the sensibility of holding tracking error ( TE) constant for a skilled active manager. The authors demonstrate that as the concentration of names in the S&P 500 increases, its diversification fades. When this happens, constant tracking error is the enemy of the skilled diversified manager, other things equal. Using multivariate classification and regression trees (CART), they show that high tracking dominates low tracking when index concentration is low, trending lower, and return dispersion is high. Low tracking dominates when the opposite index conditions exist. The authors conclude that the power of a flexible TE process in wealth creation is dominant over inflexibility in the benchmark.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "626eb000af4181b6c9bd79fdb6447652",
  "timestamp": "2025-05-15T01:00:27.377781"
}