{
  "id": 1455,
  "title": "The Application of ESL Robust Regression in the Single Index Model",
  "abstract": "Robust regression has been a common method to solve some portfolio selection problem by using traditional ordinary least square estimation (OLS). However, the outliers in the realistic data often break the data consistency rules which make the ordinary least square estimation lose efficacy. Here, the Exponential Squared Loss (ESL) robust regression is considered to eliminate the influence of outliers. By adjusting the square loss function into the exponential squared loss function and adaptive LASSO penalty function, the parameters estimation accuracy are improved. In this way, it reduces the impact of outliers in historical returns on the investment portfolio decision. Then, combined with the characteristic of the stock market in China, the data of Shenzhen A share market is used to validate the robust of the single index model. The result indicates the advantage of the ESL robust regression by comparing the estimation accuracy of the ESL robust estimation with OLS estimation and M estimation. Finally, the portfolio efficient frontier reveals the stability of ESL robust regression in the single index model.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "77e0543fa3bdcebd0cd095fd93dfa835",
  "timestamp": "2025-05-15T00:56:08.285985"
}