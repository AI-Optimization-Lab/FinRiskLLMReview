{
  "id": 3495,
  "title": "The nonlinear effect of financial and fiscal policies on poverty alleviation in China-An empirical analysis of Chinese 382 impoverished counties with PSTR models",
  "abstract": "Using panel smooth transition regression (PSTR) models, this paper studies the effects of financial and fiscal policies on poverty reduction across 382 poverty-stricken counties in China. The findings are that both fiscal and financial policies have a positive influence on poverty reduction, and their relationships are nonlinear. For either a high or low degree of poverty, fiscal policies are effective for poverty reduction, while financial policies have a greater impact on poverty reduction when there is a medium degree of poverty. Therefore, in deciding which policies should be prioritized for reducing poverty, the level of poverty should be taken into account. To be more specific, when a portfolio of poverty-reduction policies is implemented, fiscal policies should be prioritized at the beginning, when the incidence of poverty is high. Then, financial support should come to the forefront as the poverty level drops, and fiscal support should be stepped up when the poverty level continues to drop.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7d4ceaa161b4a331f099d737a4baa42d",
  "timestamp": "2025-05-15T01:17:54.048129"
}