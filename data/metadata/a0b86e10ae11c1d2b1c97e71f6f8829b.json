{
  "id": 3601,
  "title": "Income Conservatism in the U.S. Technology Sector",
  "abstract": "I investigate the extent and nature of income conservatism in the financial statement numbers of firms in the U. S. technology sector. Technology firms are predicted to have greater income conservatism than other U. S. firms because they are subject to both higher shareholder litigation risk and conservative accounting standards such as SFAS 2. In the absence of a generally accepted measure of conservatism, I examine several proxies, including loss incidence and accounting rates of return, operating cash flow and nonoperating accrual levels, and regression coefficients from the earnings-return model in Basu (1997). Relative to other companies, technology firms' earnings are characterized by higher (and intertemporally increasing) levels of both conditional and unconditional conservatism. These differences are both statistically and economically significant. Further analysis suggests that technology firms' higher conservatism results primarily from lower operating cash flows due to R&D expensing and more income-decreasing accounting accruals linked to litigation risk. The results of this study are potentially useful to financial analysts, researchers, regulators, managers, and other users of financial statements.",
  "year": 2011,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a0b86e10ae11c1d2b1c97e71f6f8829b",
  "timestamp": "2025-05-15T02:27:54.101410"
}