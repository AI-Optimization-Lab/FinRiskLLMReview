{
  "id": 1448,
  "title": "Black-Litterman Portfolio Optimization Using Gaussian Mixture Model",
  "abstract": "The Black-Litterman portfolio model based on the Gaussian mixture model is proposed in this study. Different from current popular research patterns in BlackLitterman portfolio, we do not employ some forecasting algorithms to build investment views, but use the clustering method to dig the historical samples and find the representative centroids as the investment experts. Due to GMM believes that the data follows a mixed normal distribution, with each component is assigned a probability value. We build three types of Black-Litterman portfolio based on the designed selection algorithm, the optimistic-style GMM-BL, the pessimistic-style GMM-BL, and the maximum probability GMM-BL. The corresponding numerical experiments focus on testing the out-of-sample performance of the proposed portfolio models and the baseline strategy, where the maximum probability GMM-BL and the pessimistic-style GMM-BL achieve the highest Sharpe ratio and Calmar ratio, and the optimistic-style GMM-BL shows similar performance with the 1/N model. Although the proposed framework tends to be conservative, the out-of-sample effectiveness still demonstrates its practical value.",
  "year": 2025,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "136e7bd32c16ac3a7a8e8efc2cb40c81",
  "timestamp": "2025-05-15T00:56:08.259549"
}