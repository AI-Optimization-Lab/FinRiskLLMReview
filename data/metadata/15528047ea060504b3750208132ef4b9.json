{
  "id": 266,
  "title": "An improved least squares Monte Carlo valuation method based on heteroscedasticity",
  "abstract": "Longstaff-Schwartz's least squares Monte Carlo method is one of the most applied numerical methods for pricing American-style derivatives. We examine the algorithms regression step, demonstrating that the OLS regression is not the best linear unbiased estimator because of heteroscedasticity. We prove the existence of heteroscedasticity for single-asset and multi-asset payoffs numerically and theoretically, and propose weighted-least squares MC valuation method to correct for it. An extensive numerical study shows that the proposed method produces significantly smaller pricing bias than the Longstaff-Schwartz method under several well-known price dynamics. An empirical pricing exercise using market data confirms the advantages of the improved method. (c) 2017 Elsevier B.V. All rights reserved.",
  "year": 2017,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "15528047ea060504b3750208132ef4b9",
  "timestamp": "2025-05-15T01:30:19.063091"
}