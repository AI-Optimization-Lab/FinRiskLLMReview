{
  "id": 179,
  "title": "Forecasting stock volatility and value-at-risk based on temporal convolutional networks",
  "abstract": "In recent years, deep learning has attracted increasing popularity in modern financial fields. The volatility of financial asset returns as well as the Value-at-Risk (VaR) play a significant role in many applications such as risk management, investment portfolios and etc. Thus, it is extremely essential to accurately estimate volatility and VaR. Temporal convolutional networks (TCNs), a relatively new deep learning architecture for solving sequential modeling tasks, have demonstrated convincingly good performance in many applications. In this paper, we utilize TCNs to forecast stock volatility and VaR. To the best of our knowledge, this is the first attempt to address this task with TCNs. In the experiments conducted with both synthetic data and some real stock data, TCNs are compared with other twelve popular models which include nine conventional approaches (i.e., three GARCH-type models with each being considered three tail distributions) and three deep learning methods (i.e., LSTM, LSTM with attention mechanism and GRU). The Friedman test followed by the Nemenyi post-hoc test is also employed to analyze whether TCNs perform significantly better than the other methods across the real stock datasets. As for volatility modeling, experimental results show that TCNs outperforms all the other methods in terms of RMSE (root mean squared error) and MAE (mean absolute error). In the meantime, TCNs behave best in calculating VaR when evaluating their performance with several metrics. More importantly, the superiority of TCNs over GARCH-type methods are statistically significant. As a result, TCNs can be regarded as an important technique to forecast return volatility and the associated VaR.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9ec3c44ab6544d102752342fbe902f0d",
  "timestamp": "2025-05-15T01:35:04.395188"
}