{
  "id": 27,
  "title": "Attention-enhanced graph neural network for financial distress and bankruptcy risk prediction",
  "abstract": "Accurate prediction of financial distress and bankruptcy risk is crucial for informed investment decisions and effective risk management. Traditional predictive approaches, which primarily rely on linear models or shallow machine learning techniques, often fail to capture the intricate non-linear interdependencies. To address this limitation, this paper proposes an attention-enhanced graph neural network (AE-GNN) model that integrates an advanced attention mechanism to enhance predictive accuracy. By structuring corporate financial data as a feature-based graph, AE-GNN effectively processes complex financial relationships through graph neural networks, employing feature aggregation and a multi-attention mechanism. Extensive experimental evaluations conducted on three benchmark datasets demonstrate that AE-GNN significantly outperforms traditional models in predictive performance, highlighting its potential as a powerful tool for financial risk assessment.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8a2456e6ac88ec3b4ada04d6bbf63fa9",
  "timestamp": "2025-05-15T01:46:03.792260"
}