{
  "id": 272,
  "title": "Credit Risk Evaluation Using: Least Squares Support Vector Machine with Mixture of Kernel",
  "abstract": "Credit risk evaluation under the background of big data has been the major focus of financial and banking industry due to recent financial crises. Recent studies have revealed that emerging modern optimization techniques are advantageous to statistical models for credit risk evaluation, such as LS-SVM. In this paper, a least squares support vector machine with mixture kernel (LS-SVM-MK) is proposed to solve the problem of the traditional LS-SVM model, such as the loss of sparseness and robustness. Thus that will result in slow testing speed and poor generalization performance. The revision model LS-SVM-MK is equivalent to solve a linear equation set with deficient rank just like the over complete problem in independent component analysis. A minimum of 1-penalty based object function is chosen to get the sparse and robust solution. Some credit card datasets are used to demonstrate the effectiveness of this model. The experimental results show that LS-SVM-MK can obtain a small number of features and improve the generalization ability of LS-SVM.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e29c2e3d8852a96ffd82d27466d6cc36",
  "timestamp": "2025-05-15T01:49:04.533857"
}