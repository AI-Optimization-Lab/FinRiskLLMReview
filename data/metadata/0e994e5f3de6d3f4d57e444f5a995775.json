{
  "id": 5155,
  "title": "Earnings management in municipal firms: evidence from Portugal",
  "abstract": "Purpose This paper investigates the earnings management (EM) practices of Portuguese municipal firms (MFs), which are a special type of public-sector entity. Design/methodology/approach MFs are identified using a dedicated list maintained by the Portuguese Government. Accounting data are collected from the Bureau Van Dijk's AMADEUS database. The Burgstahler and Dichev's (1997) methodology and panel data regression methods are employed to examine the EM practices of Portuguese MFs. Findings Portuguese MFs manage earnings to report small and positive net earnings and to avoid disclosing losses. There is evidence that such companies are more likely to engage in EM practices when facing higher liquidation risk and financial leverage, lower liquidity and in certain periods of the local election cycle. Originality/value This is one of the first studies investigating the EM practices of MFs, directly contributing to the literature that explores the relationship between state ownership and the quality of financial statements. The paper has important implications for the MFs' stakeholders, especially regulators and supervising authorities.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0e994e5f3de6d3f4d57e444f5a995775",
  "timestamp": "2025-05-15T02:44:43.986450"
}