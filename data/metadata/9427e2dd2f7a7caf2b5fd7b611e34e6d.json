{
  "id": 3653,
  "title": "Robust Prediction Intervals for Valuation of Large Portfolios of Variable Annuities: A Comparative Study of Five Models",
  "abstract": "Valuation of large portfolios of variable annuities (VAs) is a well-researched area in the actuarial science field. However, the study of producing reliable prediction intervals for prices has received comparatively less attention. Compared to point prediction, the prediction interval can calculate a reasonable price range of VAs and help investors and insurance companies better manage risk to maintain profitability and sustainability. In this study, we address this gap by utilizing five different models in conjunction with bootstrapping techniques to generate robust prediction intervals for variable annuity prices. Our findings show that the Gradient Boosting regression (GBR) model provides the narrowest intervals compared to the other four models. While the Random sample consensus (RANSAC) model has the highest coverage rate, but it has the widest interval. In practical applications, considering the trade-off between coverage rate and interval width, the GBR model would be a preferred choice. Therefore, we recommend using the gradient boosting model with the bootstrap method to calculate the prediction interval of valuation for a large portfolio of variable annuity policies.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9427e2dd2f7a7caf2b5fd7b611e34e6d",
  "timestamp": "2025-05-15T01:19:26.145012"
}