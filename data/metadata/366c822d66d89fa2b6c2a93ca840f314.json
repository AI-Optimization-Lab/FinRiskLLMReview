{
  "id": 2039,
  "title": "Evaluating the Performance of Factor Pricing Models for Different Stock Market Trends: Evidence from China",
  "abstract": "This paper examines the performance of three famous factor pricing models in markets of bull, bear, and consolidation in China. Empirical results show that these models explain the time-series variations in portfolio returns in bearish market reasonably well, but fail to explain the cross-sectional variations. Another two findings are revealed by instability tests. First, the three models are more unstable in trending (i.e., bearish and bullish) markets under time-series regression due to the higher stock price synchronicity. Second, greater instability causes the unitary parameter estimates less reliable and brings about difficulties in explaining the cross-sectional portfolio returns in trending markets.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "366c822d66d89fa2b6c2a93ca840f314",
  "timestamp": "2025-05-15T01:02:10.366126"
}