{
  "id": 4808,
  "title": "Modelling Financial Market Volatility Using Asymmetric-Skewed-ARFIMAX and -HARX Models",
  "abstract": "This study aims to propose an improved modelling framework for high frequency volatitliy in financial stock market. Extended heterogeneous autoregressive (HAR) and fractionally integrated autoregressive moving average (ARFIMA) models are introduced to model the S&P500 index using various realized volatility measures that are robust to jumps. These measures are the tripower variation volatility, and the realized volatities integrated with the nearest neighbor truncation (NNT) approach, namely the minimum and the median realized volatilities. In order to capture volatility clustering and the asymmetric property of various realized volatilities, the HAR and ARFIMA models are extended with asymmetric GARCH threshold specification. In addition, the asymmetric innovations of various realized volatilities are characterized by a skewed student-t distribution. The empirical findings show that the extended model returns the best performance in the in-sample and out-of-sample forecast evaluations. The forecasted results are used in the determination of value-at-risk for S&P500 market.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3cd17a74c8cb28e12e28d86bb474d0d4",
  "timestamp": "2025-05-15T02:41:00.910634"
}