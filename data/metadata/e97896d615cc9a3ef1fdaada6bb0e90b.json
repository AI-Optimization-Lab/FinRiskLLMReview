{
  "id": 614,
  "title": "商业银行宏微观审慎监管协调性研究——基于PVAR模型的实证分析",
  "abstract": "微观审慎监管的局限性和系统性金融风险传染性的增强,使商业银行宏微观审慎协调监管成为当前金融监管改革的趋势。本文选取2008-2017年我国14家上市商业银行半年度数据为样本,在分析银行信贷周期不同阶段宏微观审慎监管作用机制的基础上,采用面板向量自回归(PVAR)模型分析我国银行业宏微观审慎监管协调性。结果表明,微观审慎监管中不良贷款率对当前我国商业银行稳定性的影响长期存在且较为明显,流动比率和核心资本充足率对银行稳定性的冲击作用存在但长期来看影响作用并不显著;宏观审慎监管中广义信贷/GDP偏离度和银行业集中度这两个监管指标对商业银行稳定性的影响都较为明显且长期存在;宏微观审慎监管协调运作能够缓解单一政策实施对金融和经济系统的冲击力度,更有助于金融系统的长期稳定。因此,我国银行业监管不仅要在微观方面加强防范内部信贷违约风险,还要从宏观方面关注信贷结构调整和银行理财业务所可能带来的溢出风险。",
  "year": 2019,
  "source": "CNKI",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e97896d615cc9a3ef1fdaada6bb0e90b",
  "timestamp": "2025-05-14T22:31:55.717680"
}