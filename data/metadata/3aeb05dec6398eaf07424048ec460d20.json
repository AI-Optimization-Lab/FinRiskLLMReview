{
  "id": 200,
  "title": "Identifying Fintech risk through machine learning: analyzing the Q&A text of an online loan investment platform",
  "abstract": "Financial risks associated with Fintech have been increasing with its significant growth in recent years. Aiming at addressing the problem of identifying risks in online lending investment under a financial technology platform, we develop a Q&A text risk recognition model based on attention mechanism and Bi-directional Long Short-Term Memory. First, the Q&A pairing on the text data set is carried out, and the matching data set is selected for the next analysis. Secondly, the online loan investment platform is assessed by the named entity recognition of the question text. Finally, the risk level of the corresponding investment platform is evaluated based on the answer text. The experimental results show that the proposed model has achieved improved precision, recall, F1-score, and accuracy compared with other models. Our proposed model can be applied to identify the risks from the text posted on online loan investment platforms and can be used to guide investors' investment and improve the management of financial technology platforms.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3aeb05dec6398eaf07424048ec460d20",
  "timestamp": "2025-05-15T01:35:04.487487"
}