{
  "id": 3620,
  "title": "Tail risks in large portfolio selection: penalized quantile and expectile minimum deviation models",
  "abstract": "Accurate estimation and optimal control of tail risk is important for building portfolios with desirable properties, especially when dealing with a large set of assets. In this work, we consider optimal asset allocation strategies based on the minimization of two asymmetric deviation measures, related to quantile and expectile regression, respectively. Their properties are discussed in relation with the 'risk quadrangle' framework introduced by Rockafellar and Uryasev [The fundamental risk quadrangle in risk management, optimization and statistical estimation. Surv. Oper. Res. Manag. Sci., 2013, 18(1-2), 33-53], and compared to traditional strategies, such as the mean-variance portfolio. In order to control estimation error and improve the out-of-sample performance of the proposed models, we include ridge and elastic-net regularization penalties. Finally, we propose quadratic programming formulations for the optimization problems. Simulations and real-world analyses on multiple datasets allow to discuss pros and cons of the different methods. The results show that the ridge and elastic-net allocations are effective in improving the out-of-sample performance, especially in large portfolios, compared to the un-penalized ones.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0e48ac90e60dce7fee45b679724b1ac4",
  "timestamp": "2025-05-15T01:19:00.726456"
}