{
  "id": 602,
  "title": "Bankruptcy Prediction: Data Augmentation, LLMs and the Need for Auditor's Opinion",
  "abstract": "Predicting bankruptcy is crucial for managing financial risk in corporations. This study emphasizes incorporating the auditor's opinion text into prediction models to improve their ability to assess financial health. These opinions provide essential insights as they offer an independent assessment, complementing other predictive inputs like the management's discussion and analysis. However, the rarity of bankruptcy cases in the data introduces a challenging issue due to severe class imbalance. To address this, we propose a method to generate synthetic positive samples using a variational autoencoder and integrate the multi-source input in a late fusion setting. We showcase that both data augmentation and using multiple textual sources improve the performance of existing models on a related benchmark dataset. Additionally, we evaluate LLMs when used for data augmentation in the proposed method and in a zero-shot prediction setting, discussing important aspects to consider when incorporating them in a predictive pipeline.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "19154b537a66c8e9baea5598fb987fab",
  "timestamp": "2025-05-15T01:40:14.299432"
}