{
  "id": 2102,
  "title": "Nonlinear prediction of conditional percentiles for Value-at-Risk",
  "abstract": "We propose, implement and evaluate a new approach to predicting conditional distribution tail percentiles, which corresponds to Value-at-Risk (VaR) when applied to financial asset return series. Our approach differs from current methods for measuring VaR in two basic ways. Firstly, while the standard variance-covariance framework assumes that asset return distributions are normal, we make no assumptions about the shape of the entire distribution; instead we focus on estimating only the relevant percentile. Secondly, while the standard approach utilizes only the historical returns and covariances of the assets comprising the portfolio being measured, our method conditions on other additional relevant exogenous variables. We use the mean weighted absolute error function (MWAE), a generalization of the well-known mean absolute error (MAE) cost function. It is a familiar statistical property that the MAE is minimized at the median, or 50th percentile of the data; the generalization allows for estimation of arbitrary p-percentiles. There are two methods to estimating values from data: a lazy method keeps all of the data to compute the desired value at any given point; in contrast, an eager method uses the data to estimate a model which is used to generate the desired output. In this paper, we investigate two approaches for applying the MWAE to estimate tail percentiles in this paper, one using each method. The first, kernel percentile regression, is a lazy method, while the second, neural network percentile regression, is an eager method.",
  "year": 1999,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8767867db028059addaf3a1f1725cf5a",
  "timestamp": "2025-05-15T02:10:59.104334"
}