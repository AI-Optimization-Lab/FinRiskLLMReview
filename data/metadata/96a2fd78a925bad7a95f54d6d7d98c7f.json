{
  "id": 1583,
  "title": "Forecasting cryptocurrency returns with machine learning",
  "abstract": "This article employs machine learning models to predict returns for 3703 cryptocurrencies for the 2013 - 2021 period. Based on daily data, we build an equal (capital)-weighted portfolio that generates 7.1 % (2.4 %) daily return with a 1.95 (0.27) Sharpe ratio. We obtain an out-of-sample R2 of 4.855 %. Our results suggest that cryptocurrencies behave like conventional assets than fiat currencies since variables, including lagged returns, can predict future returns. As assets, cryp-tocurrencies are not weakly efficient, and production costs do not determine their prices. Returns for small cryptocurrencies are more predictable than larger ones. The predictive power of the 1 -day lagged return is stronger than all other features (predictors) combined. The results offer new insights for crypto investors, traders, and financial analysts.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "96a2fd78a925bad7a95f54d6d7d98c7f",
  "timestamp": "2025-05-15T00:57:12.724860"
}