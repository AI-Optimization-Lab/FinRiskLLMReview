{
  "id": 1098,
  "title": "A new macro-financial condition index for the euro area",
  "abstract": "A new time-domain decomposition for weakly stationary or trend stationary processes is introduced. The method is based on trigonometric polynomial modeling, and it is explicitly devised to disentangle medium to long-term and short-term fluctuations in macroeconomic and financial series. A multivariate extension involving sequential univariate decompositions and Principal Components Analysis is also provided. Based on this multivariate approach, new composite indexes of macro-financial conditions for the euro area are introduced. The indicators suggest that most of the GDP contraction during the current pandemic has been of short-term, cyclical nature. Moreover, the financial cycle might have currently achieved a peak area. Hence, the risk of further, deeper disruptions is high, particularly as a new sovereign/corporate debt crisis were not eventually avoided.(c) 2021 EcoSta Econometrics and Statistics. Published by Elsevier B.V. All rights reserved.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a4c527215c28fea98e47082a1f5b868a",
  "timestamp": "2025-05-15T01:58:43.476153"
}