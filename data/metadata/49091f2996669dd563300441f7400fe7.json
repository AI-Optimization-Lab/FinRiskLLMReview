{
  "id": 2248,
  "title": "Heterogeneous Graph Attention Network for Small and Medium-Sized Enterprises Bankruptcy Prediction",
  "abstract": "Credit assessment for Small and Medium-sized Enterprises (SMEs) is of great interest to financial institutions such as commercial banks and Peer-to-Peer lending platforms. Effective credit rating modeling can help them make loan-granted decisions while limiting their risk exposure. Despite a substantial amount of research being conducted in this domain, there are three existing issues. Firstly, many of them are mainly developed based on financial statements, which usually are not publicly-accessible for SMEs. Secondly, they always neglect the rich relational information embodied in financial networks. Finally, existing graph-neural-network-based (GNN) approaches for credit assessment are only applicable to homogeneous networks. To address these issues, we propose a heterogeneous-attention-network-based model (HAT) to facilitate SMEs bankruptcy prediction using publicly-accessible data. Specifically, our model has two major components: a heterogeneous neighborhood encoding layer and a triple attention output layer. While the first layer can encapsulate target nodes' heterogeneous neighborhood information to address the graph heterogeneity, the latter can generate the prediction by considering the importance of different metapath-based neighbors, metapaths, and networks. Extensive experiments in a real-world dataset demonstrate the effectiveness of our model compared with baselines.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "49091f2996669dd563300441f7400fe7",
  "timestamp": "2025-05-15T02:12:51.057187"
}