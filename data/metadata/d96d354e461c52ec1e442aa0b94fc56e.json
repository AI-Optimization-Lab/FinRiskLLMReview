{
  "id": 118,
  "title": "Revolutionising Financial Portfolio Management: The Non-Stationary Transformer's Fusion of Macroeconomic Indicators and Sentiment Analysis in a Deep Reinforcement Learning Framework",
  "abstract": "In the evolving landscape of portfolio management (PM), the fusion of advanced machine learning techniques with traditional financial methodologies has opened new avenues for innovation. Our study introduces a cutting-edge model combining deep reinforcement learning (DRL) with a non-stationary transformer architecture. This model is designed to decode complex patterns in financial time-series data, enhancing portfolio management strategies with deeper insights and robustness. It effectively tackles the challenges of data heterogeneity and market uncertainty, key obstacles in PM. Our approach integrates key macroeconomic indicators and targeted news sentiment analysis into its framework, capturing a comprehensive picture of market dynamics. This amalgamation of varied data types addresses the multifaceted nature of financial markets, enhancing the model's ability to navigate the complexities of asset management. Rigorous testing demonstrates the model's efficacy, highlighting the benefits of blending diverse data sources and sophisticated algorithmic approaches in mastering the nuances of PM.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d96d354e461c52ec1e442aa0b94fc56e",
  "timestamp": "2025-05-15T00:39:32.576955"
}