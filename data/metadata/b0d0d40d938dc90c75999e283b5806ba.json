{
  "id": 1723,
  "title": "Value-at-risk student prescription trees for price personalization",
  "abstract": "Value-at-risk (VaR) models use statistical techniques to estimate potential losses in financial portfolios over a specified time period. In contrast, student prescription trees (SPTs) are interpretable policy optimizers that estimate individual consumer demand and utilize decision trees to segment customers into personalized pricing segments. In this paper, we combine VaR with SPT by calculating the probability of revenue loss associated with making a personalized pricing policy more complex. Similar to VaR, we use Monte Carlo methods to estimate risk. Because policy complexity comes at the expense of interpretabiltiy, we seek to balance complexity against potential gains. Interpretability is important in the context of business decision policies because they are auditable and can improve stakeholder buy-in. In our approach, we utilize full Bayesian revenue posteriors and require that all revenue gains from decision tree splits be statistically significant, with the maximum potential loss remaining below the threshold determined by the risk preferences of the client stakeholders.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b0d0d40d938dc90c75999e283b5806ba",
  "timestamp": "2025-05-15T02:06:52.665162"
}