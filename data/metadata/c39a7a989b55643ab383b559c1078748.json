{
  "id": 1234,
  "title": "Learning regulator influence on internal risk weights",
  "abstract": "Current financial regulation allows banks to use internal models for determining their risk-based capital requirements. Economic theory suggests that banks have incentives to report downward-biased risk weights, a problem that regulators aim to address by auditing banks' models. We study whether different audit standards by different regulators affect internal risk weights, indicating that banks subject to laxer standards may report too low risk weights. We present a supervised learning method to study this question that accounts for the fact that risk weights for the same portfolio could vary across banks even if there was no misreporting. Our approach is based on a generalized least squares technique that learns an individual model for each bank, which we combine with Group Lasso shrinkage to study the importance of different features. We analyze a large sample of internal risk weights for European banks. Our results show that regulator characteristics do have an economically significant impact on risk weights and that their importance is similar to that of relevant economic variables. We conclude that this provides empirical support for the claim that banks do indeed report biased risk weights, thereby undermining regulations aimed at preventing another financial crisis.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c39a7a989b55643ab383b559c1078748",
  "timestamp": "2025-05-15T02:00:30.545492"
}