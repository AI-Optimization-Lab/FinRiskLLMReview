{
  "id": 4771,
  "title": "Robust Estimation of Value-at-Risk Through Correlated Frequency and Severity Model",
  "abstract": "In this article we develop a novel method for accurately estimating the Value-at-Risk (VaR) in the context of modern Operational Risk Management (ORM). We develop a method called Data Partition of Frequency and Severity (DPFS) which more accurately computes the VaR in a modern ORM context. The DPFS involves using clustering analysis to partition the frequency and severity components of the loss data. Since the fundamental theory of modern ORM implicitly assumes a correlation between frequency and severity of losses, we argue that this approach should follow naturally. To test this idea, we perform simulation based studies which show that in a theoretical situation, the new DPFS method will perform better than and in worst case as well as the current VaR best practices (which we call classical method). In addition, we implement our methodology on two publicly available datasets: (1) Financial Index data of S&P 500; (2) Chemical Loss spills as tracked by the US National Coast Guard. We observe that the classical VaR calculation inaccurately estimates the data in both the simulation and real-world data studies while DPFS attains much more accurate VaR estimates.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "34373a3e7de0dce4f7e9ccf1e170f9d3",
  "timestamp": "2025-05-15T02:41:00.785488"
}