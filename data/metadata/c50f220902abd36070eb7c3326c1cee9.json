{
  "id": 2077,
  "title": "A Comparison of Prediction Methods for Credit Default on Peer to Peer Lending using Machine Learning",
  "abstract": "Social lending or peer to peer lending (p2p lending) has emerged as a viable digital platform where lenders and borrowers can do business without the involvement of financial institutions. P2p lending has gained significant momentum recently, with some platform has reached billion-dollar loan circulation. However, p2p lending platforms are not free from any form of risks. A higher return on investment for investor comes with a risk of the loan and interest not being repaid. For this purpose, this research proposes a tree-based classification method for predicting whether a loan will go bad or default before the loan is approved. The high dimensionality of the dataset needs to be processed and chosen carefully. This paper proposes a Binary PSO with SVM to perform feature selection for the dataset and Extremely Randomized Tree (ERT) and Random Forest (RF) as the classifiers. In this research, BPSOSVM-ERT and BPSOSVM-RF are compared with several performance metrics. The experimental results show BPSOSVM can produce subset of features without decreasing the performance from the original features and ERT can outperform RF in several performance metrics. (C) 2019 The Authors. Published by Elsevier B.V.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c50f220902abd36070eb7c3326c1cee9",
  "timestamp": "2025-05-15T02:10:58.998083"
}