{
  "id": 3200,
  "title": "Concordance-based predictive measures in regression models for discrete responses",
  "abstract": "Dependence measures are often used in practice in order to assess the quality of a regression model. This is for instance the case with Kendall's tau and other association coefficients based on concordance probabilities. However, in case the response variable is discrete, correlation indices are often bounded and restricted to a sub-interval of . Hence, in this context, small positive values of Kendall's tau may actually support goodness of prediction when getting close to its highest attainable value. In this paper, we derive the best-possible upper bounds for Kendall's tau when the response variable is discrete. Two cases are considered, depending on whether the score is continuous or discrete. Also, we illustrate the obtained upper bounds on a motor third-party liability insurance portfolio.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5e2dd72aa6a957c5f50636f567487e85",
  "timestamp": "2025-05-15T01:14:54.854158"
}