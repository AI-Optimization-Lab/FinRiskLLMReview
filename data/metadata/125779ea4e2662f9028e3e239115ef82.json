{
  "id": 691,
  "title": "Portfolio model for analyzing human resources: An approach based on neuro-fuzzy modeling and the simulated annealing algorithm",
  "abstract": "This paper presents a new model for developing a human resources portfolio based on a neuro-fuzzy approach. The adaptive neural network is constructed based on the Boston Consulting Group (BCG) portfolio matrix. The adaptive neural network was established by applying the simulated annealing algorithm. The model enables decision makers to evaluate and assess human resources potential in accordance with the environment and its circumstances. The purpose of creating this model is to enable insight into the existing potential and plan assets to improve and promote the employees' potential in a company. The model allows the priorities of the suggested strategies to be defined, which eliminates one of the flaws of the classic BCG portfolio matrix. In this neuro-fuzzy model the input variables are described using fuzzy sets that are represented by Gaussian functions. Using expert reasoning a unique knowledge base is formed which enables employees to be scheduled by strategies. The portfolio model is tested in a realistic industrial environment. (C) 2017 Elsevier Ltd. All rights reserved.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "125779ea4e2662f9028e3e239115ef82",
  "timestamp": "2025-05-15T00:47:18.427963"
}