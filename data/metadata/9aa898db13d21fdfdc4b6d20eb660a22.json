{
  "id": 1969,
  "title": "Patent co-citation analysis of Eli!Lilly & Co. patents",
  "abstract": "This is the third in a series of articles that applies patent citation analysis to pharmaceutical patents. The authors use co-citation clustering to identify the major technology fronts that Eli Lilly & Co. is pursuing. The analysis covers all US patents assigned to Lilly and issued from 1975 through to 1998. Lilly's patents fall into 132 Clusters, which in turn form 13 Groups. Statistics of self-citations to the Clusters identify technology fronts of key importance to Lilly. The authors examine the patents in one of these areas, pertaining to raloxifene and related compounds, in more detail. The article shows how the technique can take a large number of patents, almost 3,000 in this case, and organise them into Groups without the need for an expert to read and classify each. The technique quickly organises the patents in a portfolio and gives a starting paint that can help experts focus on areas of major importance or interest. A company can apply this technique to identify core areas of its patent portfolio, areas where it might face stiff competition, areas where others have found value within its patent portfolio, and areas where licensing opportunities may exist. Competitors applying this technique can see areas of a rival's strength and areas the rival has abandoned or is not following up on.",
  "year": 1999,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9aa898db13d21fdfdc4b6d20eb660a22",
  "timestamp": "2025-05-15T01:01:41.621075"
}