{
  "id": 3292,
  "title": "A Semi-Naive Bayes Model to Forecast the Probability Distribution of Excess Returns in the US Stock Market",
  "abstract": "In this paper, a semi-naive Bayes method to forecast the distribution of future excess returns of stocks in a multi-dimensional variable space is presented. Unlike regression models, the proposed model takes into account both the nonlinearity and the interaction between the variables. The conditional probability distributions of excess returns for the largest 1500 U.S. stocks are numerically estimated using only historical price and volume data. The probability distributions are then used to calculate the expected returns as a function of 20 variables. The model predictions are tested with a market neutral portfolio comprised of 21 long and 21 short stocks with an average turnover of one month. An average annual return of 31.8% with a Sharpe ratio of 2.34 was obtained over a 20-year time period, from 1987 to 2006.",
  "year": 2012,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9d184f065cba2bb98d7367ad1282aaba",
  "timestamp": "2025-05-15T01:15:54.678703"
}