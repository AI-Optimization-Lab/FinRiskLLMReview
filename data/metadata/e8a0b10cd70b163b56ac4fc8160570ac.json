{
  "id": 529,
  "title": "Click Fraud Detection with LightGBM",
  "abstract": "Fraud in the current web advertising scenario represents a serious risk to the Internet economy and advertising industry. One of the most widespread and daunting problem in online advertising is click fraud. In spite of the fact that online advertisers constantly improve their traffic filtering techniques, they still lack effective defense to detect click fraud independently. Thus, having an effective fraud detection algorithm is pivotal for online advertising businesses. In this paper we analyzed click patterns over a generous dataset covering 200 million clicks over 4 days. The key idea was to measure the journey of a user's click across their portfolio and flag IP addresses who produce lots of clicks, but never end up in installing apps. In our study, we used the modern machine learning algorithm, LightGBM - a Gradient Boosting Decision Tree-type method. The algorithm gave an average precision of 98%. In our research, the literature review was the central source to confirm our results.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e8a0b10cd70b163b56ac4fc8160570ac",
  "timestamp": "2025-05-15T00:44:21.100148"
}