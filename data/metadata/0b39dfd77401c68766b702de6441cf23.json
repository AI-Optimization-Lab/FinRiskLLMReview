{
  "id": 7258,
  "title": "Long-term event study of timber real estate investment trust conversions",
  "abstract": "long-term financial performance of four timber real estate investment trust (REIT) conversions in the United States is evaluated by an event study with one-, two-, and three-year event windows. Three types of benchmarks are used in gauging the abnormal returns. The first benchmark is a portfolio of firms that are closest in size and book-to-market ratio to the timber REITs, the second is a portfolio of pre-conversion timber firms, and the third is an equal -weighted timber exchange traded fund (ETF) comprised of selected forest firms. Four approaches are used to calculate abnormal returns. Buy-and-hold abnormal returns and cumulative abnormal returns measure the preliminary abnormal returns, zero -investment portfolio approach with rolling regression evaluates the market -based risk premiums, and panel data analyses capture the relative advantages of REITs over their competitors within the timber industry. On average, annualized abnormal returns of 0.5% and 8.9% are identified before and after the REIT conversions. There is no difference between variances of pre-and post-event annualized abnormal returns. Therefore, structural changes have added values to the timber firms in the long run. Published by Elsevier B.V.",
  "year": 2017,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0b39dfd77401c68766b702de6441cf23",
  "timestamp": "2025-05-15T03:06:35.450650"
}