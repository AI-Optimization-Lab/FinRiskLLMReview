{
  "id": 472,
  "title": "A Novel Dynamic Neural System for Nonconvex Portfolio Optimization With Cardinality Restrictions",
  "abstract": "The Markowitz model, a portfolio analysis model that won the Nobel Prize, lays the theoretical groundwork for modern finance. The transaction cost and the cardinality restriction, which were not covered in Markowitz model, are becoming increasingly important with the advent of high-frequency trading era. However, it remains a challenging problem to consider those constraints due to the nonconvex nature of the problem. A novel dynamic neural network, inspired by its successes in machine learning, is developed to tackle this difficult issue. Theoretical analysis is provided for the convergence of the designed neural network. Experimental results using real stock market data confirm the effectiveness of the proposed model. With the proposed model, the cost function characterizing the overall risks, and rewards is reduced by 123.6% from -4.549x10(-5) to -1.0173x10(-4). This indicates that the proposed strategy is successful in reducing risks and increasing rewards.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "814cc7218d7c45f5d652482fd454401f",
  "timestamp": "2025-05-15T00:43:47.392061"
}