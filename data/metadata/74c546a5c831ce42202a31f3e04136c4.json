{
  "id": 314,
  "title": "Optimal portfolio selection using quantile and composite quantile regression models",
  "abstract": "The portfolio optimization problem can be reformulated by a regression model. Mean-variance portfolios, which are constructed using the sample mean and covariance matrix of asset returns can be considered as a function of the ordinary least squares estimator of the linear regression coefficients. It is well known that this estimator has several drawbacks. In particular many researchers have pointed out its high sensitivity in the presence of outliers and its loss of efficiency in the presence of small deviations from the normality assumption. In this paper we propose a novel regression approach for optimizing portfolios by means of quantile regression models. The proposed portfolios are constructed using certain robust and distribution-free estimators and can be obtained by solving a single linear program, where estimation and portfolio optimization are performed in a single step. Our numerical results on simulated and empirical data confirm that the proposed portfolios behave much better than the traditional mean-variance portfolio, in terms of the Sharpe ratio and expected loss in utility.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "74c546a5c831ce42202a31f3e04136c4",
  "timestamp": "2025-05-15T00:42:00.418822"
}