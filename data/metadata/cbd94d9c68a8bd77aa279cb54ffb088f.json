{
  "id": 498,
  "title": "GANs and synthetic financial data: calculating VaR",
  "abstract": "Generative Adversarial Neural nets (GANs) are a new branch of machine learning techniques. A GAN learns to generate new data from the training data set. We examine the characteristics of the fake financial data using GANs trained on samples of daily S&P 500 and FTSE 100 index values. GANs feature two competing neural networks in a game theoretic context. The Generator net generates pseudo data that is presented to the discriminator net which then attempts to distinguish between the real and the fake data. This facilitates unsupervised learning on the dataset. The generative network generates data sets, while the discriminative network evaluates them. Equilibrium is reached when the generator can fool the discriminator half the time. Potential convergence difficulties led to the development of Wasserstein GANs, which we use in the analysis. We examine the characteristics of the generated fake S&P500 and FTSE 100 data sets. A key issue is how closely does the fake series mimic the real series? We explore this using a variety of metrics including regression analysis, applications of moments and characteristic functions, plus Random Forest analysis. We provide a practical application using the fake data to calculate Value at Risk (VaR).",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cbd94d9c68a8bd77aa279cb54ffb088f",
  "timestamp": "2025-05-15T01:51:31.312689"
}