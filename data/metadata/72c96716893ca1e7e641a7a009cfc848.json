{
  "id": 178,
  "title": "Currency total return swaps: valuation and risk factor analysis",
  "abstract": "Currency total return swaps (CTRS) are hybrid derivative instruments that allow us to simultaneously hedge against credit and currency risks. We develop a structural credit risk model to evaluate CTRS premia. An empirical test on a sample of 23,005 price observations from 59 underlying issuers yields an average percentage error of around 10%. This indicates that, beyond interest rate risk, firm-specific factors are major drivers of the variations in the valuation of these instruments. Regression analysis of residuals shows that exchange rate determinants account for up to 40% of model pricing errors, indicating that a currency risk premium affects the CTRS price significantly but only marginally, which confirms the prevalence of credit risk in the pricing of CTRS.",
  "year": 2013,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "72c96716893ca1e7e641a7a009cfc848",
  "timestamp": "2025-05-15T01:29:09.107621"
}