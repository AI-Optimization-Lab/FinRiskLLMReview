{
  "id": 4570,
  "title": "Performance Measurement and Incentive Intensity",
  "abstract": "In this study, we examine the performance measurement and the intensity of pay for performance in a sample of manufacturing establishments. First, we carry out a descriptive analysis of the configuration of pay for performance plans. The analysis reveals that the adoption of measures of results, such as productivity, volume or quality, are the most widely used. Second, we perform a regression analysis of the relationship between performance measures and incentive intensity while controlling for a range of factors. According to our estimations, the use of at least one measure of results promotes total incentive intensity as well as the intensity of individual pay for performance. On the contrary, measures of human resource management outcomes, subjective measures and financial measures are not significant, or have a negative effect on the intensity of pay for performance. These findings could be explained by the fact that the measures of results display, on average, good characteristics in terms of uncontrollable risk, controllable risk, impact on firm value, distortion and manipulability. On the contrary, the human resource management outcomes, subjective and financial measures are more problematic regarding some of these properties.",
  "year": 2017,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f4cc50fd0d9ee5273900047d3762a330",
  "timestamp": "2025-05-15T02:38:56.208130"
}