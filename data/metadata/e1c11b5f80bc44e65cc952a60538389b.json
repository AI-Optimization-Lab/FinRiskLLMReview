{
  "id": 2748,
  "title": "Latent state recognition by an enhanced hidden Markov model",
  "abstract": "In this paper, we start from relaxing assumptions of traditional hidden Markov model then develop a novel framework for decoding the latent states, from which the dynamics of multi-variable financial data is generated. To construct the framework, we model the observed variables as a p-order vector autoregressive process, allow the latent state to evolve through a semi-Markov chain, and shrink the auto-regression and covariance matrices via a penalized maximization likelihood method. Using the 50-dimensional simulated data, the 12-dimensional 5-min order book data of the Chinese CSI 300 index component stocks, the 49-dimensional daily data of U.S. industry portfolio, and 1-dimensional hourly data of four primary foreign exchange rates, our empirical analyses show that the proposed model outperforms the alternative model in accurately recognizing anomalous events and achieves better sharp ratio in a pseudo trading strategy via the latent states. The superior performance is across the data frequency of minute, hour and daily, the dimension of one, twelve, and fifty, the data type of stock, foreign exchange rate, and industry portfolio. (c) 2020 Elsevier Ltd. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e1c11b5f80bc44e65cc952a60538389b",
  "timestamp": "2025-05-15T01:10:16.435740"
}