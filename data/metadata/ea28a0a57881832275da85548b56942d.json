{
  "id": 428,
  "title": "Simulation/Regression Pricing Schemes for CVA Computations on CDO Tranches",
  "abstract": "We devise simulation/regression numerical schemes for pricing the CVA on CDO tranches, where CVA stands for Credit Valuation Adjustment, or price correction accounting for the defaultability of a counterparty in an OTC derivatives transaction. This is done in the setup of a continuous-time Markov chain model of default times, in which dependence between credit names is represented by the possibility of simultaneous defaults. The main idea of this article is to perform the nonlinear regressions which are used for computing conditional expectations, in the time variable for a given state of the model, rather than in the space variables at a given time in diffusive setups. This idea is formalized as a lemma which is valid in any continuous-time Markov chain model. It is then implemented on the targeted application of CVA computations on CDO tranches.",
  "year": 2014,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ea28a0a57881832275da85548b56942d",
  "timestamp": "2025-05-15T01:31:50.752971"
}