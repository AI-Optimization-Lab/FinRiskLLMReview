{
  "id": 427,
  "title": "Computation of estimates in segmented regression and a liquidity effect model",
  "abstract": "Weighted least squares (WLS) estimation in segmented regression with multiple change points is considered. A computationally efficient algorithm for calculating the WLS estimate of a single change point is derived. Then, iterative methods of approximating the global solution of the multiple change-point problem based on estimating change points one-at-a-time are discussed. It is shown that these results can also be applied to a liquidity effect model in finance with multiple change points. The liquidity effect model we consider is a generalization of one proposed by Cetin et at. [2006. Pricing options in an extended Black Scholes economy with illiquidity: theory and empirical evidence. Rev. Financial Stud. 19, 493-529], allowing that the magnitude of liquidity effect depends on the size of a trade. Two data sets are used to illustrate these methods. (c) 2007 Elsevier B.V. All rights reserved.",
  "year": 2007,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2ce4757727ab710c4f0a658850433a75",
  "timestamp": "2025-05-15T01:31:50.750990"
}