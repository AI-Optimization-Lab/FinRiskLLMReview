{
  "id": 1899,
  "title": "Forecasting volatility of China's crude oil futures based on hybrid ML-HAR-RV models",
  "abstract": "Crude oil futures are central to global economic stability, with their volatility shaping financial markets worldwide. Forecasting volatility in China's emerging crude oil futures market presents unique challenges, particularly during market stress events such as the COVID-19 pandemic and geopolitical disruptions. This study develops hybrid ML-HAR-RV models that integrate machine learning with econometric methods to enhance predictive accuracy and economic interpretability. Our analysis reveals pronounced jumps in volatility, with asymmetric responses to market shocks. Notably, the HAR-RV model incorporating signed jumps significantly improves predictive performance. Hybrid ML-HAR-RV models, especially those leveraging signed jumps, demonstrate superior forecasting capability. These findings refine the understanding of volatility dynamics in emerging futures markets and offer actionable insights for risk management and policy design. Beyond China, our framework provides a scalable approach for modeling commodity market volatility under external shocks, contributing to broader financial modeling and economic strategy.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "74c49c5323c7d2900eec52f0c56694e2",
  "timestamp": "2025-05-15T02:08:39.076740"
}