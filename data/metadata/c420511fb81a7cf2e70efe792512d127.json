{
  "id": 1279,
  "title": "Sparse and Stable Portfolio Selection With Parameter Uncertainty",
  "abstract": "A number of alternative mean-variance portfolio strategies have been recently proposed to improve the empirical performance of the classic Markowitz mean-variance framework. Designed as remedies for parameter uncertainty and estimation errors in portfolio selection problems, these alternative portfolio strategies deliver substantially better out-of-sample performance. In this article, we first show how to solve a general portfolio selection problem in a linear regression framework. Then we propose to reduce the estimation risk of expected returns and the variance-covariance matrix of asset returns by imposing additional constraints on the portfolio weights. With results from linear regression models, we show that portfolio weights derived from new approaches enjoy two favorable properties: sparsity and stability. Moreover, we present insights into these new approaches as well as their connections to alternative strategies in literature. Four empirical studies show that the proposed strategies have better out-of-sample performance and lower turnover than many other strategies, especially when the estimation risk is large.",
  "year": 2015,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c420511fb81a7cf2e70efe792512d127",
  "timestamp": "2025-05-15T00:53:42.192325"
}