{
  "id": 287,
  "title": "Machine Learning Detection for Financial Statement Fraud",
  "abstract": "This study intends to develop a methodology of fraudulent transaction detection model. The algorithm of XGBoost integrating the techniques of SMOTE sampling method and Bayesian Hyperparameter Optimization, is proposed to separate fraud transactions from non-fraud transactions. The experimental results based on the public data set of financial statement fraud from Kaggle website show the proposed model is better than the commonly used binary-classification methods, such as Logistic Regression, SVM, KNN, Random Forest, XGBoost without Hyperparameter Tuning and Multilayer Perceptron. The method of establishing fraud detection models assists people who lack the machine learning domain expertise for the modeling and tuning parameter techniques. It can help to detect abnormal transactions as early as possible and carry out risk management for banking industry.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "da06a16d392551ac1ace58bf42f7d705",
  "timestamp": "2025-05-15T01:49:04.596850"
}