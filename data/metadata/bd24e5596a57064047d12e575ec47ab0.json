{
  "id": 2002,
  "title": "New mutual fund managers: Why do they alter portfolios?",
  "abstract": "This study examines why a new fund manager changes the mutual fund holding portfolio of his or her predecessor immediately after management turnover. The study considers three possible explanations: private information, reputation concerns, and grace periods for new managers to sell underperforming stocks. Monthly data for the study come from a unique database of the Securities Investment Trust and Consulting Association in Taiwan over the period from 2004 to 2012. Both the regression models and the fuzzy-set qualitative comparative analysis (fsQCA) confirm that for the one-year period following a change of manager, portfolio turnover contributes to new managers' outperformance of their predecessors, thus supporting the private information hypothesis. However, for the three-month period following a change of manager, causal asymmetry occurs: portfolio turnover can lead to outperformance or underperformance outcomes, supporting the hypotheses of private information and successors' grace period. (C) 2015 Elsevier Inc. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "bd24e5596a57064047d12e575ec47ab0",
  "timestamp": "2025-05-15T01:02:10.145768"
}