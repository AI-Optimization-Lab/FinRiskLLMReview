{
  "id": 6446,
  "title": "A large CVaR-based portfolio selection model with weight constraints",
  "abstract": "Although the traditional CVaR-based portfolio methods are successfully used in practice, the size of a portfolio with thousands of assets makes optimizing them difficult, if not impossible to solve. In this article we introduce a large CVaR-based portfolio selection method by imposing weight constraints on the standard CVaR-based portfolio selection model, which effectively avoids extreme positions often emerging in traditional methods. We propose to solve the large CVaR-based portfolio model with weight constraints using penalized quantile regression techniques, which overcomes the difficulties of large scale optimization in traditional methods. We illustrate the method via empirical analysis of optimal portfolios on Shanghai and Shenzhen 300 (HS300) index and Shanghai Stock Exchange Composite (SSEC) index of China. The empirical results show that our method is efficient to solve a large portfolio selection and performs well in dispersing tail risk of a portfolio by only using a small amount of financial assets. (C) 2016 Elsevier B.V. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f969df05dd210bbb9f4506858a54787f",
  "timestamp": "2025-05-15T02:58:05.741978"
}