{
  "id": 878,
  "title": "Fraud Risk Monitoring System for E-Banking Transactions",
  "abstract": "Fraudulent e-banking transactions have caused great economic loss every year. Thus, it is important for financial institutions to make the e-banking system more secure, and improve the fraud detection system. Researches for the fraud risk monitoring are mainly focused on score rules and data driven model. The score rule is based on expertise, which is vulnerable to new patterns of frauds. Data driven model is based on machine learning classifiers, and usually has to handle the imbalanced classification problem. In this paper, we propose a novel fraud risk monitoring system for e-banking transactions. Model of score rules for online real-time transactions and offline historical transactions are combined together for the fraud detection. Parallel big data framework: Kafka, Spark and MPP Gbase which integrated with a machine learning algorithm is presented to handle offline massive transaction logs. Experimental results show the effectiveness of our proposed scheme over a real massive dataset of e-banking transactions. This evaluation leads us to identify research gaps and challenges to consider in future research endeavors.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fce2035da3537f680bc17d8d79cde556",
  "timestamp": "2025-05-15T01:56:17.502729"
}