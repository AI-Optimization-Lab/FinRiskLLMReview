{
  "id": 2944,
  "title": "The Risk of Individual Stocks' Tail Dependence with the Market and Its Effect on Stock Returns",
  "abstract": "Traditional beta is only a linear measure of overall market risk and places equal emphasis on upside and downside risks, but actually the latter is always much stronger probably due to the trading mechanism like short-sale constraints. Therefore, this paper employs the nonlinear measure, tail dependence, to measure the extreme downside risks that individual stocks crash together with the whole market and investigates whether such tail dependence risks will affect stock returns. Our empirical evidence based on Shanghai A shares confirms that most stocks display nonnegligible tail dependence with the whole market, and, more importantly, such tail dependence risks can indeed provide additional information beyond beta and other factors for asset pricing. In cross-sectional regression, it is proved that this tail dependence does help to explain monthly returns on Shanghai A shares, whereas the time-series regression further indicates that mimicking portfolio returns for tail dependence can capture strong common variation of Shanghai A stock returns.",
  "year": 2015,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2a243e21f3a97a1a48de7a683472db1a",
  "timestamp": "2025-05-15T01:12:09.096182"
}