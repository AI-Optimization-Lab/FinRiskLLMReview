{
  "id": 2860,
  "title": "Additional learning in computational intelligence and its applications to risk management problems",
  "abstract": "Learning machines should grow up in order to adapt to the environment changing over time. It has been observed that additional learning plays an effective role to this end. Since the rule for classification becomes more and more complex with only additional learning, however, some appropriate forgetting is also needed. It seems natural that much old data are forgotten as time elapses. On the other hand, it is expected to be more effective to forget unnecessary data actively. In this chapter, several methods for active forgetting including Radial Basis Function networks and the potential method are reviewed. The effectiveness of additional learning with active forgetting is shown by some risk management problems in stock portfolio problems and in land-slide disaster forecasting problems.",
  "year": 2005,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "44443376710786198452dce9a8743899",
  "timestamp": "2025-05-15T01:11:17.411762"
}