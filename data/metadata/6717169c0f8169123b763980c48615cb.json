{
  "id": 2544,
  "title": "Using the Tobin Q model to evaluate the impact of credit risks on the bank's market value during the corona pandemic",
  "abstract": "PurposeThis paper aims to assess the impact of credit risk on the market values of private banks during the corona pandemic. Design/methodology/approachThis study is identifying critical issues of credit risk at six great private banks. A conceptual framework is designed based on the Tobin Q model for investigating study hypotheses. Quantitative financial analysis methods have been used for processing data, such as financial ratios, arithmetic mean and multiple linear regression. FindingsThe most important result of this study is the lack of influence of credit risk on the market value of selected banks. Because the dimensions of credit risk have critical importance in increasing or decreasing the market value, these banks must continue to adopt quantitative financial analysis to measure credit risks to avoid their risk. Originality/valueThis study elaborates the need for financial indicators to help assess the market value of banks during the economic crises caused by the closure of commercial institutions during the corona pandemic. There is continued increase in bank credit to support these institutions, borrowers and cash withdrawals, which may affect their market reputation.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6717169c0f8169123b763980c48615cb",
  "timestamp": "2025-05-15T02:16:32.692059"
}