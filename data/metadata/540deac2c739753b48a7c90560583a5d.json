{
  "id": 1292,
  "title": "Integrated PPM Process: Scale Development and Validation",
  "abstract": "This study aims to propose and validate a structural model on project portfolio management, identifying the core processes. Moreover, this study aims to investigate the relation between project portfolio management and performance. The model is proposed and validated through a survey-based research, applying structural equation modeling. The total sample size comprises 103 valid questionnaires. The project portfolio management measurement model validated is composed by a set of 11 processes as follows: knowledge of the organizational context; opportunity identification; decision criteria; classification; selection, prioritization, optimization and sequencing; balancing; approval; resource allocation; formation of portfolio; and project portfolio management infrastructure. The findings indicate that there is a strong relationship between project portfolio management and performance. (C) 2016 Elsevier Ltd. APM and IPMA. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "540deac2c739753b48a7c90560583a5d",
  "timestamp": "2025-05-15T00:54:24.599486"
}