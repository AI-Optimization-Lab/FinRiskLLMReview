{
  "id": 3181,
  "title": "Trendy solutions: Why do states adopt Sustainable Energy Portfolio Standards?",
  "abstract": "Thirty-four states had adopted Sustainable Energy Portfolio Standards (SEPS) or similar goals by the end of 2008, with 14 adoptions since 2006. There appears to be something trendy about SEPS and states may adopt SEPS when internal variables would indicate otherwise. This analysis extends the current discussion of SEPS adoption beyond internal variables, relying on innovation and diffusion theory. Logistic regression with SEPS adoption as the dependent variable is used to test internal determinants and diffusion measures for the years 1997-2008. Of the internal determinants variables, affluence and government ideology were found to be positive and significant. The results show that regional and neighbor diffusion variables are significant in SEPS adoption decisions-even when accounting for ideological distance from previous adopters. (C) 2009 Elsevier Ltd. All rights reserved.",
  "year": 2009,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8c2c8722b9cb09683cbd82f7beae88fb",
  "timestamp": "2025-05-15T01:14:16.285172"
}