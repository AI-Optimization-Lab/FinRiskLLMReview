{
  "id": 1489,
  "title": "Financial credit-risk evaluation with neural and neurofuzzy systems",
  "abstract": "Credit-risk evaluation decisions are important for the financial institutions involved due to the high level of risk associated with wrong decisions. The process of making credit-risk evaluation decision is complex and unstructured. Neural networks are known to perform reasonably well compared to alternate methods for this problem. However, a drawback of using neural networks for credit-risk evaluation decision is that once a decision is made, it is extremely difficult to explain the rationale behind that decision, Researchers have developed methods using neural network to extract rules, which are then used to explain the reasoning behind a given neural network output. These rules do not capture the learned knowledge well enough. Neurofuzzy systems have been recently developed utilizing the desirable properties of both fuzzy systems as well as neural networks. These neurofuzzy systems can be used to develop fuzzy rules naturally. In this study, we analyze the beneficial aspects of using both neurofuzzy systems as well as neural networks for credit-risk evaluation decisions. (C) 1999 Elsevier Science B.V. All rights reserved.",
  "year": 1999,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "44cd29c2b25d0b2b480cadf9a06cfeae",
  "timestamp": "2025-05-15T02:03:39.791969"
}