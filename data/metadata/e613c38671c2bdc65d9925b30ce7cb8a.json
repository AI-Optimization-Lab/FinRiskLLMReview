{
  "id": 2226,
  "title": "Estimation risk for value-at-risk and expected shortfall",
  "abstract": "For a given time series of daily losses that display volatility clustering, the exact nextday and ten-day value-at-risk (VaR) and expected shortfall (ES) are unknown. The usual procedure is to approximate these values by replacing true parameter values with estimates in the formulas for VaR and ES. Parameter estimation errors for a GARCH(1,1) model for this time series lead to approximate VaR and ES that differ from the exact VaR and ES, respectively. Accurate estimation of the VaR and ES is very important for the proper management of financial risks. In this paper, we find linear regression models in which the response variable is the approximate VaR (ES) and the explanatory variable is the exact VaR (ES). We use these linear regression models to determine the properties of the approximate VaR (ES), conditional on the corresponding exact value. For a given value of the exact VaR (ES), the approximate VaR (ES) is close to being an unbiased estimator of the corresponding exact value, but it may differ from this exact value by more than 10% of the exact value with substantial probability.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e613c38671c2bdc65d9925b30ce7cb8a",
  "timestamp": "2025-05-15T02:12:50.978407"
}