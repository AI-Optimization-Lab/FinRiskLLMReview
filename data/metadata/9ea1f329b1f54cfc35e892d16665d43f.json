{
  "id": 173,
  "title": "Implementation of machine learning in lâˆž-based sparse Sharpe ratio portfolio optimization: a case study on Indian stock market",
  "abstract": "Constructing the optimal portfolio by determining and selecting the best combinations of multiple portfolios is computationally challenging due to its exponential complexity. This paper considers the above issue and demonstrates an efficient portfolio selection method based on the sparse minimax Sharpe ratio model involving pre-selected stocks by an unsupervised machine learning approach. Different clustering techniques, such as k-means, fuzzy c-means, and ward linkage, have been used to cluster the stock market data into a finite number of clusters created based on their return rates and related risk levels. Several validity indices have been applied to arrive at the most appropriate number of groups to opt into the portfolio. Further, the sparse minimax Sharpe ratio model is implemented for the selection of the most efficient portfolio. Finally, the efficacy of the developed technique is justified and validated by illustrating a numerical example based on the historical dataset taken from the Bombay stock exchange (BSE), India.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9ea1f329b1f54cfc35e892d16665d43f",
  "timestamp": "2025-05-15T00:40:02.113538"
}