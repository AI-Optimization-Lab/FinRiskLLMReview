{
  "id": 192,
  "title": "非法吸收公众存款罪适用泛化的纠偏",
  "abstract": "非法吸收公众存款罪的审判实践表明，“存款”内涵的异化解读、集资对象的形式性判定、资金用途的认定缺位已成本罪适用泛化之症结。对此应回归金融风险防范立场之上定位本罪的保护法益，并以之为据限定性理解本罪的主、客观构成要件要素以及实质违法性，从而实现正向入罪口径与反向出罪口径的同步收缩。在正向入罪口径端，一方面从客观构成要件要素出发将存款还原为还本付息的资本、货币经营行为，并对集资对象进行“不特定”与“多数”的双重限定；另一方面从主观构成要件要素出发结合既有司法解释，借助虚假意思表示及融资目的实现行为类型化限定。在反向出罪口径端，应结合资金实际用途与清偿情况对本罪违法性的有无及程度进行实质性判断，以此实现非法吸收公众存款罪处罚范围的理性限缩。",
  "year": 2023,
  "source": "CNKI",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a809a6f306ea0a0280ff6400a38369f7",
  "timestamp": "2025-05-14T22:27:22.036098"
}