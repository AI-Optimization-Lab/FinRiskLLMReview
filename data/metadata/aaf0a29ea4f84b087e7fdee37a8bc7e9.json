{
  "id": 3938,
  "title": "Building as a virtual power plant, magnitude and persistence of deferrable loads and human comfort implications",
  "abstract": "This work uses high resolution data from 130 electricity sub-meters to characterise a 12,500m(2) commercial building as a virtual power plant (VPP) by assessing magnitude and duration of electrical loads suitable for demand response (DR). In 2018, the building had a peak hourly demand of 48 W/m(2) and its electricity consumption (183.2 kWh/m(2)/yr.) was within low to medium range of air-conditioned UK portfolio. Deferrable loads from heat pumps, air handling units, lifts, lighting, circulating pumps and dry air coolers were used to illustrate building's DR capability over a maximum duration of 4 h per DR cycle. On average, deferrable loads form 46.4% of total building electricity consumption and across a 4-hour DR cycle can be characterised as having an initial power (and stored energies) of 28 kW (401 +/- 117 kWh); 109 kW (571 +/- 82 kWh); and finally 138 kW (625 +/- 18 kWh) for 100%, 41.5% and 24.6% of time respectively. Following a DR event, the HVAC ability to restore original indoor climate was found to be at least twice as fast as climatic drift during the event. A linear regression model was found to be weak in using external temperature to predict the magnitude of aggregated deferrable loads. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "aaf0a29ea4f84b087e7fdee37a8bc7e9",
  "timestamp": "2025-05-15T01:22:11.988897"
}