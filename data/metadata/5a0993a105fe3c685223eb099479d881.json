{
  "id": 73,
  "title": "Deep reinforcement learning for portfolio management",
  "abstract": "Portfolio management facilitates trading off risks against returns for multiple financial assets. Reinforcement Learning (RL) is one of the most promising algorithms for portfolio management. However, these state-of-the-art RL algorithms only complete the task of portfolio management, i.e., acquire the different asset features of portfolio, without considering the global context information from portfolio, which leads to non-optimal portfolio representations; Moreover, the corresponding optimizations are implemented using only the loss function in the viewpoint of RL, without considering the relationships between the local asset information and global context embeddings, which leads to non-optimal portfolio policies. To deal with these issues, this paper proposes a Task-Context Mutual Actor-Critic (TC-MAC) algorithm for portfolio management. Specifically, TC-MAC algorithm is developed based on: (1) representation learning introduces a proposed Task-Context (TC) learning algorithm, which not only encodes the task (i.e., acquire different asset features) of portfolio, but also encodes the global dynamic context of portfolio, thus which helps to learn optimal portfolio embeddings; (2) policy learning introduces a proposed Mutual Actor-Critic (MAC) framework, which can measure the relationships between local embedding of each asset and global context embeddings by maximizing mutual information, the corresponding Mutual-Information loss function combines with RL loss function (i.e., Actor-Critic loss) to collectively optimize the whole algorithm, thus which helps to learn optimal portfolio policies. Experimental results on real-world datasets demonstrate the superior performance of TC-MAC algorithm over the well-known traditional portfolio methods and these state-of-the-art RL algorithms, at the same time, show its advantageous transferability. (c) 2023 Elsevier B.V. All rights reserved.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "5a0993a105fe3c685223eb099479d881",
  "timestamp": "2025-05-15T00:38:57.031734"
}