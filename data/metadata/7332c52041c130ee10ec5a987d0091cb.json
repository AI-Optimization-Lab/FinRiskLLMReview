{
  "id": 4324,
  "title": "On using artificial neural networks for calibrating tempered stable Levy processes to probabilities of crossing absorbing barriers",
  "abstract": "We propose a new method for calibrating tempered stable Levy processes based on an artificial neural network (ANN), which takes probabilities of crossing a number of fixed barriers by a random walk as input data, and demonstrate its performance for the widely used CGMY model. To train the network we use real historical data and a synthetic dataset. We download and prepare the former to create a sequence of histograms with historical probabilities of crossing the set of barriers by log-returns of the underlying asset. To construct the synthetic dataset, we generate the values of the CGMY model's parameters and calculate the respective probabilities of crossing the barriers as prices of synthetic one-touch-digital options by means of an effective numerical method, which is based on the fast Wiener-Hopf factorization technique. After that, we become able to calibrate the parameters for this model by means of the trained ANN, using the probabilities as input data. As the result, we obtain a fast method to calibrate the CGMY Levy model, which can be used to solve risk management problems on financial markets especially for the case where the asset under consideration is highly liquid and highly volatile at the same time (e.g. cryptocurrencies).",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7332c52041c130ee10ec5a987d0091cb",
  "timestamp": "2025-05-15T02:36:05.158677"
}