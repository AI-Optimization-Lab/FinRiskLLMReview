{
  "id": 2896,
  "title": "Efficient Mimicking Portfolios in Asset Pricing Tests",
  "abstract": "The classic cross-sectional regression (CSR) and mimicking portfolio (MIM) procedures estimate factor risk premia on a test asset span and the resulting tests of asset pricing models are performed with reduced degrees of freedom. Although we can restrict the risk premia of traded factors to equal expected returns, imposing such restrictions on nontraded factors is difficult, which may prevent full performance evaluation. We suggest testing with efficient MIMs that project factors onto a return space spanned by test assets and benchmark traded factors. The generalized method of moments (GMM) tests show that this approach generates more powerful tests andfair comparison against a benchmark model.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f3e33c83a3dbf2431db2d894689e9743",
  "timestamp": "2025-05-15T01:11:48.305047"
}