{
  "id": 693,
  "title": "Return prediction by machine learning for the Korean stock market",
  "abstract": "In this study, we aim to forecast monthly stock returns and analyze factors influencing stock prices in the Korean stock market. To find a model that maximizes the cumulative return of the portfolio of stocks with high predicted returns, we use machine learning models such as linear models, tree-based models, neural networks, and learning to rank algorithms. We employ a novel validation metric which we call the Cumulative net Return of a Portfolio with top 10% predicted return (CRP10) for tuning hyperparameters to increase the cumulative return of the selected portfolio. CRP10 tends to provide higher cumulative returns compared to out-of-sample R-squared as a validation metric with the data that we used. Our findings indicate that Light Gradient Boosting Machine (LightGBM) and Gradient Boosted Regression Trees (GBRT) demonstrate better performance than other models when we apply a single model for the entire test period. We also take the strategy of changing the model on a yearly basis by assessing the best model annually and observed that it did not outperform the approach of using a single model such as LightGBM or GBRT for the entire period.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "96078d0e40a062886be1daea203c8ea3",
  "timestamp": "2025-05-15T00:47:18.444166"
}