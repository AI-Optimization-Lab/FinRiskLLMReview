{
  "id": 1830,
  "title": "Towards low-impact food products through reverse engineering: A functionality-driven approach",
  "abstract": "Currently, food products are formulated by a resource-intensive separation of agro-materials into pure components (isolates). These ingredients are subsequently combined again in food products. Alternatively, impure mildly refined ingredients require fewer resources but are difficult to use in product formulation. Machine learning can predict the properties of these impure ingredients, which facilitates their selection process. This study aims to formulate sustainable ingredient formulations based on predicted techno-functional properties instead of purity (composition). Extensively and mildly refined ingredients from yellow peas and lupine seeds are matched to a product portfolio based on gelation, viscosity, emulsifying stability, and foaming capacity. Formulations based on techno-functional properties include more milder refined ingredients and result in up to 70% reduced global warming potential, water usage, and raw materials compared to formulations based on composition and isolates only. This approach contributes to reducing the environmental impact of the food chain and should be extended to other ingredients or properties.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a231ea7ea8132237e9ca80f373f4010d",
  "timestamp": "2025-05-15T00:59:53.293852"
}