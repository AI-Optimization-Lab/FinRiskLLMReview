{
  "id": 2527,
  "title": "Markov regime-switching quantile regression models and financial contagion detection",
  "abstract": "In this paper, we propose a Markov regime-switching quantile regression model, which considers the case where there may exist equilibria jumps in quantile regression. The parameters are estimated by the maximum likelihood estimation (MLE) method. A simulation study of this new model is conducted covering many scenarios. The simulation results show that the MLE method is efficient in estimating the model parameters. An empirical analysis is also provided, which focuses on the detection of financial crisis contagion between United States and some European Union countries during the period of sub prime crisis from the angle of financial risk. The degree of financial contagion between markets is subsequently measured by utilizing the quantile regression coefficients. The empirical results show that in a crisis situation, the interdependence between United States and European Union countries dramatically increases. (C) 2015 Elsevier B.V. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8c3f33c20482cae016782df75c06d7a9",
  "timestamp": "2025-05-15T02:16:32.607378"
}