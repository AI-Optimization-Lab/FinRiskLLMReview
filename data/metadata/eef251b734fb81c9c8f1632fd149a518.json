{
  "id": 1511,
  "title": "How do apes ape?",
  "abstract": "In the wake of telling critiques of the foundations on which earlier conclusions were based, the last 15 years have witnessed a renaissance in the study of social learning in apes. As a result, we are able to review 31 experimental studies from this period in which social learning in chimpanzees, gorillas, and orangutans has been investigated. The principal question framed at the beginning of this era, Do apes ape? has been answered in the affirmative, at least in certain conditions. The more interesting question now is, thus, How do apes ape? Answering this question has engendered richer taxonomies of the range of social-learning processes at work and new methodologies to uncover them. Together, these studies suggest that apes ape by employing a portfolio of alternative social-learning processes in flexibly adaptive ways, in conjunction with nonsocial learning. We conclude by sketching the kind of decision tree that appears to underlie the deployment of these alternatives.",
  "year": 2004,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "eef251b734fb81c9c8f1632fd149a518",
  "timestamp": "2025-05-15T00:56:39.036888"
}