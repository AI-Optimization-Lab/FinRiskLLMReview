{
  "id": 1518,
  "title": "Cepstral-based clustering of financial time series",
  "abstract": "In this paper, following the Partitioning Around Medoids (PAM) approach and the fuzzy theory, we propose a clustering model for financial time series based on the estimated cepstrum which represents the spectrum of the logarithm of the spectral density function. Selecting the optimal set of financial securities to build a portfolio that aims to maximize the risk-return tradeoff is a largely investigated topic in finance. The proposed model inherits all the advantages connected to PAM approach and fuzzy theory and it is able to compute objectively the cepstral weight associated to each cepstral coefficient by means of a suitable weighting system incorporated in the clustering model. In this way, the clustering model is able to tune objectively the different influence of each cepstral coefficient in the clustering process. The proposed clustering model performs better with respect to other clustering models. The proposed clustering model applied to each security sharpe ratio provides an efficient tool of clustering of stocks. (c) 2020 Published by Elsevier Ltd.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "35820b4d5393f2977b64c1f5da7997db",
  "timestamp": "2025-05-15T02:04:20.003817"
}