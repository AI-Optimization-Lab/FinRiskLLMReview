{
  "id": 66,
  "title": "A SVM Stock Selection Model within PCA",
  "abstract": "In the financial market, well-performing stocks usually have some specific features in financial figures. This paper introduces a machine learning method of support vector machine to construct a stock selection model, which can do the nonlinear classification of stocks. However, the accuracy of SVM classification is very sensitive to the quality of training set. To avoid the direct use of complicated and highly dimensional financial ratios, we bring the principal component analysis (PCA) into SVM model to extract the low-dimensional and efficient feature information, which improves the training accuracy and efficiency as well as preserve the features of initial data. As empirical results show, based on support vector machine, within PCA after norm-standardization, the stock selection model achieves the entire accuracy of 75.4464% in training set and of 61.7925% in test set. Further, the PCA-SVM stock selection model contributes the annual earnings of stock portfolio to outperforming those of A-share index of Shanghai Stock Exchange, significantly. (C) 2014 Published by Elsevier B.V. Open access under CC BY-NC-ND license.",
  "year": 2014,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ae8020386242f88e03021bb7ddd8d0bd",
  "timestamp": "2025-05-15T00:38:56.998663"
}