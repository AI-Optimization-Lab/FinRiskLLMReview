{
  "id": 3759,
  "title": "Forecasting peer-to-peer platform default rate with LSTM neural network",
  "abstract": "Peer-to-peer (P2P) online lending as an innovative financial derivative has emerged around the world, which is different from a bank loan in several aspects, such as borrower qualification and source of funding. These differences potentially increase the risk of P2P loans. Therefore, it is critical to develop suitable methods for the default rate prediction of P2P platform. This paper adopts a new approach, named long short-term memory network (LSTM), to study the default rate of monthly fresh loans in US P2P lending platform Lending Club from 2008 to 2015. Our experimental results suggest that, compared with some traditional models like ARIMA, SVM and ANN, the LSTM network has the highest accuracy of default rate prediction. Besides, the performance of the proposed LSTM is robust in diverse time-series cross-validation modes and time periods. Further results demonstrate that it is the unique ability of extracting time-series information makes the LSTM outperform traditional approaches.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "080e658c0ccfa3acb3c063d983e2d330",
  "timestamp": "2025-05-15T02:29:28.094488"
}