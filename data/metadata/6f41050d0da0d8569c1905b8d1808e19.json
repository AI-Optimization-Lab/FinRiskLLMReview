{
  "id": 898,
  "title": "Analysis of herding behavior in individual investor portfolios using machine learning algorithms",
  "abstract": "This paper examines the determinants of herding at both stock and individual investor levels and studies the portfolio performance of herd vs. non-herd portfolios using machine learning algo-rithms. The disposition effect and the attention effect seem to explain herding behavior at the stock level. At the individual investor level, the cumulative number of buys and portfolio values reduce the prediction of herding behavior, while high values of portfolio return lead to a small increase in herding. Individuals who herd do not outperform either market or non-herd portfolios, suggesting that herding is a behavioral bias. Thus, such behavior seems to destabilize stock markets, creating temporary discrepancies in stock prices followed by reversals back to funda-mentals. The most predictive factor in the performance tests of individual portfolios is the market risk premium and using equally-weighted factors rather than value-weighted factors seem to provide more consistent results in the portfolio performance analyses.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6f41050d0da0d8569c1905b8d1808e19",
  "timestamp": "2025-05-15T00:49:36.252111"
}