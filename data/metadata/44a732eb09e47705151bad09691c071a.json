{
  "id": 3425,
  "title": "Non-linear impact of globalization on financial crimes: a case of developing economies",
  "abstract": "Purpose The Financial Action Task Force defines money laundering as processing of these criminal proceeds to disguise their illegal origin. This is the major portion of financial crime that has ties across borders and like all financial crimes which are well planned and camouflaged, this crime is difficult to detect and deter. Over the years, on one side, globalization has provided development opportunities, it has also become one reason for the pervasiveness of money laundering. This has led to a disturbance in the global financial system and social unrest as proceeds from money laundering are being used in terrorism. The purpose of this study is to explore the non linear effect of globalization on financial crime in the form of money laundering. Design/methodology/approach An investigation based on 119 developing countries from the time period of 1985 till 2015 is conducted in this study. The panel quantile regression model was used to estimate antecedents of money laundering. Findings The study confirmed that globalization follows an inverted U-shaped relationship with money laundering. Furthermore, indicators such as investment portfolio and socioeconomic conditions have a significant effect on money laundering. Originality/value The panel quantile regression model was used to estimate antecedents of money laundering.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "44a732eb09e47705151bad09691c071a",
  "timestamp": "2025-05-15T01:16:55.560834"
}