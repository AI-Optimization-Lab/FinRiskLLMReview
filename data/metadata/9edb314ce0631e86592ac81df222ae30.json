{
  "id": 4025,
  "title": "Dynamics of market states and risk assessment",
  "abstract": "Previous research explored various conditions of financial markets based on the similarity of correlation structures and classified as market states. We introduce modifications to previous selection criteria for these market states, mainly due to increased attention to the transition matrix between the states. Clustering and thus market states are fixed by the optimization of two parameters - number of clusters and noise-suppression, but in similar conditions, we give preference to the clustering which avoids large jumps in the transition matrix. We found statistically significant results applying this model to the S&P 500 and Nikkei 225 markets for the pre-COVID-19 pandemic era (2006-2019). Retaining the epoch length of 20 trading days but reducing the shift of the epoch to a single trading day we are led to the concept of a trajectory of the market in the space of correlation matrices. We may visualize these states after dimensional scaling to two or three dimensions. This approach, using dynamics, improves the options of risk assessment, opens the door to dynamical treatments of markets (e.g. hedging), and shows noise-suppression in a new light.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9edb314ce0631e86592ac81df222ae30",
  "timestamp": "2025-05-15T02:32:27.477178"
}