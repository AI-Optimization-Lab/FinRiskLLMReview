{
  "id": 120,
  "title": "Ghost Expectation Point with Deep Reinforcement Learning in Financial Portfolio Management",
  "abstract": "Reinforcement learning algorithms have a wide range of applications in diverse areas, such as portfolio management, automatic driving, and visual object detection. This paper introduces a novel network architecture Ghost expectation point (GXPT) embedded in a deep reinforcement learning framework based on GhostNet, which is constructed using convolutional neural networks and ghost bottleneck modules. The Ghost bottleneck module can generate many Ghost feature maps, improving the ability of the network to extract information from the real-world market. Furthermore, the number of parameters and floating point operations (FLOPs) is reduced. We use the GXPT to realize Jiang et al.'s Ensemble of Identical Independent Evaluators (EIIE) framework. In the EIIE framework, GhostNet is adapted to implement Identical Independent Evaluators to evaluate the growth potential of each asset. In our experiments, we chose the Accumulated Portfolio Value (APV) and the Sharpe Ratio (SR) to assess the efficiency of our strategy in the back-test. It is found that our strategy is at least 5.11% and 29.9% higher than the comparison strategies in APV and SR, respectively.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4064a0ac7e223e9bc94c51d384c2b826",
  "timestamp": "2025-05-15T00:39:32.583990"
}