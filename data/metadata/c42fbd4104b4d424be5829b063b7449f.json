{
  "id": 3014,
  "title": "THE REVIVAL OF THE FELDSTEIN-HORIOKA PUZZLE AND MODERATION OF CAPITAL FLOWS AFTER THE GLOBAL FINANCIAL CRISIS (2008/09)",
  "abstract": "This study investigates the recent trend of the Feldstein-Horioka puzzle and the underlying reasons for moderation in capital flows. This issue is analysed quite inadequately for the period after the Global Financial Crisis, which represents a crucial turning point for economic climate and policies. The Feldstein-Horioka Puzzle is estimated using the World's 13 largest economies, with panel GMM regression, between 1996 and 2016. We uncover that the Global Financial Crisis had a persistent detrimental effect on capital liberalization, after which the Feldstein-Horioka puzzle has revived and capital mobility has decreased. We suggest two possible explanations for such moderation in capital flows: the increasing risk perception and risk aversion behaviour of fund supplying countries, which increases the home bias, and capital controls against free flow of capital that have been applied after the Global Financial Crisis of 2008/2009.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c42fbd4104b4d424be5829b063b7449f",
  "timestamp": "2025-05-15T02:21:52.144183"
}