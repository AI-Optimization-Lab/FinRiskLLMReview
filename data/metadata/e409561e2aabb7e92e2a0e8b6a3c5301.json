{
  "id": 739,
  "title": "Leveraging Cloud Computing for Stock Market Forecasting with Reinforcement Learning",
  "abstract": "Stock market volatility poses significant challenges for models aiming to consistently outperform benchmark indices, as market-specific characteristics can undermine model performance across different markets. This variability necessitates extensive retraining, which is both time-consuming and computationally expensive with traditional sequential methods. To address these issues, we employed cloud-based concurrent execution to optimize the training of five reinforcement learning algorithms for stock market forecasting. Our approach led to a 19.40% reduction in training time compared to sequential methods. Notably, during the COVID-19 pandemic, the reinforcement learning algorithms demonstrated superior performance, with the Proximal Policy Optimization (PPO) algorithm achieving a 60% increase in portfolio returns compared to traditional benchmarks. This enhanced efficiency and performance underline the benefits of leveraging concurrent execution and cloud-based resources for advanced financial modeling.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e409561e2aabb7e92e2a0e8b6a3c5301",
  "timestamp": "2025-05-15T00:47:18.664215"
}