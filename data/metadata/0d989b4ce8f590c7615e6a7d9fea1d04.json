{
  "id": 1759,
  "title": "Time-varying tail dependence networks of financial institutions",
  "abstract": "In this study we characterize systemic risk by the way that financial institutions are interconnected. We construct time-varying tail dependence networks to investigate the complex interdependencies in the financial system. Time-varying lower-tail dependence networks covering the 2008-17 period are constructed for 48 Chinese financial institutions based on the time-varying Clayton copula model and the minimum spanning tree algorithm. We find that tail dependence among financial institutions increases during crises, with clear peaks during the global financial crisis as well as the Chinese interbank market money shortage in 2013 and China's stock market crash in 2015. From the structures of time-varying lower-tail dependence networks, we observe strong clustering, both within and across sectors. Securities companies are found to play an important role in cross-sectoral tail risk transfer. We also identify seven systemically important financial institutions for the Chinese financial system using topological properties.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0d989b4ce8f590c7615e6a7d9fea1d04",
  "timestamp": "2025-05-15T02:06:52.789889"
}