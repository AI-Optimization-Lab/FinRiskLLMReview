{
  "id": 1900,
  "title": "Geopolitical risk, CEO power, and corporate lobbying: Do powerful CEOs lobby more?",
  "abstract": "We offer fresh empirical evidence on the impact of geopolitical risk (GPR) on corporate lobbying behavior. Using a US sample, we show that firms, on average, scale down their lobbying expenditures when global and US-specific GPR increase. Geopolitical acts have a more negative impact than geopolitical threats. These outcomes remain resilient after controlling for firm-level political action committee (PCA) campaign contributions. Utilizing matched samples, we further show that CEO power moderates this effect, implying that firms with powerful CEOs lobby more when geopolitical risks are high. Finally, we demonstrate that firms with powerful CEOs that intensify their lobbying efforts exhibit stronger financial performance during geopolitical crises. Our findings are consistent with the stewardship perspective of CEO behavior.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "893a798f09731a2e4170a4c18805912d",
  "timestamp": "2025-05-15T02:08:39.077742"
}