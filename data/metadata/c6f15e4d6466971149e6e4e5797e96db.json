{
  "id": 1610,
  "title": "A novel financial performance metric to minimize misclassification costs in model selection",
  "abstract": "A novel financial performance metric (FPM) is introduced seeking to minimise the misclassification cost arising from false positives and false negatives in credit risk assessment. Using the German Credit Dataset (GCD), important financial variables are simulated according to four different asset classes to enable a more accurate and reliable, multidimensional model selection. The misclassification cost arising from FPM is compared with commonly used statistical metrics and the credit scoring example dependent cost matrix (CSEDCM) metric. The results show that CSEDCM underestimates false prediction costs by as much as 99% compared to the FPM. A range of high- performance machine learning methods was compared using FPM and statistical metrics. The Multi-Layer Perceptron outperformed other methods on statistical metrics and overall on financial costs, while a mix of algorithms worked best on either side of the decision threshold. The results confirmed that the proposed FPM would provide a significant financial benefit to organisations.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "c6f15e4d6466971149e6e4e5797e96db",
  "timestamp": "2025-05-15T02:05:08.293268"
}