{
  "id": 7596,
  "title": "Making technological innovation greener: Does firm digital transformation work?",
  "abstract": "Although firms' digital transformation (FDT) and green technological innovation (GTI) are both critical for firm development, we still know little about the precise effects between the two or their influence mechanisms. Combining diffusion of innovation (DOI) theory and the technology-organization-environment (TOE) framework, this study analyzes a sample of listed non-financial Chinese firms from 2010 to 2021 using a fixed-effect and mediating effect model to reveal FDT's effect on GTI and its mechanism. Our findings show that (1) FDT can promote both GTI quantity and quality, but it is more significant for GTI quantity; (2) the positive effects can be obtained by reducing firm risk-taking and increasing R&D intensity; and (3) the role of FDT in promoting GTI is particularly significant in firms in the Midwest region, state-owned enterprises, and firms in high-tech sectors, but has negligible effect on high-polluting firms. Our conclusions remain robust after applying the Poisson model, excluding the effects of COVID-19, adopting the generalized method of moments instrumental variable regression, and using Sobel tests. The findings contribute to the literature by combining DOI and TOE into the same framework and empirically analyzing how FDT can influence GTI.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f50e87d446242d3ac0f07a9f83cf4fc4",
  "timestamp": "2025-05-15T03:10:05.624706"
}