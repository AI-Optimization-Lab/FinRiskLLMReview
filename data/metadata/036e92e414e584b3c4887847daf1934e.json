{
  "id": 3480,
  "title": "Agribusiness diversification and technological innovation efficiency: A U-shaped relationship",
  "abstract": "In recent years, agribusiness firms have increasingly diversified their product portfolio and expanded into new business areas through internal growth or acquisition, which can lead to the reallocation of innovation resources. This study examines the impact of agribusiness diversification (AD) on technological innovation efficiency (TIE) in China. Based on an 8-year longitudinal dataset of 101 Chinese listed agribusiness firms from 2012 to 2019, this study applies the Tobit regression technique to reveal the underlying mechanism between AD and TIE. The empirical results show that: (1) the AD-TIE relationship follows a U-shaped pattern, that is, at low levels of AD, the AD-TIE relationship is negative; at high levels of AD, the AD-TIE relationship is positive. (2) government subsidies (GS) negatively moderate the U-shaped relationship between AD and TIE, such that GS flattens the U-shaped curve. Our findings have important implications for agribusiness managers and public policymakers. [EconLit citations: H25, L25, Q16].",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "036e92e414e584b3c4887847daf1934e",
  "timestamp": "2025-05-15T01:17:25.556096"
}