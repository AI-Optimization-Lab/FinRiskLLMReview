{
  "id": 388,
  "title": "Latent factor model for asset pricing",
  "abstract": "One of the fundamental questions in asset pricing is Why different assets earn different average returns?' In this paper, we designed an autoencoder based asset pricing model to explain the return difference among the stocks in an index. The trained autoencoder generates a set of latent representations that constitutes a combined -`communal'- factor to better explains a large portion of the return differences among the stocks in an index. After analyzing all the stocks in S&P-500, Russel-3000, and NASDAQ-100, we found that our proposed latent factor model outperforms many other factor models in predicting the next day's return. Notably, the experiment results show that on average non-communal stocks earn 0.05% over communal stocks. However, the risk associated with this non-communal stock is also 0.8% higher than communal stocks. The experiments confirm that the superior performance comes from the compensation of high risk associated with these non-communal stocks. Investors will benefit from our latent factor model to identify these communal and non-communal stocks for a high return while diversifying their asset portfolio. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "740d5754b8f7d413353b31fef309553a",
  "timestamp": "2025-05-15T00:35:17.568081"
}