{
  "id": 1775,
  "title": "Credit Risk Models for Financial Fraud Detection: A New Outlier Feature Analysis Method of XGBoost With SMOTE",
  "abstract": "Outlier detection is currently applied in many fields, where existing research focuses on improving imbalanced data or enhancing classification accuracy. In the financial area, financial fraud detection puts higher demands on real-time and interpretability. This paper attempts to develop a credit risk model for financial fraud detection based on an extreme gradient boosting tree (XGBoost). SMOTE is adopted to deal with imbalanced data. AUC is the assessment indicator, and the running time is taken as the reference to compare with other frequently used classification algorithms. The results indicate that the method proposed by this paper performs better than others. At the same time, XGBoost can obtain a ranking of important features that impact the classification results when performing classification tasks, making the evaluation results of the model interpretable. The above shows that the model proposed in the paper is more practical in solving credit risk assessment problems. It has faster response times, reduced costs, and better interpretability.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1bccdbb3dd2240a5adfb7fd554d2ac5d",
  "timestamp": "2025-05-15T02:07:25.682311"
}