{
  "id": 1329,
  "title": "Selection of Market Window Size in Portfolio Strategies",
  "abstract": "We propose a feasible selection method of market window size for four state-of-the-art portfolio strategies. Market window size is a common parameter in the machine learning strategy for portfolio selection. However, in previous researches, the selection of market window size often lacks the guidance of scientific theories. In this paper, we analyze the sensitivity of market window size for four strategies on six benchmark data sets respectively. We study the distribution rule of the best market window sizes, which bring the peak total wealth, and then present the market window size selection method that is effective whether there is ample history data or not. What's more, to appraise the result of our method, we divide the benchmark data sets into two parts. We select the appropriate window size in the first part by our method, while the second part is used to test. By comparing with the wealth achieved in the second part using original method, we find that our selection method can effectively optimize the final results.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3032f210bfb95f72983fe3c7f1560bd3",
  "timestamp": "2025-05-15T00:54:24.683729"
}