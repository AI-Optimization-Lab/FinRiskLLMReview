{
  "id": 1974,
  "title": "Does Bitcoin add value to global industry portfolios?",
  "abstract": "Bitcoin has been increasingly viewed as a new form of investment, yet its role as an asset in a diversified industry portfolio is not well understood. In this paper, we explore the dynamic interdependence between Bitcoin and the ten global industry sectors classified by the Global Industry Classification Standard. We find, in accordance with previous literature, that Bitcoin is relatively isolated from traditional industries. While the near-zero correlation with traditional financial assets offers some diversification benefits to investors, these benefits are counterbalanced by the volatility of the asset. Bitcoin's optimal presence in a minimum variance portfolio is only about 1 percent - a weight that is robust to various methods for estimating the return covariance matrix. Bitcoin's optimal weight in portfolios maximizing Sharpe and Sortino ratios are on the magnitude of 10 to 20 percent. Hence, the value of Bitcoin as an asset in a diversified portfolio critically depends on investors' views about the future of Blockchain technology. Crown Copyright (C) 2020 Published by Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "915b53c7f95e5676f22601768d92cc5d",
  "timestamp": "2025-05-15T01:01:41.629333"
}