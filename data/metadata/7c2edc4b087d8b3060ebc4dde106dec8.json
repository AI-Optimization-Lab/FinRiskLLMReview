{
  "id": 614,
  "title": "Multi-quantile systemic financial risk based on a monotone composite quantile regression neural network",
  "abstract": "This study proposes a novel perspective to calibrate the conditional value at risk (CoVaR) of countries based on the monotone composite quantile regression neural network (MCQRNN). MCQRNN can fix the quantile crossing problem, which is more robust in CoVaR estimating. In addition, we extend the MCQRNN method with quantile-on-quantile (QQ), which can avoid the bias in quantile regression. Building on the estimation results, we construct a systemic risk spillover network across countries in the Asia-Pacific region by considering the suffering and overflow effects. A comparison among MCQRNN, QRNN, and MCQRNN-QQ indicates the significance of monotone composite quantiles in modeling CoVaR. Additionally, the network analysis of composite risk spillovers illustrates the advantages of MCQRNN-QQ-CoVaR compared with QRNN-CoVaR. Moreover, the average composite systemic suffering index and the average composite systemic overflow index are introduced as country-specific measures that enable identifying systemically relevant countries during extreme events.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7c2edc4b087d8b3060ebc4dde106dec8",
  "timestamp": "2025-05-15T01:53:24.730806"
}