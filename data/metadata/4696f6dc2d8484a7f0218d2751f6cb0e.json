{
  "id": 4130,
  "title": "Crisis-driven regulation: evidence from engineering China's banking wealth management market",
  "abstract": "This article uses the banking wealth management market to illustrate how the Chinese government intervened in the banking sector involved in crisis response reform packages. It argues that the regulatory processes regarding the banking wealth management market before and after 2015 support the crisis-driven regulation hypothesis. This article finds that the Chinese government's initial response was to strengthen enforcement; it also made significant changes in legislation, including the redefinition of related products, classification of investors, and adding more mandatory disclosure rules. Finally, the Chinese government adjusted its financial supervisory architecture. It further compares the Chinese government's regulatory strategies responding to the 2015 domestic systemic risk crisis with those adopted by its counterparts in developed countries following the 2008 global financial crisis. This article argues that the Chinese government purposefully adjusted regulatory strategies to regain control over state-owned banks and promote state-led financialisation.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4696f6dc2d8484a7f0218d2751f6cb0e",
  "timestamp": "2025-05-15T02:33:42.035581"
}