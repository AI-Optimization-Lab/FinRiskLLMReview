{
  "id": 2982,
  "title": "Improved Shrinkage Estimators of Covariance Matrices With Toeplitz-Structured Targets in Small Sample Scenarios",
  "abstract": "Shrinkage regularization is an effective strategy to estimate the covariance matrix of multi-variate random vector in small sample scenarios. The purpose of this paper is to propose improved linear shrinkage estimators of covariance matrix as two types of Toeplitz-structured target matrices are respectively employed in the shrinkage procedure. Under Gaussian and non-Gaussian distributions, the corresponding shrinkage estimators are respectively obtained in closed form by unbiasedly estimating the unknown scalar quantities which involve the true covariance matrix. Compared with the existing estimators of same type, the proposed covariance estimators show a significant improvement on the mean squared error in numerical simulations. Moreover, example applications including portfolio risk estimation and classification of real data are provided for verifying the performance of proposed covariance estimators in small sample scenarios.",
  "year": 2019,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0582de170354ed39d036e63c7999fddd",
  "timestamp": "2025-05-15T01:12:09.244343"
}