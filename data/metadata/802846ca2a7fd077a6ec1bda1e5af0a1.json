{
  "id": 8981,
  "title": "Validating and Updating the OHTS-EGPS Model Predicting 5-year Glaucoma Risk among Ocular Hypertension Patients Using Electronic Records",
  "abstract": "Objective: To validate and update the Ocular Hypertension Treatment Study-European Glaucoma Prevention Study (OHTS-EGPS) model predicting risk of conversion from ocular hypertension (OHT) to glaucoma using electronic medical records (EMR). Design: Evaluation and update of a risk prediction algorithm using EMRs and linked visual field (VF) tests. Participants: Newly diagnosed OHT patients attending hospital glaucoma services in England. Inclusion criteria are as follows: intraocular pressure (IOP) 22 to 32 mmHg (either eye); normal baseline VF test, defined as Glaucoma Hemifield Test (GHT) within normal range in a reliable VF test; at least 2 VF tests in total; no significant ocular comorbidities. Methods: Risk factors are as follows: age, ethnicity, sex, IOP, vertical cup-to-disc ratio, central corneal thickness, VF pattern standard deviation, family history of glaucoma, systemic hypertension, diabetes mellitus, and glaucoma treatment. Glaucoma conversion was defined as 2 consecutive and reliable VF tests with GHT outside normal limits and/or need for glaucoma surgery. For validation, the OHTS-EGPS model was applied to predict a patient's risk of developing glaucoma in 5 years. In the updating stage, the OHTS model was refitted by re-estimating the baseline hazard and regression coefficients. The updated model was cross-validated and several variants were explored. Main Outcome Measures: Measures of discriminative ability (c-index) and calibration (calibration slope) were calculated and pooled across hospitals using random effects meta-analysis. Results: From a total of 138 461 patients from 10 hospital glaucoma services in England, 9030 patients with OHT fitted the inclusion criteria. A total of 1530 (16.9%) patients converted to glaucoma during this follow-up period. The OHTS-EGPS model provided a pooled c-index of 0.61 (95% confidence interval: 0.60-0.63), ranging from 0.55 to 0.67 between hospitals. The pooled calibration slope was 0.45 (0.38-0.51), ranging from 0.25 to 0.64 among hospitals. The overall refitted model performed better than the OHTS-EGPS model, with a pooled c-index of 0.67 (0.65-0.69), ranging from 0.65 to 0.75 between hospitals. Conclusions: We performed an external validation of the OHTS-EGPS model in a large English population. Refitting the model achieved modest improvements in performance. Given the poor performance of the OHTSEGPS model in our population, one should use caution in its application to populations that differ from those in the OHTS and EGPS. Financial Disclosure(s): Proprietary or commercial disclosure may be found in the Footnotes and Disclosures at the end of this article. Ophthalmology Glaucoma 2025;8:143-151 (c) 2024 by the American Academy of Ophthalmology. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/ 4.0/).",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "802846ca2a7fd077a6ec1bda1e5af0a1",
  "timestamp": "2025-05-15T03:24:26.303519"
}