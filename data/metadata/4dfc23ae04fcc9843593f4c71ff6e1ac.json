{
  "id": 500,
  "title": "Predicting macro-financial instability-How relevant is sentiment? Evidence from long short-term memory networks",
  "abstract": "This paper examines the relevance of sentiment in predicting overall financial system instability using long-run short-term memory networks. Weekly data on the US financial system, consumer sentiment, producer sentiment, and investor sentiment is collected from 21 January 1994 to 27 December 2019, and different models are developed to predict the one-week-ahead levels of financial stress in the US financial system. We find that models using sentiment indices outper-form those relying solely on historical financial stress and risk data. This result is robust to comparisons with an alternative deep learning method and out-of-sample predictions. It consti-tutes an argument in favor of behavioral finance and Minsky's (Knell, 2015) financial instability hypothesis against the Efficient Market Hypothesis. As it concretely identifies the main indicators for predicting US financial stress one week in advance, the study provides relevant recommen-dations for policymakers and investors in terms of macroprudential policies and portfolio management.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4dfc23ae04fcc9843593f4c71ff6e1ac",
  "timestamp": "2025-05-15T01:38:59.292258"
}