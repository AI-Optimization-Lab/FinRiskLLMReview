{
  "id": 6526,
  "title": "A Gamma Ornstein-Uhlenbeck model driven by a Hawkes process",
  "abstract": "We propose an extension of the Gamma-OU Barndorff-Nielsen and Shephard model taking into account jump clustering phenomena. We assume that the intensity process of the Hawkes driver coincides, up to a constant, with the variance process. By applying the theory of continuous-state branching processes with immigration, we prove existence and uniqueness of strong solutions of the SDE governing the asset price dynamics. We propose a measure change of self-exciting Esscher type in order to describe the relation between the risk-neutral and the historical dynamics, showing that the Gamma-OU Hawkes framework is stable under this probability change. By exploiting the affine features of the model we provide an explicit form for the Laplace transform of the asset log-return, for its quadratic variation and for the ergodic distribution of the variance process. We show that the proposed model exhibits a larger flexibility in comparison with the Gamma-OU model, in spite of the same number of parameters required. We calibrate the model on market vanilla option prices via characteristic function inversion techniques, we study the price sensitivities and propose an exact simulation scheme. The main financial achievement is that implied volatility of options written on VIX is upward shaped due to the self-exciting property of Hawkes processes, in contrast with the usual downward slope exhibited by the Gamma-OU Barndorff-Nielsen and Shephard model.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d3ad83f9a9405d3b9376ca1521dedcb8",
  "timestamp": "2025-05-15T02:59:07.472728"
}