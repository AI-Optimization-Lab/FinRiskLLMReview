{
  "id": 3593,
  "title": "Co-optimizing transmission and BESS expansions with system strength constraints",
  "abstract": "The increased penetration of renewables through converter interfaced generation (CIG) and the associated displacement of synchronous generators (SGs) can significantly reduce system strength. Although some network enhancements, such as the installation of battery energy storage systems (BESS), can partially counterbalance the reduction in strength, current network planning methodologies overlook system strength effects. In this context, this work proposes a new optimization model for co-optimizing investments in network expansions and battery systems to meet, at minimum cost, system strength requirements quantified through short circuit levels (SCL). Recognizing that SCL needs vary on a case-by-case basis, this model incorporates a logistic regression analysis trained on offline time-domain dynamic simulations; this approach effectively correlates SCL with system stability, providing a tailored strategy for each power system. To solve the optimization, we also propose an algorithm to iteratively find the optimal solution for the co-optimized portfolio of new transmission and energy storage investments. Through various case studies, we demonstrate the importance of considering system strength requirements in planning studies with significant penetration of CIG.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4b2adabd4869ea7bfadfdf2aab19e94a",
  "timestamp": "2025-05-15T01:19:00.622678"
}