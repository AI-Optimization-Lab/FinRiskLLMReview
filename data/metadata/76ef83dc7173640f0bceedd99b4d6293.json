{
  "id": 3079,
  "title": "Practice of a Two-Stage Model Using Support Vector Regression and Black-Litterman for ETF Portfolio Selection",
  "abstract": "Robo-advisor is a hot topic in the field of financial technology (FinTech) in recent years. This study proposes a programmatic ETF portfolio configuration that combines SVR and Black-Litterman's two-stage model. The results of the study showed that the MSE of the first stage of the model was 2.7970 with good performance. According to the prediction result of the first stage, the parameter setting of the Black-Litterman is performed, and then the model is constructed to further adjust the configuration. The final results show that under the same risk value, the SVM+BL two-stage model proposed by this study has a higher return rate than historical return and implied return. Therefore, it can be provided as a reference for the development of an ETF investment strategy.",
  "year": 2019,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "76ef83dc7173640f0bceedd99b4d6293",
  "timestamp": "2025-05-15T02:22:25.083880"
}