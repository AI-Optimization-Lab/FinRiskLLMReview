{
  "id": 2243,
  "title": "Fund Concentration: A Magnifier of Manager Skill",
  "abstract": "Investors who believe an active manager has insights or forecasts with the potential to earn higher relative returns may look for the manager to concentrate their portfolios. Because foresight is imperfect and correct forecasts will lead to more favorable outcomes and incorrect forecasts to less favorable outcomes, this analysis seeks to explore the impact of decisions to increase concentration on performance. We used portfolio sorts, standard OLS regression methods, and quantile regression methods to test the effect of portfolio concentration in various ways. We found that concentration has a significant convex relationship with the absolute value of relative returns. That is, as the effective number of holdings increases, its association with positive and negative relative performance diminishes. Additionally, we find that increasing concentration has a pronounced positive impact on performance for outperforming funds; the opposite is true for underperforming funds. Most importantly, higher levels of concentration generally hurt the poorly performing funds more than it helps the outperforming funds. For investors and gatekeepers who focus on holdings-based measures of concentration, the findings show that you can't just look at holdings-based measures of concentration without considering industry exposures.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "58045b142cbc902ea0e6469625ee94bd",
  "timestamp": "2025-05-15T01:05:04.383638"
}