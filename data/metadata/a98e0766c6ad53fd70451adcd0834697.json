{
  "id": 6466,
  "title": "Factors associated with reunification: A longitudinal analysis of long-term foster care",
  "abstract": "Longitudinal analysis and a secondary sample of 411 children were used to examine how child welfare worker engagement with families and parent receipt of needed services shaped the outcomes for children in long-term foster care. The data came from the National Survey of Child and Adolescent Well-Being. Multinomial logistic regression showed reunification to be likeliest for neglected children who had caseworkers deeply involved with their families; whose families needed housing and financial assistance but not domestic violence services, specifically; and who were provided appropriately matched services. Adoption was likeliest for neglected children who had caseworkers deeply involved with their families; whose families needed substance-abuse services but not housing services; whose families had a high risk of re-reporting; whose parents were married; who were White and relatively young; and who had experienced foster care for relatively longer periods. Implications for services and training are discussed. (C) 2010 Elsevier Ltd. All rights reserved.",
  "year": 2010,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a98e0766c6ad53fd70451adcd0834697",
  "timestamp": "2025-05-15T02:58:34.067537"
}