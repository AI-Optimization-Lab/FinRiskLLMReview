{
  "id": 5445,
  "title": "Emergency Saving and Household Hardship",
  "abstract": "Households with limited income and wealth often struggle to access the financial liquidity needed to address unexpected expenses or income drops. Emergency savings can act as form of insurance against such economic shocks and reduce the risk of hardships that influence family wellbeing. Prior research has established that threshold amounts of liquid assets can reduce the risk of economic hardship. This study used a measure of self-reported emergency saving behavior to examine whether households who reported saving for emergencies were less likely to experience subsequent economic hardships in a longitudinal sample of households in disadvantaged neighborhoods from the Annie E. Casey Foundation's Making Connections project. Results across a range of regression models suggest that households who saved for emergencies experienced slightly less overall hardship and were less likely to report several specific hardships, such as food insecurity and having a phone disconnected, three years later. This study supports the idea that small, unrestricted savings may play a protective role for low-income households.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "ad002ab0b3a786e9f5d4f1f09a0133c5",
  "timestamp": "2025-05-15T02:47:43.047690"
}