{
  "id": 2312,
  "title": "A retail investor in a cobweb of social networks",
  "abstract": "In this study, using AI, we empirically examine the irrational behaviour, specifically attentiondriven trading and emotion-driven trading such as consensus trading, of retail investors in an emerging stock market. We used a neural network to assess the tone of messages on social media platforms and proposed a novel Hype indicator that integrates metrics of investor attention and sentiment. The sample of messages, which are written in Russian with slang expressions, was retrieved from a unique dataset of social network communication of investors in the Russian stock market. Applying different portfolio designs, we evaluated the effectiveness of the new Hype indicator against the factors of momentum, volatility, and trading volume. We found the possibility of building a profitable trading strategy based on the Hype indicator over a 6-month time horizon. Over short periods, the Hype indicator allows investors to earn more by buying stocks of large companies, and over << longer >> periods, this indicator tends to perform better for illiquid stocks of small companies. As consensus trading tends to produce negative returns, the investment strategy of `Go against the crowd' proves rewarding in the medium term of 3 months.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6915b8a930896a228b79f7741a31af86",
  "timestamp": "2025-05-15T01:05:39.627331"
}