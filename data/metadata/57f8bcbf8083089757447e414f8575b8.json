{
  "id": 182,
  "title": "DGIC: A Distributed Graph Inference Computing Framework Suitable For Encoder-Decoder GNN",
  "abstract": "*A graph is a structure that can express the relationship between objects. The emergence of GNN enables deep learning to be applied in the field of graphs. However, most GNNs are trained offline and cannot be directly used in real-time monitoring scenarios such as financial risk control. In addition, due to the large scale of graph data, a single machine often cannot meet actual needs, and there are bottlenecks such as throughput performance. Therefore, we propose a distributed graph inference computing framework, which can be applied to Encoder-Decoder GNN models. We complete the adaptation of the model by disassembling the graph data and using the extension storage and dynamic invocation mechanism to solve the model invocation problem. For inference performance, we implement dynamic graph construction through incremental composition and decouple the inference process to apply to different scenarios, so that GNNs conforming to the Encoder-Decoder style can be applied to the framework. A large number of experiments show that this method has good timeliness while improving the throughput upper limit, and can maintain the model effect of multi-tasking.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "57f8bcbf8083089757447e414f8575b8",
  "timestamp": "2025-05-15T01:35:04.409689"
}