{
  "id": 3626,
  "title": "Modeling default risk via a hidden Markov model of multiple sequences",
  "abstract": "Default risk in commercial lending is one of the major concerns of the creditors. In this article, we introduce a new hidden Markov model (HMM) with multiple observable sequences (MHMM), assuming that all the observable sequences are driven by a common hidden sequence, and utilize it to analyze default data in a network of sectors. Efficient estimation method is then adopted to estimate the model parameters. To further illustrate the advantages of MHMM, we compare the hidden risk state process obtained by MHMM with that from the traditional HMMs using credit default data. We then consider two applications of our MHMM. The calculation of two important risk measures: Value-at-risk (VaR) and expected shortfall (ES) and the prediction of global risk state. We first compare the performance of MHMM and HMM in the calculation of VaR and ES in a portfolio of default-prone bonds. A logistic regression model is then considered for the prediction of global economic risk using our MHMM with default data. Numerical results indicate our model is effective for both applications.",
  "year": 2010,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "215af22719516a7bbc2f910ba5f227a2",
  "timestamp": "2025-05-15T01:19:00.764937"
}