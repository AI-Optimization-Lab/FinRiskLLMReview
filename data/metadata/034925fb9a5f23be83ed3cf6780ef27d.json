{
  "id": 32,
  "title": "Using AI to assess corporate climate transition disclosures",
  "abstract": "Company transition plans toward a low-carbon economy are key for effective capital allocation and risk management. This paper proposes a set of 64 indicators to comprehensively assess transition plans and develops a Large Language Model-based tool to automate the assessment of company disclosures. We evaluate our tool with experts from 26 institutions, including financial regulators, investors, and non-governmental organizations. We apply the tool to the sustainability reports from carbon-intensive Climate Action 100+ companies. Our results show that companies tend to disclose more information related to target setting (talk), but less information related to the concrete implementation of strategies (walk). In addition, companies that disclose more information tend to have lower emissions. Our results highlight the need for increased scrutiny of companies' efforts and potential greenwashing risks. The complexity of transition activities presents a major challenge for comprehensive large-scale assessments. As shown in this paper, novel and flexible approaches using Large Language Models can serve as a remedy.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "LLMs",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "034925fb9a5f23be83ed3cf6780ef27d",
  "timestamp": "2025-05-15T01:45:16.408596"
}