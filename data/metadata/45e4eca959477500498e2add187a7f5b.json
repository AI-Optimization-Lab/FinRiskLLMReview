{
  "id": 4976,
  "title": "Research on Classification Method of Bank Statement in Low Resource and Cross-domain Scenarios",
  "abstract": "Bank statement classification is of great value in the field of risk control and marketing in the financial industry, but the lack of annotated samples, cumbersome remark information, and large differences among different industries pose great challenges to model training. In this paper, based on the idea of Prompt-based Learning, we propose PDT: Prompt-based Data-to-text Template, which converts structure data into text containing background information, with the purpose of further releasing the capability of the pre-trained model. The experimental results show that the method can effectively solve the model training problem in low-resource and cross-domain scenario, and the model shows better performance under both Bidirectional Language Model (LM) method and Seq-to-Seq LM method compared with the original input.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "45e4eca959477500498e2add187a7f5b",
  "timestamp": "2025-05-15T02:43:06.715978"
}