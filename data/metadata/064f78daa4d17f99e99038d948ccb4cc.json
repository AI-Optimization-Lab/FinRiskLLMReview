{
  "id": 5745,
  "title": "Policy implications of the Federal Reserve study of credit risk models at major US banking institutions",
  "abstract": "The current regulatory capital standard for banks - the Basle Accord - is a lose/lose proposition. Regulators cannot conclude that a bank with a nominally high regulatory capital ratio has a correspondingly low probability of insolvency. On the other hand, because the Accord often levies a capital charge out of proportion to the true economic risk of a position, banks must engage in regulatory capital arbitrage (or exit their low risk business lines). Since such arbitrage is costly, the capital regulations keep banks from maximizing the value of the financial firm. Regulators need to answer three questions: (1) What are the goals of prudential regulation and supervision? (2) How should bank soundness be defined and quantified? (3) At what level should a minimum soundness standard be set in order to meet the (perhaps conflicting) goals of prudential regulation and supervision? possible answers to these questions are attempted, then the paper analyzes the two leading proposals for rationalizing the Accord - a modified-Basle (or ratings-based) approach and a full-models approach. (C) 2000 Elsevier Science B.V. All rights reserved. JEL classification: G2; G18; G28.",
  "year": 2000,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "064f78daa4d17f99e99038d948ccb4cc",
  "timestamp": "2025-05-15T02:50:53.951901"
}