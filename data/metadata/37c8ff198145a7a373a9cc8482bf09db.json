{
  "id": 2737,
  "title": "A module generation algorithm for product architecture based on component interactions and strategic drivers",
  "abstract": "Benefits of modularity are often achieved from module independence that allows for independent development to reduce overall lead time and economies of scale due to sharing similar modules across products in a product family. Current modularity methods tend to describe only one of these views, either the module-module independence or the product-product shared module similarity. This paper proposes a new hybrid module generation algorithm that balances both module independence and product similarity, allowing product similarity strategy to influence the coupling-driven architecture considerations. The proposed method builds on two popular matrix-based methods: the design structure matrix approach and modular function deployment that each has been developed to support these two different aspects of the module generation. This paper presents a novel algorithm that integrates both views and allows a balanced clustering that takes both interactions and company portfolio strategy into account. Usefulness of the algorithm is presented using a cordless handheld vacuum cleaner as a case study and by comparing it to alternative approaches.",
  "year": 2014,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "37c8ff198145a7a373a9cc8482bf09db",
  "timestamp": "2025-05-15T01:09:43.413731"
}