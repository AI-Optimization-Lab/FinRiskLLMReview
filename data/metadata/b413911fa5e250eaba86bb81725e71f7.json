{
  "id": 1113,
  "title": "A LEARNING CONTRACT FOR THE SUBJECT TIME SERIES FOLLOWING THE BOLONIA CRITERIA",
  "abstract": "The new framework of the Superior European Education System involves the search for novel teaching approaches which improve the quality of the present university teaching. One of these emergent methodologies is the Learning Contract. This is a special agreement established between the student and the teacher which contains the activities and results that the students have to get through supervised learning, as well as mutual expectations and responsibilities related to the learning process. In this way, the Learning Contract has a double purpose. On the one hand, it is an evaluation system of the personal progress of the student and on the other, it is an excellent instrument to motivate the student and make him responsible in his university period. Recently, we have created a Portfolio for the subject Time Series of the third course of Statistics Degree as an evaluation system. We think that the design of a Learning Contract associated with this Portfolio would increase the teaching quality of the subject in that this would help the students to understand their role in the overall learning process. So, in the present communication we develop a specific Learning Contract for the subject Time Series.",
  "year": 2010,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b413911fa5e250eaba86bb81725e71f7",
  "timestamp": "2025-05-15T00:52:00.351450"
}