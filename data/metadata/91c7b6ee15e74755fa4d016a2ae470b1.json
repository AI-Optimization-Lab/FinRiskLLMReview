{
  "id": 2379,
  "title": "Multi-Factor Models and Gradient Boosting for Improved Factor Selection",
  "abstract": "Multi-factor models are widely applied for explaining and predicting financial asset returns. Conventionally, methods like the Fama-MacBeth regression are employed to test factor validity, yet they overlook the potential heterogeneity in factor impacts across different assets. This paper proposes an improved approach by leveraging CatBoost and LightGBM to classify differentiate scenarios where factors are effective or ineffective, aiming to enhance the accuracy of factor selection. Using data from macro funds spanning 2018 to 2023, we first compare the performance of investment portfolios constructed without factor screening against those subjected to the Fama-MacBeth test. The results reveal instability in the traditional method. Subsequently, we train CatBoost and LightGBM, and construct portfolios by retaining funds exhibiting valid factor influences as predicted by the models. The improved method yields higher average portfolio returns but also increased return volatility. Further analysis indicates that the performance of our method is closely tied to the prediction accuracy of the models and the degree of matching between the training and test data distributions. When these align, our improved method can achieve considerable returns; conversely, a mismatch leads to extreme portfolio return fluctuations. These findings highlight the importance of enhancing model generalization capabilities and optimizing portfolio construction approaches for future research.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "91c7b6ee15e74755fa4d016a2ae470b1",
  "timestamp": "2025-05-15T01:06:10.794532"
}