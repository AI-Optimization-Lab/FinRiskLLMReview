{
  "id": 997,
  "title": "Aggregating exponential gradient expert advice for online portfolio selection under transaction costs",
  "abstract": "As an application of machine learning techniques in the field of portfolio management, online portfolio selection (OLPS) aims at optimising the allocation of wealth in an uncertain environment. When making investment decisions, the transaction cost is such an important factor that investor should not ignore. Thus, this paper extends an existing online portfolio selection strategy Continuous Aggregating Exponential Gradient (CAEG) (Yang et al., 2022) in the presence of transaction costs. The proportional transaction costs model is constructed when the transaction costs are incorporated into the decision-making process, and we call this new strategy CAEG(C) . Theoretical guarantee proves that the mean of the logarithmic cumulative wealth of CAEG(C) has an asymptotic upper bound with that of its benchmark. The numerical examples demonstrate the impact of transaction costs on the proposed CAEG(C) strategy on the one hand, and on the other hand, verify that CAEG(C) outperforms other related OLPS strategies and is comparable to its benchmark strategy.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9c67c64369eb524db1db89bc4ae15009",
  "timestamp": "2025-05-15T00:50:40.739388"
}