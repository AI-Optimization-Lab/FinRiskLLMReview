{
  "id": 5013,
  "title": "An Empirical Investigation of Value at Risk (VaR) Forecasting Based on Range- Based Conditional Models",
  "abstract": "Value at Risk (VaR) is a widely used measure of market risk. Precision in the estimation of volatility leads to accurate VaR forecasts. As volatility is time-varying and has a clustering effect, GARCH class of volatility models is helpful in modeling volatility more precisely. Studies have also shown that range-based volatility estimates are more efficient than traditional models that use only closing prices. Therefore, this study uses the GARCH family of volatility models to model and forecast VaR. The study compares conventional models that use closing prices alone, like GARCH and TARCH, with range-based models like RGARCH and RTARCH models, where the range defined as daily high price minus low price is introduced as an exogenous variable, to explore if the latter provides better predictive accuracy. All the models are back-tested using the Kupiec (1995) unconditional coverage and Christoffersen (1998) conditional coverage tests. The data period in the study ranges from 2003-2021, and we consider five BRICS indices and three major developed economies, namely, the USA, the UK, and Germany. An empirical investigation shows that range-based models do a better job in VaR forecasting as it has more information content than daily closing prices, thereby giving more accurate VaR estimates. The study hopes this finding will greatly help stakeholders like financial institutions, regulators, and practitioners in more effective market risk",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "93e0f1ac1b563dae911138d29e8cea86",
  "timestamp": "2025-05-15T02:43:40.082826"
}