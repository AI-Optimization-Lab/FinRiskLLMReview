{
  "id": 1267,
  "title": "Trading futures spread portfolios: applications of higher order and recurrent networks",
  "abstract": "This paper investigates the modelling and trading of oil futures spreads in the context of a portfolio of contracts. A portfolio of six spreads is constructed and each spread forecasted using a variety of modelling techniques, namely, a cointegration fair value model and three different types of neural network ( NN), such as multi-layer perceptron ( MLP), recurrent, and higher order NN models. In addition, a number of trading filters are employed to further improve the trading statistics of the models. Three different filters are optimized on an in-sample measure of down side risk-adjusted return, and these are then fixed out-of-sample. The filters employed are the threshold filter, correlation filter, and the transitive filter. The results show that the best in-sample model is the MLP with a transitive filter. This model is the best performer out-of-sample and also returns good out-of-sample statistics.",
  "year": 2008,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "16250cdabe6b19049ab26507e65df918",
  "timestamp": "2025-05-15T00:53:42.155605"
}