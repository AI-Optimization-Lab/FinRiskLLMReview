{
  "id": 4568,
  "title": "Comparative Study on Logit and BGEVA Models in Assessing Corporate Default Risk: Based on Large Data of American Listed Companies",
  "abstract": "Using the large financial data of 5926 American listed companies spanning from 1990 to 2015, this paper compares the performance of logit model and binary generalized extreme value additive (BGEVA) model in assessing corporate default risk, during which, only four regression variables including trailing S&P 500 return, US interest rate, firm's trailing stock return and firm's distance to default are used. By the comparisons between the estimated cumulative default values with the actual ones per year, we find that BGEVA model performs better in default estimation. Furthermore, along with the decrease of default number, logit model could underestimate the default risk, while BGEVA model maintains a relatively robust estimate, showing an obvious advantage in assessing default of rare events.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "eab0b6fa85ca58b97fd5ed4e5e9c407e",
  "timestamp": "2025-05-15T02:38:56.188352"
}