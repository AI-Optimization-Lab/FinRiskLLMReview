{
  "id": 209,
  "title": "政府引导基金研究述评与建模展望——基于文献计量与投资组合的视角",
  "abstract": "对国内外政府引导基金的发展研究进行系统性综述,从设立动机、发展模式和投资结果三方面进行梳理,运用CiteSpace对检索到的国内外文献进行比较分析。研究结果表明:国内关于政府引导基金的研究在文献数量、共现强度等方面均与国外存在一定差异,尚未形成明确的研究聚类;目前政府引导基金的研究热点和前沿已从数量规模分析转为基金质量绩效评价,特别是对基金质量的多角度综合绩效考量。基于此,进一步对综合考虑政府引导基金多种绩效的前沿研究方向进行展望分析,通过构建多目标投资组合选择模型,为政府引导基金提供综合考虑经济绩效、创新绩效和社会绩效的投资决策机制,以解决社会资本和政府资本投资目标不一致而产生的矛盾,进而达到吸引社会资本、优化基金结构、提高综合投资效益的目的。",
  "year": 2020,
  "source": "CNKI",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "607f20336563cbdfe9eb3933c18454b6",
  "timestamp": "2025-05-14T22:21:50.503263"
}