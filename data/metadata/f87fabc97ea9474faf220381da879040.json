{
  "id": 1384,
  "title": "A New Time-Aware LSTM based Framework for Multi-label Classification on Healthcare Data",
  "abstract": "Medical prevention is a very important aspect of healthcare informatics research through the prediction of medical events (e.g., disease diagnosis). In this work, we propose a deep learning approach to perform multi-label prediction on acts of medical care and treatments. The proposed approach utilizes a time-aware long short-term memory network and extends it with additional information from a fuzzy clustering of the same portfolio. The former mechanism (time-aware) is used to handle the temporal irregularity between the elements of a medical trajectory whereas the latter mechanism (fuzzy clustering) assists in modeling the heterogeneity among patients and treatments. Using a large portfolio of reimbursed medical records (over 16 million consumed acts of medical care) by a healthcare insurance in France, we show that our approach outperforms traditional and deep learning methods in medical multi-label prediction. Our work has implications for supporting medical prevention and more broadly improving the quality of healthcare services and insurance.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f87fabc97ea9474faf220381da879040",
  "timestamp": "2025-05-15T00:54:56.552130"
}