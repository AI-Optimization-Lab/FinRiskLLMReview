{
  "id": 3995,
  "title": "The pediatric surgeon-scientist: An evolving breed or endangered phenotype?",
  "abstract": "Introduction: National Institute of Health (NIH) funding is a gold-standard of achievement; we examined trends in NIH-funded pediatric surgeons. Methods: NIH Research Portfolio Online Reporting Tools (RePORT) was queried for American Pediatric Surgical Association (APSA) members (2012 vs 2022). Demographics and time-to-award (TTA) from fellowship were compared. Number of grants, funding allotment, award classification, administering institutes/centers, research type were studied. Results: Thirty-eight (4.6%) APSA members were NIH-funded in 2012 compared to 37 (2.9%) in 2022. Of funded surgeons in 2022, 27% were repeat awardees from 2012. TTA was similar (12 vs 14years, p=0.109). At each point, awards were commonly R01 grants (40 vs 52%, p = 0.087) and basic science-related (76 vs 63%, p = 0.179). Awardees were predominantly men (82% in 2012 vs 78% in 2022, p=0.779) and White (82% in 2012 vs 76% in 2022, p=0.586). Median amount per grant increased: $254,980 (2012) to $364,025 (2022); by $96,711 for men and $390,911 for women. Median awards for White surgeons increased by $215,699 (p=0.035), and decreased by $30,074 for non-White surgeons, though not significantly (p=0.368). Conclusion: The landscape of NIH-funded pediatric surgeons has remained unchanged between time points. With a substantial number of repeat awardees, predominance of R01 grants, and a median TTA over a decade after fellowship graduation, the phenotypes of early career pediatric surgeon-scientists are facing academic endangerment.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "293648e8e35c7ffaaff2313e110625e2",
  "timestamp": "2025-05-15T01:23:07.239486"
}