{
  "id": 1076,
  "title": "Explainable Machine Learning Models Assessing Lending Risk",
  "abstract": "This work aims to assess the credit risk of a financial institution using various machine learning models that are both powerful and interpretable. To meet the challenge posed by imbalanced data, different sampling techniques were tested with machine learning algorithms such as Random Forest (RF) and XGBoost. The results revealed that the RF model combined with Randomoversampler and the XGB model with SMOTEENN offered the best performance in terms of precision, recall, and F1-score. To ensure the comprehensibility of the decisions made by these high-performance models, interpretability techniques such as SHAP and LIME were applied, revealing the relative contributions of the different variables. The study shows that these results are useful for the responsible deployment of decision support models in the financial sector.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "887f903fdf12e7bc3e01f198a0839c85",
  "timestamp": "2025-05-15T01:58:43.359891"
}