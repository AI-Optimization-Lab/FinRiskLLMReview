{
  "id": 6993,
  "title": "Factors associated with non-completion of TB preventive treatment in Brazil",
  "abstract": "BACKGROUND: Among Brazilian initiatives to scale up TB preventive therapy (TPT) are the adoption of the 3HP regimen (12 weekly doses of rifapentine and isoniazid [INH]) in 2021 and the implementation in 2018 of the TPT surveillance information system. Since then, 63% of the 76,000 eligible individuals notified completed TPT. Recommended regimens in this period were 6H, 9H (6 or 9 months of INH) and 4R (4 months of rifampicin).OBJECTIVE: To analyse the factors associated with TPT non-completion.METHODS: We analysed the cohort of TPT notifica-tions from 2018 to 2020. Robust variance Poisson regression model was used to verify the association of TPT non-completion with sociodemographic, clinical and epidemiological variables. RESULTS: Of the 39,973 TPT notified in the study period, 8,534 (21.5%) were non-completed, of which 7,858 (92.1%) were lost to follow-up. Age 15-60 years (relative risk [RR] 1.27, 95% confidence interval [95% CI] 1.20- 1.35), TPT with isoniazid (RR 1.40, 95% CI 1.19-1.64) and Black/mixed race (RR 1.17, 95% CI 1.09-1.25) were associated with a higher risk of non-completion.CONCLUSION: Individuals in situations of social and financial vulnerability such as being Black/pardo race, younger and on longer TPT regimens were more likely to be associated with TPT incompletion.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "998f0dc8a8914545568fc4812d146f3c",
  "timestamp": "2025-05-15T03:03:55.805598"
}