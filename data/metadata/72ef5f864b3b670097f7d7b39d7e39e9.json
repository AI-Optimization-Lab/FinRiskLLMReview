{
  "id": 963,
  "title": "Portfolio Optimization Using a Novel Data-Driven EWMA Covariance Model with Big Data",
  "abstract": "Recently there has been a growing interest in using machine learning methods with empirical variance covariance matrix of returns to study Markovitz portfolio optimization. The statistical technique of graphical LASSO (GL) for stock selection in the portfolio assumes that the asset returns are normally distributed, independent random variables with constant variance. In this paper sign correlations and the autocorrelations of the absolute values of the returns are used to show that the returns are non-normal with time-varying volatility. We use the recently proposed data-driven exponentially weighted moving average (DDEWMA) volatility model to estimate the covariance matrix of asset returns in Markowitz portfolio optimization. Empirical results with big data (consists of 444 stocks for a period of 7 years downloaded from Yahoo Finance) show that the proposed DDEWMA variance covariance matrix model outperforms (larger Sharpe ratio) the model with empirical variance covariance matrix.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "72ef5f864b3b670097f7d7b39d7e39e9",
  "timestamp": "2025-05-15T00:50:08.745007"
}