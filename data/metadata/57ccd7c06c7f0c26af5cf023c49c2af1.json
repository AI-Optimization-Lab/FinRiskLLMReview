{
  "id": 211,
  "title": "Investment Biases in Reinforcement Learning-based Financial Portfolio Management",
  "abstract": "As one of the most applicable problems in reinforcement learning (RL), a number of RL-based system designs for financial portfolio management (PM) have been proposed by researchers, which are proven to achieve outstanding performance in terms of capital returns. However, these systems' degrees of bias proxies in investment are often ignored and rarely inspected in the existing research. A high-yield RL-based system does not sufficiently indicate it is unbiased. The system might as well display the biases which humans may have during financial trading owing to the resemblant decision-making and rewarding mechanism between human investors and RL agents. In this study, we investigate the proxies of two biases in financial investment, disposition effect (DE) and narrow framing (NF), in a cutting-edge RL-based system for PM: MSPM. The experimental results on 135 different portfolios during the year 2021 indicate that MSPM has significantly lower degrees of DE (-0.1477) and NF (0.0904) compared to human investors. We confirm the RL-based system's capacity to overcome the biases which human investors often have in financial investment.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "57ccd7c06c7f0c26af5cf023c49c2af1",
  "timestamp": "2025-05-15T00:40:34.147278"
}