{
  "id": 3310,
  "title": "Non-Gaussian models for CoVaR estimation",
  "abstract": "In this paper we show how to obtain estimates of CoVaR based on models that take into consideration some stylized facts about multivariate financial time series of equity log returns: heavy tails, negative skew, asymmetric dependence, and volatility clustering. While the volatility clustering effect is captured by AR-GARCH dynamics of the GlostenJagannathan-Runkle (GJR) type, the other stylized facts are explained by non-Gaussian multivariate models and copula functions. We compare the different models in the period from January 2007 to March 2020. Our empirical study conducted on a sample of listed banks in the euro area confirms that, in measuring CoVaR, it is important to capture the time-varying dynamics of the volatility. Additionally, a correct assessment of the heaviness of the tails and of the dependence structure is needed in the evaluation of this systemic risk measure. (c) 2021 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "11fb68f82d753c2a6360a7189aaf8c29",
  "timestamp": "2025-05-15T02:24:39.772821"
}