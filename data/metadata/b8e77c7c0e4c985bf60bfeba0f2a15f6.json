{
  "id": 2742,
  "title": "Who Became Victims of Financial Frauds during the COVID-19 Pandemic in Japan?",
  "abstract": "The COVID-19 pandemic has provided a unique opportunity for fraudsters to innovatively swindle money through the trade of necessary goods and services. Although several incidents of financial fraud were reported during the pandemic, there is a lack of studies comparing financial frauds before and during the pandemic and the risk factors associated with frauds. This study uses two waves of a panel survey conducted before and during the pandemic and applies mean comparison tests and logit regressions to investigate financial frauds at the aggregate and specific levels. The comparative analysis shows no significant change in financial frauds at the aggregate level between before and during the pandemic. However, refund frauds for men have increased, while loan guarantee frauds for women have decreased significantly during the pandemic. The regression results show that being male, younger in age, living with family, having employment status, having a household income, household assets, having financial literacy, having a myopic view of the future, and having careful buying habits are associated with the probability of being victims of financial frauds during the pandemic. The study reveals differences in risk factors associated with victims of financial frauds at the aggregate and specific levels. The results further imply that risk factors differ across the types of fraud, which authorities should consider while combating financial frauds.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "b8e77c7c0e4c985bf60bfeba0f2a15f6",
  "timestamp": "2025-05-15T02:18:37.176893"
}