{
  "id": 6612,
  "title": "An application of sensitivity analysis to hedge funds",
  "abstract": "The present study investigates a sample of 142 live hedge funds via a data envelopment analysis (DEA) sensitivity analysis using a super-efficiency model. We use data from Barclay Hedge to examine the sensitivity of six inputs (ie, the standard deviation of monthly returns, management and performance fees, leverage, number of employees and portfolio net exposure). We emphasize in particular the impact of the net portfolio exposure and the number of employees, which have not been considered before. The sensitivity analysis model analyzes the stability of hedge funds as a result of changes in inputs. DEA will identify which hedge funds have more efficient operations, and the sensitivity analysis will show which inputs are sensitive to the results of a fund's operations. The evidence in the paper shows that, with the exception of the sensitivity to leverage, only 42% of all funds achieve efficiency status. The rest appear inefficient. When leverage is considered, approximately 75% of all funds are efficient or super-efficient. Using regression analysis, the study provides a further in-depth analysis of the factors that affect the efficiency status with respect to each input. The results in this paper have implications for financial risk management and risk model validation.",
  "year": 2016,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f3f41c5f6ad62836de315f31c3d0df42",
  "timestamp": "2025-05-15T02:59:37.104816"
}