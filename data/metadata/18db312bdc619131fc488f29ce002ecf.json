{
  "id": 3846,
  "title": "Get rich quick, scheme or script? The effect of cryptoculture on the susceptibility of fraud victimization among cryptocurrency purchasers",
  "abstract": "Purpose: Despite the precipitous rise of cryptocurrency in recent years, little is known regarding the utility and legitimacy of such fiat currencies. This research assesses whether or not a cryptoculture exists, and if so, its influence on fraud and/or cryptocurrency loss. Methods: Logistic Regression was used to examine the relationship between cryptoculture and cryptocurrency loss or fraud in a sample of 919 survey respondents who previously purchased cryptocurrency. Results: Results support the existence of a cryptoculture. Respondents who reported higher levels of confidence in cryptocurrency were significantly more likely to also report fraud or loss. This was despite also admitting to feeling more vulnerable when purchasing cryptocurrency. These seemingly conflicting results may actually point to a false sense of confidence by cryptopurchasers driven by cryptoculture, which encourages purchasing decisions to outweigh initial skepticism or distrust. Conclusions: Underlying mechanisms of cryptocurrency purchasing decisions (like cryptoculture) should continue to be examined in greater depth. Over one-third of cryptocurrency purchasers reported that at least some of their cryptocurrency portfolio is missing, yet many remain confident in their ability to navigate the market. This study provides a nuanced analysis of the factors influencing how victims of fraud may be influenced to participate in risky purchases.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "18db312bdc619131fc488f29ce002ecf",
  "timestamp": "2025-05-15T01:21:41.256675"
}