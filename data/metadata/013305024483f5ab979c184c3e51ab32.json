{
  "id": 185,
  "title": "Fourier transform based LSTM stock prediction model under oil shocks",
  "abstract": "This paper analyses the impact of various oil shocks on the stock volatility prediction by using a Fourier transform-based Long Short-Term Memory (LSTM) model. Oil shocks are decomposed into five components following individual oil price change indicators. By employing a daily dataset involving S&P 500 stock index and WTI oil futures contract, our results show that different oil shocks exert varied impacts on the dynamics of stock price volatility by using gradient descent. Having exploited the role of oil shocks, we further find that the Fourier transform-based LSTM technique improves forecasting accuracy of the stock volatility dynamics from both statistical and economic perspectives. Additional analyses reassure the robustness of our findings. Clear comprehension of the future stock market dynamics possesses important implications for sensible financial risk management.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "013305024483f5ab979c184c3e51ab32",
  "timestamp": "2025-05-15T01:35:04.429686"
}