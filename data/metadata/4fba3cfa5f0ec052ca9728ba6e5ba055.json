{
  "id": 1021,
  "title": "An improved historical simulation method to estimate the amount of refined oil retail value at risk VaR",
  "abstract": "The international crude oil market is complicated in itself and with the rapid development of China in recent years, the dramatic changes of the international crude oil market have brought some risk to the security of China's oil market and the economic development of China. Value at risk (VaR), an effective measurement of financial risk, can be used to assess the risk of refined oil retail sales as well. However, VaR, as a model that can be applied to complicated nonlinear data, has not yet been widely researched. Therefore, an improved Historical Simulation Approach, historical stimulation of genetic algorithm to parameters selection of support vector machine, HSGA-SVMF, in this paper, is proposed, which is based on an approach the historical simulation with ARMA forecasts, HSAF. By comparing it with the HSAF and HSGA-SVMF approach, this paper gives evidence to show that HSGA-SVMF has a more effective forecasting power in the field of amount of refined oil.",
  "year": 2013,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4fba3cfa5f0ec052ca9728ba6e5ba055",
  "timestamp": "2025-05-15T01:58:08.443696"
}