{
  "id": 711,
  "title": "Stock Price Prediction Using GRU, SimpleRNN and LSTM",
  "abstract": "In today's era, stock prediction has become one of the dominant real world application. Most of the times, scientists attempted to establish a direct connection between information macroeconomic factors and stock returns; however, with the revelation of nonlinear slants in financial exchange record returns, there has been a significant shift in the scientists' focus toward the nonlinear expectation of stock returns. Even though various articles on nonlinear measurable presenting of stock returns have appeared since then, the huge demand is the nonlinear model which is specified before the estimation is performed. Predicting stock value is a difficult task that necessitates a solid algorithmic structure to calculate returns. Because stock prices are volatile and depend on the market up and down, forecasting stock prices becomes difficult. It has never been easy to invest in a portfolio of assets; the abnormalities of the financial market prevent simple models from accurately predicting future asset values. Machine learning, which is teaching computers to execute activities that would ordinarily need human intelligence, is the current scientific study hot topic. This paper explores gated recurrent units (GRU), simple recurrent neural network (Simple RNN) and long short term memory (LSTM) models for stock price prediction.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "246fcf796ee71c31b552889e9e6396a6",
  "timestamp": "2025-05-15T00:47:18.524088"
}