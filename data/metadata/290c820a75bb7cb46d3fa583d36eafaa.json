{
  "id": 1457,
  "title": "Jump Aggregation, Volatility Prediction, and Nonlinear Estimation of Banks' Sustainability Risk",
  "abstract": "Extreme financial events usually lead to sharp jumps in stock prices and volatilities. In addition, jump clustering and stock price correlations contribute to the risk amplification acceleration mechanism during the crisis. In this paper, four Jump-GARCH models are used to forecast the jump diffusion volatility, which is used as the risk factor. The linear and asymmetric nonlinear effects are considered, and the value at risk of banks is estimated by support vector quantile regression. There are three main findings. First, in terms of the volatility process of bank stock price, the Jump Diffusion GARCH model is better than the Continuous Diffusion GARCH model, and the discrete jump volatility is significant. Secondly, due to the difference of the sensitivity of abnormal information shock, the jump behavior of bank stock price is heterogeneous. Moreover, CJ-GARCH models are suitable for most banks, while ARJI-R2-GARCH models are more suitable for small and medium sized banks. Thirdly, based on the jump diffusion volatility information, the performance of the support vector quantile regression is better than that of the parametric quantile regression and nonparametric quantile regression.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "290c820a75bb7cb46d3fa583d36eafaa",
  "timestamp": "2025-05-15T02:03:00.818727"
}