{
  "id": 3536,
  "title": "Extreme connectedness between NFTs and US equity market: A sectoral analysis",
  "abstract": "This study examines the returns connectedness between NFTs and US sector stock markets. For this purpose, we use the recently developed technique of quantile-based regression to explore the dependence structure under various conditions. Our results support the view that connectedness between NFTs and sectoral markets is characterized by asymmetry and heterogeneity in the extreme conditions compared to the median quantile and mean-based approach. Under normal conditions, all NFTs except the ENJ are net recipients of the return spillover from the sectoral stock markets, whereas, financial, consumer staple, and industrial stocks are major net transmitters to the system. However, we observe heterogeneity at both tails, as all NFTs act as net transmitter (recipient) at the higher (lower) quantiles. This confirms the asymmetric dependence structure. We also compute the static optimal weights and hedge ratios using TVP-VAR model for the stocks/NFTs portfolios and show that investors and portfolio managers may consider including NFTs in their holdings to achieve diversification benefits.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4ec05626604a3ec2352eb7a6ec6e3828",
  "timestamp": "2025-05-15T01:17:54.234011"
}