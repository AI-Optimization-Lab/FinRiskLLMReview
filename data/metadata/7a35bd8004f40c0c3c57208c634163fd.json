{
  "id": 3649,
  "title": "Cryptocurrencies, Diversification and the COVID-19 Pandemic",
  "abstract": "This paper features an analysis of cryptocurrencies and the impact of the COVID-19 pandemic on their effectiveness as a portfolio diversification tool and explores the correlations between the continuously compounded returns on Bitcoin, Ethereum and the S&P500 Index using a variety of parametric and non-parametric techniques. These methods include linear standard metrics such as the application of ordinary least squares regression (OLS) and the Pearson, Spearman and Kendall's tau measures of association. In addition, non-linear, non-parametric measures such as the Generalised Measure of Correlation (GMC) and non-parametric copula estimates are applied. The results across this range of measures are consistent. The metrics suggest that, whilst the shock of the COVID-19 pandemic does not appear to have increased the correlations between the cryptocurrency series, it appears to have increased the correlations between the returns on cryptocurrencies and those on the S&P500 Index. This suggests that investments in cryptocurrencies are not likely to offer key diversification strategies in times of crisis, on the basis of evidence provided by this crisis.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7a35bd8004f40c0c3c57208c634163fd",
  "timestamp": "2025-05-15T01:19:26.128242"
}