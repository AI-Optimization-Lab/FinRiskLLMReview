{
  "id": 3202,
  "title": "A return spillover network perspective analysis of Chinese financial institutions' systemic importance",
  "abstract": "The objective of this study is to analyze systemic importance of Chinese financial institutions and its influential factors based on return spillover network. We first investigate the return spillover effects among financial institutions and construct the return spillover networks by Granger causality in vector autoregressive (VAR) models. Then we calculate six network centralities (degree centrality, closeness centrality, betweenness centrality, modified Katz centrality, eccentricity centrality and information centrality) to measure systemic importance of financial institutions. Because different centrality measures are correlated with each other, we use the principal component analysis method to obtain comprehensive information about systemic importance of financial institutions. Finally, we identify the major factors, including market and accounting variables, which affect systemic importance of the financial institutions with panel data regression analysis. We find that financial institutions with larger tail risk of stock return, higher return on equity, lower turnover rate and lower assets growth rate tend to be associated with greater systemic importance. (C) 2018 Elsevier B.V. All rights reserved.",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7a0a35291cf1124df37b9e14977e7b79",
  "timestamp": "2025-05-15T02:23:33.595348"
}