{
  "id": 1165,
  "title": "Dynamic Portfolio Optimization Using a Hybrid MLP-HAR Approach",
  "abstract": "Forecasting asset volatility is a common and important application in finance, often used as a measure of the future risk of an investment. With high-frequency data availability in the last few decades, new volatility models have been proposed that make use of realized volatility measures. These models have been empirically shown to provide better performance when compared with classical low-frequency approaches. In this paper, we propose a methodology combining an MLP neural network with heterogeneous autoregressive models for daily forecasting of the covariance matrix of a portfolio and for dynamically allocating optimal weights. Using Cholesky decomposition we guarantee the positive definite estimates. We used real historical data of 5 stocks of the Brazilian market in our experiments and the results demonstrated statistically superior forecasts compared wills the baselines HAR, HARQ, ARFIMA, and DCC, using the Model Confidence Set test. We also evaluate the performance of the portfolios using the matrix estimates from the different models as parameters to Global Minimum Variance Portfolio. Our approach achieved the smallest values of portfolio variance compared with the baselines.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1d5a484fad1ca2bd02fc5ec5935ef237",
  "timestamp": "2025-05-15T00:52:33.862010"
}