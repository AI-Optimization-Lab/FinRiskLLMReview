{
  "id": 3436,
  "title": "Using patent citation patterns to infer innovation market competition",
  "abstract": "We propose an empirical strategy to estimate competition in innovation markets. Our method relates firms' market return on equity to information about patent citation patterns. Two innovations are implemented in the methodology. First is the application of daily abnormal stock returns rather than annual measures of Tobin's q. Second is the creation of citation patterns related to the area of science a firm patents in as represented by the detailed patent classification system. We find that markets positively reward firms when patents are granted. We further find that firm's market value increases when its patent portfolio is cited. We find evidence of competition in innovation markets. The market reacts at the time that the citation occurs and does not anticipate future citations at the time of patenting. Holding this effect constant, we find that citations from patents in the same area of science tend to reduce market value. We interpret these findings as consistent with more citations indicating more valuable intellectual property but citations from competing technologies decreasing it. (C) 2011 Elsevier B.V. All rights reserved.",
  "year": 2011,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9467d7b5ad9ea92aed4f872f15f35553",
  "timestamp": "2025-05-15T01:16:55.613157"
}