{
  "id": 433,
  "title": "Prediction of Unbalanced Financial Risk Based on GRA-TOPSIS and SMOTE-CNN",
  "abstract": "The financial status of an enterprise is related to its healthy and long-term development, and whether the interests of investors and bank loans can be guaranteed. To improve the prediction accuracy of corporate financial risk, this paper proposes a prediction model for corporate financial risk that integrates GRA-TOPSIS and SMOTE-CNN. First, using GRA-TOPSIS to make a comprehensive evaluation of the financial situation of listed companies. Second, the evaluation results are clustered to obtain the scientific level and interval of financial risk, which lays the foundation for the supervised learning of the convolutional neural network. Then, the SMOTE algorithm is introduced to solve the problem of data imbalance of enterprises at all levels, and the focal loss function is used instead of the cross-entropy loss function to further balance the data. Finally, the listed companies in A shares are randomly selected, and experiments were designed to verify the performance of the model built in this paper. The results show that the prediction accuracy of the financial risk prediction model based on GRA-TOPSIS and SMOTE-CNN is 98.57%, which indicates that the model is feasible and has certain reference value.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "df955b9c6d204a1e0198c09c2611f608",
  "timestamp": "2025-05-15T01:50:52.631836"
}