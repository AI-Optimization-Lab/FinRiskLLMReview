{
  "id": 2230,
  "title": "Comparison of Claim Reserves Methods Using Insurance Portfolio Generators",
  "abstract": "Different reserving methods can be used to predict claim values in non-life insurance. This article compares two different methodological approaches to reserving methods, namely, Chain-ladder (the traditional approach to reserving in non-life insurance) and state-space modeling (the modern approach based on recursive Kalman filtering). Moreover, the paper compares both methods with the involvement of clustering which divides claims into several groups according to their similarity and ensures greater homogeneity of data. To be able to compare the accuracy of reserve predictions numerically one suggests three types of generators of large insurance portfolios that represent well the behavior of the given methods in practice (one of them is derived directly from a real Czech non-life insurance claims portfolio). The obtained results may serve as a hint to improve the state-space methodology in order to give comparable results with classical approaches to reserving since in future the state-space modeling will be important for micro reserving where the clustering gains nearly a form of individual policy contracts.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2f283f77a9335d14a370fdb71bf09186",
  "timestamp": "2025-05-15T01:04:27.481935"
}