{
  "id": 1725,
  "title": "Risks in Work-Integrated Learning: A Data-Driven Analysis",
  "abstract": "This study employs advanced data-driven and machine learning techniques to critically assess the integration of Work-Integrated Learning (WIL) into academic programs, with a focus on psychological well-being, financial, and equity and inclusion risks. Using data from the 2018 National Graduates Survey in Canada, the analysis examines how WIL programs influence students' academic and career trajectories, with particular emphasis on identifying key risk factors. The study explores psychological well-being risks associated with academic programs, financial burdens both during and after education, and equity and inclusion risks for institutions. By analysing variables related to work placements, student loans, financial assistance, and the alignment of WIL experiences with students' post-graduation employment, this research provides critical insights into the effectiveness of WIL programs from a large-scale, survey-based, big data perspective. The findings highlight key areas for improvement to mitigate these risks and enhance the overall value of WIL for students across various disciplines.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0104c09a66c0584c77fce4fa09d194fd",
  "timestamp": "2025-05-15T02:06:52.668632"
}