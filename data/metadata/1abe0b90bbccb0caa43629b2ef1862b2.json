{
  "id": 5267,
  "title": "Vulnerability to Fraud in Community Sport Organizations: A Multicountry Study on the Role of Organizational Capacity",
  "abstract": "This study develops and tests a measure for perceived vulnerability to occupational fraud and examines the relationship between organizational capacity and perceived vulnerability to fraud in community sport organizations. Drawing on the opportunity dimension of fraud triangle theory and the concept of organizational capacity, the study identifies a number of risk and protection factors for vulnerability to fraud. Board members of community sport organizations in Australia, Germany, and North America were surveyed (n = 1,256). The results offer a reliable and valid scale assessing vulnerability to fraud in community sport organizations consisting of procedural and financial dimensions. The regression analyses indicate a set of risk factors for vulnerability to fraud, including the presence of paid staff, high annual and unbalanced budgets, and owning sport facilities. Protection factors include strategic planning, relationships with other institutions, and trust within the board. This knowledge can be used to design antifraud education and training resources.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1abe0b90bbccb0caa43629b2ef1862b2",
  "timestamp": "2025-05-15T02:46:16.119430"
}