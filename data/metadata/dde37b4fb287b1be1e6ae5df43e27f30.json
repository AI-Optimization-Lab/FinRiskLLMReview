{
  "id": 5476,
  "title": "A CWGAN-GP-based multi-task learning model for consumer credit scoring",
  "abstract": "In consumer credit scoring practice, there is often an imbalanced distribution in accepted borrowers, whichmeans there are far fewer defaulters than borrowers who pay on time. This makes it difficult for traditionalmodels to function. Aside from traditional sampling methods for imbalanced data, the idea of using rejectedinformation to one's benefit is new. Without historical repayment performance, rejected data are oftendiscarded or simply disposed of during credit scoring modeling. However, these data play an important rolebecause they capture the distribution of the borrower population as well as the accepted data. Besides, dueto the increasing complexity in loan businesses, the current methods have difficulties in addressing high-dimensional multi-source data. Thus, a more effective credit scoring approach towards imbalanced data shouldbe studied. Inspired by the state-of-the-art neural network methods, in this paper, we propose a conditionalWasserstein generative adversarial network with a gradient penalty (CWGAN-GP)-based multi-task learning(MTL) model (CWGAN-GP-MTL) for consumer credit scoring. First, the CWGAN-GP model is employed tolearn about the distribution of the borrower population given both accepted and rejected data. Then, the datadistribution between good and bad borrowers is adjusted through augmenting synthetic bad data generated byCWGAN-GP. Next, we design an MTL framework for both accepted and rejected and good and bad data, whichimproves risk prediction ability through parameter sharing. The proposed model was evaluated on real-worldconsumer loan datasets from a Chinese financial technology company. The empirical results indicate that theproposed model performed better than baseline models across different evaluation metrics, demonstrating itspromising application potential.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "dde37b4fb287b1be1e6ae5df43e27f30",
  "timestamp": "2025-05-15T02:48:23.615888"
}