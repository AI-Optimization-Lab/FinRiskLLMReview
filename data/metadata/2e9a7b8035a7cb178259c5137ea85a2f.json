{
  "id": 2574,
  "title": "Improving EMI Risk Model for E-commerce with Customer Embedding Obtained through Heterogeneous Graph",
  "abstract": "Machine Learning based credit risk models are often based on handcrafted features based on credit history and financial transactions. In this work, we develop a risk model for Equated Monthly Installment (EMI) based loans offered by one of the biggest e-commerce companies in India using two types of features (a) handcrafted features derived from customer actions and account details (b) customer embedding learned from a heterogeneous graph built using their shopping behavior across various product segments to leverage the purchase and co-purchase patterns of the users. Popular algorithms capable of learning powerful node embedding from the graph structure are transductive in nature, making them difficult to use in large graphs. However, we could exploit the special nature of our graph to generalize our embedding learning method for a large customer population. We show that using our graph based embedding along with handcrafted features improves the risk model's performance trained using just the handcrafted features. We also show that the graph embedding are more powerful than features learnt through autoencoder which can capture the purchase patterns of customers but not the co-purchase patterns among them.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "2e9a7b8035a7cb178259c5137ea85a2f",
  "timestamp": "2025-05-15T02:17:01.616704"
}