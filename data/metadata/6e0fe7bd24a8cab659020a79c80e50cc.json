{
  "id": 78,
  "title": "CBI-time-changed Levy processes for multi-currency modeling",
  "abstract": "We develop a stochastic volatility framework for modeling multiple currencies based on CBI-time-changed Levy processes. The proposed framework captures the typical risk characteristics of FX markets and is coherent with the symmetries of FX rates. Moreover, due to the self-exciting behavior of CBI processes, the volatilities of FX rates exhibit self-exciting dynamics. By relying on the theory of affine processes, we show that our approach is analytically tractable and that the model structure is invariant under a suitable class of risk-neutral measures. A semi-closed pricing formula for currency options is obtained by Fourier methods. We propose two calibration methods, also by relying on deep-learning techniques, and show that a simple specification of the model can achieve a good fit to market data on a currency triangle.",
  "year": 2024,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6e0fe7bd24a8cab659020a79c80e50cc",
  "timestamp": "2025-05-15T01:26:55.068654"
}