{
  "id": 59,
  "title": "The commodity risk premium and neural networks",
  "abstract": "The paper uses linear and nonlinear predictive models to study the linkage between a set of 128 macroeconomic and financial predictors and the risk premium of commodity futures contracts. The linear models use shrinkage methods based on either naive averaging or principal components. The nonlinear models use feedforward deep neural networks (DNN) either as stand-alone or in conjunction with a long short-term memory network (LSTM). Out of the four specifications considered, the LSTM-DNN architecture best captures the risk premium, which underscores the need to estimate models that are both nonlinear and recurrent. The superior performance of the LSTM-DNN portfolio persists after accounting for transaction costs or illiquidity and is unrelated to previously-documented commodity risk factors.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0ba2bc337fda3e20b8fd4c2b27ecc463",
  "timestamp": "2025-05-15T01:33:11.380051"
}