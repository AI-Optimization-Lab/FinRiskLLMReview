{
  "id": 4779,
  "title": "Modelling random vectors of dependent risks with different elliptical components",
  "abstract": "In Finance and Actuarial Science, the multivariate elliptical family of distributions is a famous and well-used model for continuous risks. However, it has an essential shortcoming: all its univariate marginal distributions are the same, up to location and scale transformations. For example, all marginals of the multivariate Student's t-distribution, an important member of the elliptical family, have the same number of degrees of freedom. We introduce a new approach to generate a multivariate distribution whose marginals are elliptical random variables, while in general, each of the risks has different elliptical distribution, which is important when dealing with insurance and financial data. The proposal is an alternative to the elliptical copula distribution where, in many cases, it is very difficult to calculate its risk measures and risk capital allocation. We study the main characteristics of the proposed model: characteristic and density functions, expectations, covariance matrices and expectation of the linear regression vector. We calculate important risk measures for the introduced distributions, such as the value at risk and tail value at risk, and the risk capital allocation of the aggregated risks.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "285dfa3110c71d1a8e19169e7258e97f",
  "timestamp": "2025-05-15T02:41:00.818694"
}