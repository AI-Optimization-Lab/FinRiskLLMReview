{
  "id": 1483,
  "title": "Perovskite, the Chameleon CO2 Photocatalyst",
  "abstract": "The prevalence of perovskite materials in myriad technologies is traceable to their diverse compositions, structures, and forms, variations of which bestow them with chameleon-like properties, functionality, and utility. By modifying the ABO(3) archetype perovskites through isomorphic substitution, aliovalent doping, and non-stoichiometry, as well as tailoring their form through nanostructuring, heterostructuring, superstructuring, and polymorphism, the portfolio of application opportunities for perovskite materials can be greatly expanded. The focus of this perspective is to explore the thought process by which human intelligence and experiential learning enables the discovery of a champion photocatalyst for CO2 hydrogenstion by juggling the elements in perovskite oxides and at which point this well-established approach needs a helping hand from artificial intelligence and machine learning.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "bd243293dac31f35c191d0b37913760c",
  "timestamp": "2025-05-15T00:56:08.435160"
}