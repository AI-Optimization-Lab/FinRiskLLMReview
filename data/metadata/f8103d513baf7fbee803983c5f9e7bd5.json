{
  "id": 486,
  "title": "Healthcare fraud detection using primitive sub peer group analysis",
  "abstract": "Healthcare fraud is a significant problem greatly affecting the quality of healthcare services. Manual auditing of insurance claims extends to the delay in finding fraudulent behaviors causing huge financial loss and also putting the patients' health conditions at risk. Since the past decade, the automation of fraud detection using machine learning techniques has become a prominent research topic. Several automated fraud detection systems using machine learning techniques have been proposed so far. However, developing a healthcare fraud detection system that is adaptive to the systematic changes is still missing. Therefore, in this article, we develop primitive sub peer group analysis (PSPGA) for identifying the suspicious behaviors in health insurance claims. PSPGA is inspired by peer group analysis, a popular unsupervised learning technique, which identifies suspicious behaviors based on local pattern analysis. PSPGA distinguishes between the concept drifts and the sudden drifts and flags the sudden drifts as fraudulent. Moreover, PSPGA makes the fraud detection system adaptive to the concept drifts by considering the updates for peer groups over time.",
  "year": 2021,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f8103d513baf7fbee803983c5f9e7bd5",
  "timestamp": "2025-05-15T01:51:31.278938"
}