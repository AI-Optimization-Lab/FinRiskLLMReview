{
  "id": 928,
  "title": "End-to-end deep learning with neuromorphic photonics",
  "abstract": "Neuromorphic computing has emerged as a highly-promising compute alternative, migrating from von-Neuman architectures towards mimicking the human brain for sustaining computational power increases within a reduced power consumption envelope. Electronic neuromorphic chips like IBM's TrueNorth, Intel's Loihi and Mythic's AI platform reveal a tremendous performance improvement in terms of computational speed and density; at the same time, neuromorphic photonic layouts are constantly gaining ground in exploiting their large component portfolio for enabling GHz-bandwidth and low-energy neurons. Progressing in tight synergy with appropriate training techniques, this evolution has already started to translate into performance improvements in end-to-end applications, highlighting the practical perspectives of the new neural network hardware when effectively synergized with new training frameworks. Herein, we present a complete portfolio of neuromorphic photonic subsystems and architectures, highlighting their utilization in practical application scenario for time series classification and fiber transmission links. Our work extends along feed-forward and recurrent photonic NN models, demonstrating experimental results together with the required training methods for bridging the gap between software-deployed NNs and the photonic hardware. We report on the experimentally validated performance of a 10GHz photonic time series classification engine, presenting also preliminary results on how photonic neurons can replace DSP modules in end-to-end fiber transmission schemes. The perspectives of these layouts to yield energy and area efficiency benefits are discussed through a detailed energy and area breakdown of neuromorphic photonic technologies, highlighting a promising roadmap when plasmo-photonic hardware is adopted.",
  "year": 2021,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d964bd73f3de10162db0c908ef7c794a",
  "timestamp": "2025-05-15T00:49:36.380477"
}