{
  "id": 868,
  "title": "Phishing and Fraudulent Email Detection through Transfer Learning using pretrained transformer models",
  "abstract": "Phishing is a type of social engineering attack used by malicious users and cyber criminals for stealing sensitive information, installation of unwanted and malicious software, ransomware, and other advanced persistent threats on a victim's computer or mobile device. With the widespread adoption of the Internet, phishing attacks are on the rise and the recent work-from-home paradigm has increased the risk of phishing attacks targeted at large and small organizations alike. The consequences of phishing scams are often severe, ranging from financial loss, identity theft, data loss, and data theft. Though there are many ways of launching phishing attacks, phishing emails are one of the most commonly used techniques employed by cybercriminals. This is primarily because email addresses are easily obtainable and sending bulk emails is quite cheap, enabling attackers to send out a large number of emails hoping a few users will fall prey to the scam. Phishing emails are used for stealing sensitive information and credentials, delivering unwanted and malicious software, and delivering ransomware. This paper proposes a deep learning approach for detecting phishing and fraudulent emails. The proposed approach uses state-of-the-art pretrained transformer models and achieves very high accuracy, recall, and f1 score of 0.99.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4be91354942aabcc859755e67794905f",
  "timestamp": "2025-05-15T01:43:54.506681"
}