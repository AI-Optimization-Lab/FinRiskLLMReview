{
  "id": 3172,
  "title": "Prediction of Extubation Failure for Neonates with Respiratory Distress Syndrome Using the MIMIC-II Clinical Database",
  "abstract": "Extubation failure (EF) is an ongoing problem in the neonatal intensive care unit (NICU). Nearly 25% of neonates fail their first extubation attempt, requiring re-intubations that are associated with risk factors and financial costs. We identified 179 mechanically ventilated neonatal patients that were intubated within 24 hours of birth in the MIMIC-II intensive care database. We analyzed data from the patients 2 hours prior to their first extubation attempt, and developed a prediction algorithm to distinguish patients whose extubation attempt was successful from those that had EF. From an initial list of 57 candidate features, our machine learning approach narrowed down to six features useful for building an EF prediction model: monocyte cell count, rapid shallow breathing index, fraction of inspired oxygen (FiO(2)), heart rate, PaO2/FiO(2) ratio where PaO2 is the partial pressure of oxygen in arterial blood, and work of breathing index. Algorithm performance had an area under the receiver operating characteristic curve (AUC) of 0.871 and sensitivity of 70.1% at 90% specificity.",
  "year": 2012,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a59428ed38b74920f6578f2706a81201",
  "timestamp": "2025-05-15T02:23:33.502403"
}