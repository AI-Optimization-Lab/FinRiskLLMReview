{
  "id": 72,
  "title": "A new investment method with AutoEncoder: Applications to crypto currencies",
  "abstract": "This paper proposes a novel approach to the portfolio management using an AutoEncoder. In particular, features learned by an AutoEncoder with ReLU are directly exploited to portfolio constructions. Since the AutoEncoder extracts characteristics of data through a non-linear activation function ReLU, its realization is generally difficult due to the non-linear transformation procedure. In the current paper, we solve this problem by taking full advantage of the similarity of ReLU and an option payoff. Especially, this paper shows that the features are successfully replicated by applying so-called dynamic delta hedging strategy. An out of sample simulation with crypto currency dataset shows the effectiveness of our proposed strategy. (C) 2020 Elsevier Ltd. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "f7a4f5cca771b7caf410974943cbdd5d",
  "timestamp": "2025-05-15T00:31:50.346919"
}