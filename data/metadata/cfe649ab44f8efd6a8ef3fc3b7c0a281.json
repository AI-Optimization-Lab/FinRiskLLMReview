{
  "id": 2711,
  "title": "To split or to group: from divide-and-conquer to sub-task sharing for verifying multiple properties in model checking",
  "abstract": "Hardware systems complexity has constantly increased in recent years. Guaranteeing their correctness is a must. Formal verification techniques, such as model checking, now play a major role in industrial environments. Their efficiency in dealing with large sets of properties is crucial. This paper deals with property grouping, decomposition, and coverage in model checking. Property grouping is a valuable solution whenever several properties must be proved for a single model. As such sets may include easy-to-prove and/or similar properties, grouping can reduce overhead avoiding sub-tasks repetition. Property decomposition, following the divide-and-conquer paradigm, can be effective whenever a property turns out to be hard-to-prove. Our contribution is a heuristic property manager, running on top of a multi-engine model checking portfolio, aiming at productivity optimization. We compare different clustering heuristics, and we exploit decomposition strategies for property sub-setting. We also consider the problem of evaluating a coverage measure for properties, used to monitor the advancement of the verification task.",
  "year": 2018,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "cfe649ab44f8efd6a8ef3fc3b7c0a281",
  "timestamp": "2025-05-15T01:09:43.344061"
}