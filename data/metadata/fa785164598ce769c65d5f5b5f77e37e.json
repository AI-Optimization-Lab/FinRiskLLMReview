{
  "id": 17,
  "title": "Addressing the notion of trust around ChatGPT in the high-stakes use case of insurance",
  "abstract": "The public discourse concerning the level of (dis)trust in ChatGPT and other applications based on large language models (LLMs) is loaded with generic, dread risk terms, while the heterogeneity of relevant theoretical concepts and empirical measurements of trust further impedes in-depth analysis. Thus, a more nuanced understanding of the factors driving the trust judgment call is essential to avoid unwarranted trust. In this commentary paper, we propose that addressing the notion of trust in consumer-facing LLM-based systems across the insurance industry can confer enhanced specificity to this debate. The concept and role of trust are germane to this particular setting due to the highly intangible nature of the product coupled with elevated levels of risk, complexity, and information asymmetry. Moreover, widespread use of LLMs in this sector is to be expected, given the vast array of text documents, particularly general policy conditions or claims protocols. Insurance as a practice is highly relevant to the welfare of citizens and has numerous spillover effects on wider public policy areas. We therefore argue that a domain-specific approach to good AI governance is essential to avoid negative externalities around financial inclusion. Indeed, as a constitutive element of trust, vulnerability is particularly challenging within this highstakes set of transactions, with the adoption of LLMs adding to the socio-ethical risks. In light of this, our commentary provides a valuable baseline to support regulators and policymakers in unravelling the profound socioeconomic consequences that may arise from adopting consumer-facing LLMs in insurance.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "LLMs",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "fa785164598ce769c65d5f5b5f77e37e",
  "timestamp": "2025-05-15T01:45:16.334471"
}