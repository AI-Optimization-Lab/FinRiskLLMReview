{
  "id": 1762,
  "title": "Vine copula-based scenario tree generation approaches for portfolio optimization",
  "abstract": "This paper presents an efficient heuristic to generate multi-stage scenario trees for portfolio selection problems. In the case of two or more risky assets, investors need to account for the complex multivariate dependence among different assets. The dependence patterns have shown not only asymmetric and fat tails but also time-varying, and the upper and lower tails have different effect on portfolio management. In this paper, we design a new scenario generation method by combining the GARCH-type model and vine copula model to properly reflect these complex dependence patterns in multiple assets in a flexible way. A multi-stage scenario tree is generated sequentially from this model by simultaneously utilizing the simulation and clustering methods. The scenarios' nodal probabilities are determined by solving an improved moment matching model, whose objective is to maintain the central moments and lower tails of the original distribution. The resulting scenario trees are then tested on a multi-stage portfolio selection model. The experimental results prove the efficiency and advantages of our proposed scenario generation method over other existing models or methods and the positive influence of moment matching on our method.",
  "year": 2024,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "327540dabef9691e3b54dabd862a9a67",
  "timestamp": "2025-05-15T00:59:22.947698"
}