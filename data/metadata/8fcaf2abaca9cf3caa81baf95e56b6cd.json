{
  "id": 201,
  "title": "Network Traffic Prediction Model Based on Convolutional Neural Networks-Long Short-Term Memory and iTransformer",
  "abstract": "Accurately predicting network traffic is crucial for dynamically deploying computing resources in network data centers and reducing carbon emissions. In this paper, a hybrid prediction model Convolutional Neural Networks-Long Short-Term and iTransformer (CNN-LSTM-iTransformer) based on CNN-LSTM and iTransformer is proposed. CNN-LSTM is used to capture local features and long-term dependencies, while iTransformer is employed for feature relevance learning and prediction. In addition, the Huber Loss function is used to further improve the model prediction accuracy. In the experiment, the dataset was provided by Ant Financial Group, and the experimental results show that CNN-LSTM-iTransformer significantly reduces MAE to 0.112, MSE to 0.0212, MAPE to 0.123, and RWMAPE which represents prediction risk to 0.122, so the hybrid model CNN-LSTM-iTransformer achieves not only a higher prediction accuracy but also a lower prediction risk.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "deep learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8fcaf2abaca9cf3caa81baf95e56b6cd",
  "timestamp": "2025-05-15T01:35:04.499573"
}