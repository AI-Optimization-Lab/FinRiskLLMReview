{
  "id": 3171,
  "title": "Implicit recourse and credit card securitizations: What do fraud losses reveal?",
  "abstract": "In this paper, we develop and test a model of implicit recourse in asset-backed securitizations. Fraud losses on securitized assets are generally incurred by the bank and do not affect the performance of securitization trusts, while credit losses do affect the trust's performance and are potentially borne by the owner of the securitized assets. Thus, the classification of losses as either fraud or credit losses provides a potential avenue of implicit recourse to manipulate the performance of securitization trusts. Using annual data from 2001 to 2006, we find that the performance of the credit card securitization portfolio is negatively related to fraud losses reported by the bank. We examine these results in light of the proposed Basel II capital rules and argue that a bank's incentive to provide implicit recourse will increase under the anticipated regime. (C) 2007 Elsevier B.V. All rights reserved.",
  "year": 2008,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9b37ed345b5ee6e713b2633978e2f1a6",
  "timestamp": "2025-05-15T01:14:16.240405"
}