{
  "id": 6639,
  "title": "How does the choice of Value-at-Risk estimator influence asset allocation decisions?",
  "abstract": "Considering the growing need for managing financial risk, Value-at-Risk (VaR) prediction and portfolio optimisation with a focus on VaR have taken up an important role in banking and finance. Motivated by recent results showing that the choice of VaR estimator does not crucially influence decision-making in certain practical applications (e.g. in investment rankings), this study analyses the important question of how asset allocation decisions are affected when alternative VaR estimation methodologies are used. Focusing on the most popular, successful and conceptually different conditional VaR estimation techniques (i.e. historical simulation, peak over threshold method and quantile regression) and the flexible portfolio model of Campbell et al. [J. Banking Finance. 2001, 25(9), 1789-1804], we show in an empirical example and in a simulation study that these methods tend to deliver similar asset weights. In other words, optimal portfolio allocations appear to be not very sensitive to the choice of VaR estimator. This finding, which is robust in a variety of distributional environments and pre-whitening settings, supports the notion that, depending on the specific application, simple standard methods (i.e. historical simulation) used by many commercial banks do not necessarily have to be replaced by more complex approaches (based on, e.g. extreme value theory).",
  "year": 2018,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "6bd027ef969395a18e0a9d3dcadc3b07",
  "timestamp": "2025-05-15T03:00:05.012454"
}