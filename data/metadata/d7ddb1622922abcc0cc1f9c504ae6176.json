{
  "id": 2393,
  "title": "International stock market comovement in time and scale outlined with a thick pen",
  "abstract": "We quantify time-varying, bivariate and multivariate comovement between international stock market returns, across various time scales, based on a novel approach of Fryzlewicz and Oh (2011) called thick pen transform. With help of this nonparametric and simple tool, we study 11 countries and examine their comovement with respect to (non-dyadic) time scales/frequencies, development and region. We also consider all possible 2036 different combinations of two or more of these countries. In the two-country case, we make comparisons with cross-correlations, either rolling-window or based on the multi-period returns. We find that in the bivariate set-up with the USA, the BRIC countries, except for Brazil (especially over small time scales), offer diversification benefits, while in the multivariate one, clustering with respect to America or Europe (but not Asia) leads to homogeneous groups. Hence development and region cannot always be considered as ultimate clustering factors. Leave-one-out cross-validation shows a nuanced interplay of time scales, development and region as grouping factors for Brazil, Japan, Hong Kong and Russia. Additionally, we provide an example of a time-scale-dependent portfolio strategy.",
  "year": 2017,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d7ddb1622922abcc0cc1f9c504ae6176",
  "timestamp": "2025-05-15T01:06:38.025482"
}