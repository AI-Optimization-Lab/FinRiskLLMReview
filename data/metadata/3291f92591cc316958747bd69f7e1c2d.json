{
  "id": 7219,
  "title": "Inference of local regression in the presence of nuisance parameters",
  "abstract": "We consider inference based on local estimating equations in the presence of nuisance parameters. The framework is useful for a number of applications including those in economic policy evaluation based on discontinuities or kinks and in real-time financial risk management. We focus on the criterion-function-based (in particular, empirical likelihood-based) inference, and establish conditions under which the test statistic has a pivotal asymptotic distribution. In the key step of eliminating nuisance parameters in the (possibly non-smooth) criterion function, we consider two different approaches based on either concentration or Laplace-type plug-in estimation. The former is natural, and the latter does not require optimization and can be computationally attractive in applications. Our framework can easily incorporate bias correction induced by localization, and the inference is robust to the identification strength of the parameter of interest. The high-level assumptions are illustrated with several examples. We also conduct Monte Carlo simulations and provide an empirical application which assesses heterogeneous effects of academic probation in college and gender differences under the quantile regression discontinuity design. (C) 2020 Elsevier B.V. All rights reserved.",
  "year": 2020,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "3291f92591cc316958747bd69f7e1c2d",
  "timestamp": "2025-05-15T03:06:35.257545"
}