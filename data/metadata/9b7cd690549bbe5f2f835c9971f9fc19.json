{
  "id": 1593,
  "title": "Volatility forecasting and assessing risk of financial markets using multi-transformer neural network based architecture",
  "abstract": "This research introduces a more reliable model for predicting market volatility. The model incorporates Transformer and Multi-transformer layers with the GARCH and LSTM models and compares their performance to classic GARCH-type models. Euro-US Dollar FX Spot Rate (EURO-USD), Australian-US Dollar FX Spot Rate (AUDUSD), S&P 500 Index, FTSE 100 Index, Reliance Industries Ltd., and Samsung Electronics Co Ltd. are analyzed. The timeframe examined is January 2005 to December 2021, with training from 2005 to 2016 and testing from 2017 to 2021. Empirical evidence suggests that hybrid Neural-network models, notably Transformer-based models, outperform individual transformers, deep learning, neural networks, and traditional GARCH-type models, even in unpredictable conditions like the COVID-19 pandemic. Within the hybrid Neural-network models, MT-GARCH and MTL-GARCH showed lower validation error (RMSE) than the other models, showing that the bagging mechanism added to the attention mechanism of the Multi-Transformer architecture helped to lower the error in the variance in the noisy data of the daily returns of the assets, reducing the RMSE of the hybrid multi-Transformer models. In addition, three to four of the five hybrid neural-network models showed appropriate risk estimates for the entire test period, as observed from the Christoffersen test. Moreover, the lengthy sample period helps test whether hybrid models perform better than classic GARCH-type models in volatile conditions like the COVID-19 pandemic.",
  "year": 2024,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "9b7cd690549bbe5f2f835c9971f9fc19",
  "timestamp": "2025-05-15T02:05:08.211869"
}