{
  "id": 1858,
  "title": "Machine learning-based mass density model for hard magnetic 14:2:1 phases using chemical composition-based features",
  "abstract": "The Fe14Nd2B-based permanent magnets are technologically sought-after for energy conversion due to their unparalleled high energy product (520 kJ/m3). The basis for the excellent magnetic properties of the material is the 14:2:1 intermetallic phase with its outstanding intrinsic properties. Depending on the desired property portfolio, different chemical compositions of 14:2:1 phases are used. With such 14:2:1 phases, the conversion of literature data and/or measured magnetic moments (in mu B/f.u. or emu) into the magnetic saturation polarization (in Tesla) is often a challenge because the mass density, required for this, frequently does not get reported. We present a 'machine learning' mass density model for 14:2:1 phases, using chemical composition-based features (representing 33 elements). The datasets for training and testing contain 189 phases (176 compositionally different) with their literature reported densities. The model with compositional features achieved a low mean -absolute-error of 0.51 % on unseen test-dataset.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0f7eadd33ffff75f6a15f7c18b416222",
  "timestamp": "2025-05-15T01:00:27.337766"
}