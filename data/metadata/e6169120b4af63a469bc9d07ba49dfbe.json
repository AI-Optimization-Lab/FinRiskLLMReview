{
  "id": 4481,
  "title": "Statistical inference for extreme extremile in heavy-tailed heteroscedastic regression model",
  "abstract": "As a least squares analogue of quantiles, extremiles define a coherent risk measure determined by weighted expectations instead of tail probabilities. Estimating extremiles of heavy-tailed variables in a regression framework is a challenging task, especially for dependent cases. This paper develops some methods for the estimation of extreme conditional extremiles in the framework of heteroscedastic regression model with heavy-tail noises, specifically, direct and indirect methods based on the conditional extremile estimators for the residuals. We also construct corresponding bias-reduced estimators and investigate their asymptotic properties compared to the original versions. Our mathematical assumptions are satisfied in the mean-variance regression model and heteroscedastic single-index model, which makes it possible to apply our result in a series of important examples. We demonstrate our results through a simulation study and real sets of insurance and financial data analyses.& COPY; 2023 Elsevier B.V. All rights reserved.",
  "year": 2023,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "e6169120b4af63a469bc9d07ba49dfbe",
  "timestamp": "2025-05-15T02:37:47.501710"
}