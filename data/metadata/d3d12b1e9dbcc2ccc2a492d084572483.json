{
  "id": 4311,
  "title": "Small sample size problems and the power of the test in the event study methodology",
  "abstract": "This paper investigates the effect of small sample size problems on the power of the test in the event study methodologies by using the simulation technique. This paper also examines to what extent the significance test and market index abate the statistical errors resulting from small sample problems and increase the power of the test for the following three models of estimating abnormal performance: (a) mean adjusted returns model, (b) market adjusted returns model, and (c) market model. In order to assess the impact of small sample size problems by the simulation method, this paper employs four different sample size groups which consist of two hundred and fifty samples of 50, 20, 10 and 5 securities, respectively. For each sample in each sample size group, the securities are selected from the pool of 789 securities for which daily return data of at least 3 years are available on the Stock DB; of Korea Securities Research Institute (KRSI). For each security, a hypothetical event date is randomly selected with replacement using a uniform probability distribution from October 31, 1980 through December 22, 2003. This paper uses 250 daily return observations for the period around the hypothetical event day '0' of each firm, starting at day -244 and ending at day +5 relative to the event date. The first 239 days are defined as the estimation period. Parameters for each of the three performance models are estimated using the daily return data of the estimation period. The following 11 days from -5 day through +5 days are defined as the event period. A particular level of excess return is artificially introduced into a given sample by transforming its actual return data. For example, to simulate 1% abnormal performance for each security of a given sample, 0.01 is added to the actual daily return of the hypothetical event date for each sample security. In the event study methodologies, there are two types of statistical errors in the significance test of null hypothesis: Type I and Type II errors. While Type I error is to reject the null hypothesis of no excess returns when it is true, Type El error is to fail to reject the null hypothesis of no excess returns when it is false. Therefore, the power of a test is calculated as [1 - the probability of Type II errors]. The results of this paper can be summarized as follows: (1) For the sample size groups of 50 and 20 securities, the empirical distributions of mean excess returns seem close to normal and those of the test statistics are close to unit normal across all the three performance models. On the contrary. for small sample groups of 10 and 5 securities. both the mean excess returns and the test statistics are highly non-normal. (2) For a given level of excess return, larger samples more accurately detect the presence of abnormal performance across all the three performance models. It means that when there exist the small sample problems, the probability of Type II errors in the process of significance test increases, and the power of the test decreases. (3) The choice of significance test improves the power of the test for all the sample groups except the sample of 5 securities. When there is no clustering of event dates, the gains from using the share time series method as the significance test of null hypothesis are substantial for the sample size groups of 50, 20 and 10 securities in comparison to the gains from using the portfolio time series method. This result indicates that by making the appropriate choice of the significance test, I can solve the small sample problems, as long as the sample consists of more than 10 securities. (4) The choice of the market index does not lead to notable improvement in the power across all sample groups. For each sample group, with excess returns, there are no gains from using the Korea Stock Price Index instead of an equally weighted market index as a benchmark index. In summary, I conclude that for a sample of at least 20 securities. the small sample size problems would not be serious enough to distort empirical results in the event study as long as we use the share time series approach to the significance test with no time clustering.",
  "year": 2006,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "d3d12b1e9dbcc2ccc2a492d084572483",
  "timestamp": "2025-05-15T01:25:50.523887"
}