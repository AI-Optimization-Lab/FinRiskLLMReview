{
  "id": 2451,
  "title": "EXTRACTING GARCH EFFECTS FROM ASSET RETURNS USING ROBUST NMF",
  "abstract": "Identification of assets on the stock market that exhibit co-movement is a critical task for generating an efficiently diversified portfolio. We present a new application of non-negative matrix factorization to factor analysis of financial time series. We consider a conditionally heteroscedastic latent factor model, where each series is parameterized by a univariate ARCH model. Volatility clustering characteristics, e.g. GARCH effects, of the constituent assets of the Dow Jones Industrial Average are lever-aged to cluster assets based on the commonality of their volatility clusters. We present a new non-negative matrix factorization algorithm which is robust in the presence of noise, Robust NMF. We use a mixed low-rank over-complete dictionary learning approach to separate out the background Gaussian noise, emphasize the GARCH effects and achieve clearer asset groupings.",
  "year": 2009,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "1321d8ef1961e925281dd3eeffd25295",
  "timestamp": "2025-05-15T01:07:02.976919"
}