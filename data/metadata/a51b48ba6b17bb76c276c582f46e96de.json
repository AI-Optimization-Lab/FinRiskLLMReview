{
  "id": 3937,
  "title": "Extreme connectedness between renewable energy tokens and fossil fuel markets",
  "abstract": "This paper examines the aspect pertaining to the returns connectedness between renewable energy tokens, namely, Powerledger-POWR and WePower-WPR, and the fossil fuel markets, namely, WTI oil, Brent oil, and Natural gas. For this purpose, we employed a quantile-based regression approach, in order to explore the dependence structures that exist under diverse market conditions. The results of the analysis show that the element of connectedness in the renewable energy tokens-fossil fuel market nexus is characterized by asymmetry and heterogeneity in the tails that are compared to the respective mean and the median. Under normal market conditions, the WTI oil market emerges as the main net transmitter of return spillover to the renewable energy tokens. Whereas, Brent oil and natural gas markets are the net receivers of the return spillover from the digital assets. However, under periods of extreme negative returns, the Brent oil market behaves as the main net transmitter of return spillover to the renewable energy digital markets. Whereas, under period of extreme positive returns, the natural gas market appears to be the main net transmitter of return spillover to the renewable energy digital markets. Therefore, it can be fathomed that on aggregate, renewable energy digital tokens are weakly connected with fossil fuel markets, thus suggesting the addition of renewable energy tokens in the portfolio of fossil fuel markets.",
  "year": 2022,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a51b48ba6b17bb76c276c582f46e96de",
  "timestamp": "2025-05-15T01:22:11.988396"
}