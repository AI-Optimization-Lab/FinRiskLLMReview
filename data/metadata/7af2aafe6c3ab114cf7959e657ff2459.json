{
  "id": 4611,
  "title": "Estimation of Expected Shortfall Using Quantile Regression: A Comparison Study",
  "abstract": "Expected Shortfall (ES) is one of the most heavily used measures of financial risk. It is defined as a scaled integral of the quantile of the profit-and-loss distribution up to a certainly confidence level. As such, quantile regression (QR) and the closely related expectile regression (ER) methods are natural techniques for estimating ES. In this paper, we survey QR and ER based estimators of ES and introduce several novel variants. We compare the performance of these methods through simulation and through a data analysis based on four major US market indices: the S&P 500 Index, the Russell 2000 Index, the Dow Jones Industrial Average, and the NASDAQ Composite Index. Our results suggest that QR and ER methods often work better than other, more standard, approaches.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7af2aafe6c3ab114cf7959e657ff2459",
  "timestamp": "2025-05-15T02:38:56.459933"
}