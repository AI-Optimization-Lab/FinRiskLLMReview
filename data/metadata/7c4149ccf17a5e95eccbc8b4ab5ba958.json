{
  "id": 191,
  "title": "American Options in Time-Dependent One-Factor Models: Semi-Analytic Pricing, Numerical Methods, and ML Support",
  "abstract": "Semi -analytical pricing of American options in a time -dependent Ornstein-Uhlenbeck model was presented in Carr and Itkin (2021). It was shown that to obtain these prices one needs to numerically solve a nonlinear Volterra integral equation of the second kind to find the exercise boundary, which is a function of the time only. Once this is done, the option prices follow. It was also shown that computationally this method is as efficient as the forward finite difference solver, while also providing better accuracy and stability. Later this approach, called the generalized integral transform method, was significantly extended to various time -dependent one factor (Itkin et al. 2021) and stochastic volatility (Carr et al. 2022, Itkin and Muravey 2022b) models as applied to pricing barrier options. For American options, though, despite being possible, this was not explicitly reported anywhere. In this article our goal is to fill this gap and also discuss which numerical method can be efficient to solve the corresponding Volterra equations, also including machine learning.",
  "year": 2024,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7c4149ccf17a5e95eccbc8b4ab5ba958",
  "timestamp": "2025-05-15T01:29:09.143606"
}