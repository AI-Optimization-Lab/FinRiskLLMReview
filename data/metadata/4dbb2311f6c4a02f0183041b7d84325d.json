{
  "id": 1249,
  "title": "Deep-Learning Solution to Portfolio Selection with Serially Dependent Returns",
  "abstract": "This paper investigates a deep-learning solution to high-dimensional multiperiod portfolio optimization problems with bounding constraints on the control. We propose a deep neural network (DNN) architecture to describe the underlying control process. The DNN consists of K subnetworks, where K is the total number of decision steps. The feedback control function is determined solely by the network parameters. In this way, the multiperiod portfolio optimization problem is linked to a training problem of the DNN, that can be efficiently computed by the standard optimization techniques for network training. We offer a sufficient condition for the algorithm to converge for a general utility function and general asset return dynamics including serially dependent returns. Specifically, under the condition that the global minimum of the DNN training problem is attained, we prove that the algorithm converges with the quadratic utility function when the risky asset returns jointly follow multivariate autoregressive (1) models and/or multivariate generalized autoregressive conditional heteroskedasticity (1,1) models. Numerical examples demonstrate the superior performance of the DNN algorithm in various return dynamics for a high-dimensional portfolio (up to 100 dimensions).",
  "year": 2020,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "4dbb2311f6c4a02f0183041b7d84325d",
  "timestamp": "2025-05-15T00:53:42.050020"
}