{
  "id": 365,
  "title": "NIG-Levy process in asset price modeling: case of Estonian companies",
  "abstract": "As an asset is traded at fair value, its varying price trace an interesting trajectory reflecting in a general way the asset's value and underlying economic activities. These trajectory exhibit jumps, clustering and a host of other properties not usually captured by Gaussian based models. Levy processes offer the possibility of distinguishing jumps, diffusion, drift and the laxity to answer questions on frequency, continuity, etc. An important feature of normal inverse Gaussian-Levy (NIG-Levy) model is its path richness: it can model so many small jumps in a way that eliminates the need for a Gaussian component; hence, limitations arising from Gaussian based models are almost eliminated. Secondly, the characteristics listed above are reflected in the Levy triplet and are easily introduced in the modeling picture through estimated Levy parameters. Thirdly, knowledge of NIG-Levy parameters enables us to use NIG-Levy models as underlying asset price models for pricing financial derivatives. We use the R open software to calculate Levy parameters for 12 Estonian companies and choose good NIG-Levy asset price models by the method proposed by Kaarik and Umbleja (2011). We observe that not all financial data of Estonian companies trading on the Tallinn Stock Exchange between 01 Jan 2008 - 01 Jan 2012 can be effectively modeled by NIG-Levy process, despite having Levy parameters. Those positively modeled are recommended as underlying assets for pricing financial derivatives.",
  "year": 2012,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "80ac85147c96088e11ba5aa932711e1a",
  "timestamp": "2025-05-15T01:31:13.171922"
}