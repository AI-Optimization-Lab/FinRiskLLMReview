{
  "id": 4523,
  "title": "A good client gets arrested a lot: Constructing and maintaining profitable subjects through marking and surveillance",
  "abstract": "Criminal legal processing is an arduous classification, supervision, and extraction cycle increasingly administered by private entities. This article spotlights processing within commercial bail and uncovers profitable subjects-people (re)identified as future assets-as a stratifying and elusive construction with implications for criminal legal experiences. Bail agents deploy marking and surveillance like other legal professionals to process people. However, a profit objective and financial risk framework give rise to distinct applications. First, a shift in marking occurs in which legal involvement operates as credit and stratifies people into classification situations where they unevenly, and sometimes counterintuitively, access resources. Second, marked individuals are matched to different forms of surveillance that deviate in the degree of felt hassle and punishment. Surveillance is used for people to repeatedly prove their profitability in an environment where a dominant perception is that defendants are liabilities. Consequently, few can avoid the conditions that define their varying and unequal experiences.",
  "year": 2025,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "a59ff42d835d17016fc17a766194ddf7",
  "timestamp": "2025-05-15T02:38:20.303023"
}