{
  "id": 1023,
  "title": "Portfolio Optimization by Means of a Ï‡-Armed Bandit Algorithm",
  "abstract": "In this paper, we are interested in studying and solving the portfolio selection problem by means of a machine learning method. Particularly, we use a chi-armed bandit algorithm called Hierarchical Optimistic Optimization (HOO). HOO is an optimization approach that can be used for finding optima of box constrained nonlinear and nonconvex functions. Under some restrictions, such as locally Lipschitz condition, HOO can provide global solutions. Our idea consists in using HOO for solving some NP-hard variants of the portfolio selection problem. We test this approach on some data sets and report the results. In order to verify the quality of the solutions, we compare them with the best known solutions, provided by a derivative-free approach, called DIRECT. The preliminary numerical experiments give promising results.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "8b8cd3205e3e08dec045e298135abe6c",
  "timestamp": "2025-05-15T00:50:40.823194"
}