{
  "id": 361,
  "title": "Downside and upside risk spillovers from commercial banks into China's financial system: a new copula quantile regression-based CoVaR model",
  "abstract": "In this paper, we investigate the downside and upside risk spillovers from three kinds of commercial banks (state-owned commercial banks (SOCBs), joint-stock commercial banks (JSCBs) and city commercial banks (CCBs)) to China's financial system by proposing a new copula quantile regression-based CoVaR model. We find that (i) the dynamic risk spillovers show heterogeneity over time, specifically that its downward trend is significant after the stock market disaster in 2015; (ii) JSCBs display the largest risk spillovers, indicating that JSCBs are the main contributors to systemic risk in China's financial system; and (iii) the risk spillovers are not symmetrical, as the upside risk spillovers are smaller than the downside risk spillovers. Our results have crucial implications for financial regulators and investors who want to measure and prevent systemic financial risk and optimise their investment strategies.",
  "year": 2022,
  "source": "WOS",
  "area": "financial_risk",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "0290d84c95c9ad5c2db37e39d5566eeb",
  "timestamp": "2025-05-15T01:49:41.119094"
}