{
  "id": 3526,
  "title": "Statistical models to infer gas end-use efficiency in individual dwellings using smart metered data",
  "abstract": "Residential buildings can significantly contribute to the European Union's 2020 efficiency energy targets. For this reason, energy distributors and suppliers are required to provide assistance to householders to reduce energy end-use. This paper develops statistical modelling methods that can be used by suppliers to infer the gas fuel efficiency of buildings in their residential portfolio, in order to deliver improved energy management services to consumers. The study begins by estimating individual statistical building energy models for a sample of consumers and presents the resulting distribution of independent parameters. These parameter distributions are then characterised by regression models using descriptive household data that is generally known by the consumer and can be easily gathered by the energy supply company. These models are then used to compare the inferred energy end-use efficiency of the household (cooking, hot-water and space heating) to similar dwellings. Buildings with higher-than-expected gas consumption can be targeted for energy efficiency programmes. (C) 2016 Elsevier Ltd. All rights reserved.",
  "year": 2016,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "047c736dd1c1f8a9ef2bd82085730e2e",
  "timestamp": "2025-05-15T01:17:54.175911"
}