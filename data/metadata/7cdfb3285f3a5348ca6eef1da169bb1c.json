{
  "id": 402,
  "title": "An efficient portfolio loss model",
  "abstract": "We propose a new parsimonious model for valuating portfolio credit derivatives dependent on aggregate loss. The starting point is the loss distribution, which is constructed to be time dependent. We let the loss be beta distributed, and, by implication, the loss process becomes a stochastic jump process, where a jump corresponds to losses appearing simultaneously. The model matches empirical loss data well with only two parameters in addition to expected loss. The size of the jump is controlled by the clustering parameter, and the temporal correlation of jumps is controlled by the autocorrelation parameter. The full model is relatively efficient to implement, as we use a Monte Carlo at portfolio level. We derive analytical expressions for valuating tranches and for calculating regulatory capital. We provide examples of credit default swap index tranche pricing, including forward starting tranches. Comparisons are made with the one-factor Gaussian copula default time model, which fits historical loss data badly and has a deficient loss volatility term structure.",
  "year": 2019,
  "source": "WOS",
  "area": "derivatives_pricing",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "7cdfb3285f3a5348ca6eef1da169bb1c",
  "timestamp": "2025-05-15T01:31:50.632879"
}