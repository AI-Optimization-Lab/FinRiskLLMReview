{
  "id": 1251,
  "title": "Two analog neural models with the controllability on number of assets for sparse portfolio design",
  "abstract": "The aim of this research is to use continuous time neural networks for solving the sparse portfolio problem, where the objective function includes a non-differentiable e1-norm regularization term. To address the non differentiable issue, we introduce two novel continuous time neural models that are based on the Lagrange programming neural network (LPNN) framework. The proposed neural models have the ability to control the number of the assets in the resulting portfolio and adjust the weighting between risk and return. The first model, called LPNN-Approximation, handles the non-differentiable e1-norm term by utilizing a differentiable approximation. The second model merges the locally competitive algorithm (LCA) concept with the LPNN framework to address the non-differentiable term. It is called LPNN-LCA. For the LPNN-Approximation, we prove that the state of the network globally converges to the optimal solution of the sparse portfolio problem. Meanwhile, for the LPNN-LCA, we prove that all the equilibrium points of its dynamics correspond to the optimal solution of the sparse portfolio problem and are asymptotically stable. In addition, the realizations of the two models are discussed. Specifically, a thorough circuit realization for the thresholding elements of the LPNN-LCA model is presented. The effectiveness of the proposed approaches is verified by a number of numerical experiments, which show that the two proposed models outperform two state-of-the-art analog models.",
  "year": 2023,
  "source": "WOS",
  "area": "portfolio",
  "method": "machine learning",
  "keywords": [
    "machine learning",
    "supervised learning",
    "unsupervised learning",
    "reinforcement learning",
    "semi-supervised learning",
    "active learning",
    "classification",
    "regression",
    "PCA",
    "support vector machine",
    "SVM",
    "decision tree",
    "clustering",
    "principal components analysis",
    "manifold learning",
    "feature learning",
    "feature representation",
    "neural network",
    "deep learning",
    "representation learning",
    "backpropagation",
    "BP",
    "rectified linear unit",
    "ReLU",
    "sigmoid",
    "tanh",
    "hidden layer",
    "convolutional neural network",
    "CNN",
    "recurrent neural network",
    "long short-term memory",
    "LSTM",
    "sequence-to-sequence learning",
    "seq2seq",
    "encoder-decoder",
    "autoencoder",
    "denoising autoencoder",
    "deep belief network",
    "DBM",
    "restricted Boltzmann machine",
    "dropout regularization",
    "unsupervised pre-train",
    "memory network",
    "attention mechanism",
    "Large Language Model",
    "LLM",
    "In-context Learning",
    "Instruction Tuning",
    "Chain-of-Thought",
    "Few-shot Learning",
    "Zero-shot Learning",
    "Long Context Modeling",
    "Tool Manipulation",
    "Tool-augmented Model",
    "Memory Augmented Model",
    "ChatGPT",
    "GPT-4",
    "LLaMA"
  ],
  "cache_key": "636f35b03410b0fe882e9e120c0f0321",
  "timestamp": "2025-05-15T00:53:42.054037"
}